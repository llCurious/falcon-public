{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "path = f'{os.path.expanduser(\"~\")}/Downloads'\n",
    "trainset = datasets.CIFAR10(root=path, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root=path, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_CryptGPU(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_CryptGPU, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "class AlexNet_Falcon(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_Falcon, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 10),\n",
    "                # nn.ReLU(inplace=True),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet_Official(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_Official, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class AlexNet_Official_modify(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_Official_modify, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # functions to show an image\n",
    "\n",
    "\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "# # print labels\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 256, 1, 1]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 96, 10, 10]          34,944\n",
      "|    └─ReLU: 2-2                         [-1, 96, 10, 10]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 96, 4, 4]            --\n",
      "|    └─BatchNorm2d: 2-4                  [-1, 96, 4, 4]            192\n",
      "|    └─Conv2d: 2-5                       [-1, 256, 2, 2]           614,656\n",
      "|    └─ReLU: 2-6                         [-1, 256, 2, 2]           --\n",
      "|    └─MaxPool2d: 2-7                    [-1, 256, 1, 1]           --\n",
      "|    └─BatchNorm2d: 2-8                  [-1, 256, 1, 1]           512\n",
      "|    └─Conv2d: 2-9                       [-1, 384, 1, 1]           885,120\n",
      "|    └─ReLU: 2-10                        [-1, 384, 1, 1]           --\n",
      "|    └─Conv2d: 2-11                      [-1, 384, 1, 1]           1,327,488\n",
      "|    └─ReLU: 2-12                        [-1, 384, 1, 1]           --\n",
      "|    └─Conv2d: 2-13                      [-1, 256, 1, 1]           884,992\n",
      "|    └─ReLU: 2-14                        [-1, 256, 1, 1]           --\n",
      "├─Sequential: 1-2                        [-1, 10]                  --\n",
      "|    └─Flatten: 2-15                     [-1, 256]                 --\n",
      "|    └─Linear: 2-16                      [-1, 256]                 65,792\n",
      "|    └─ReLU: 2-17                        [-1, 256]                 --\n",
      "|    └─Linear: 2-18                      [-1, 256]                 65,792\n",
      "|    └─ReLU: 2-19                        [-1, 256]                 --\n",
      "|    └─Linear: 2-20                      [-1, 10]                  2,570\n",
      "==========================================================================================\n",
      "Total params: 3,882,058\n",
      "Trainable params: 3,882,058\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 13.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 14.81\n",
      "Estimated Total Size (MB): 14.93\n",
      "==========================================================================================\n",
      "Epoch: 0, Batch 0, Loss: 2.3048224449157715\n",
      "Epoch: 0, Batch 100, Loss: 2.2974398136138916\n",
      "Epoch: 0, Batch 200, Loss: 2.2966396808624268\n",
      "Epoch: 0, Batch 300, Loss: 2.302497148513794\n",
      "Epoch: 0, Batch 400, Loss: 2.3026912212371826\n",
      "Epoch: 0, Batch 500, Loss: 2.3172719478607178\n",
      "Epoch: 0, Batch 600, Loss: 2.295043468475342\n",
      "Epoch: 0, Batch 700, Loss: 2.291917324066162\n",
      "Epoch: 0, Batch 800, Loss: 2.3025920391082764\n",
      "Epoch: 0, Batch 900, Loss: 2.2999107837677\n",
      "Epoch: 0, Batch 1000, Loss: 2.318312644958496\n",
      "Epoch: 0, Batch 1100, Loss: 2.28167986869812\n",
      "Epoch: 0, Batch 1200, Loss: 2.0782015323638916\n",
      "Epoch: 0, Batch 1300, Loss: 2.015287160873413\n",
      "Epoch: 0, Batch 1400, Loss: 1.9270179271697998\n",
      "Epoch: 0, Batch 1500, Loss: 1.8393969535827637\n",
      "Epoch: 1, Batch 0, Loss: 1.8213468790054321\n",
      "Epoch: 1, Batch 100, Loss: 1.7263816595077515\n",
      "Epoch: 1, Batch 200, Loss: 1.6736961603164673\n",
      "Epoch: 1, Batch 300, Loss: 1.5477279424667358\n",
      "Epoch: 1, Batch 400, Loss: 1.9138801097869873\n",
      "Epoch: 1, Batch 500, Loss: 1.6414127349853516\n",
      "Epoch: 1, Batch 600, Loss: 1.9315215349197388\n",
      "Epoch: 1, Batch 700, Loss: 1.7508589029312134\n",
      "Epoch: 1, Batch 800, Loss: 1.6503654718399048\n",
      "Epoch: 1, Batch 900, Loss: 1.423240303993225\n",
      "Epoch: 1, Batch 1000, Loss: 1.4566900730133057\n",
      "Epoch: 1, Batch 1100, Loss: 1.6605253219604492\n",
      "Epoch: 1, Batch 1200, Loss: 1.7541927099227905\n",
      "Epoch: 1, Batch 1300, Loss: 1.3879843950271606\n",
      "Epoch: 1, Batch 1400, Loss: 1.3024446964263916\n",
      "Epoch: 1, Batch 1500, Loss: 1.6368207931518555\n",
      "Epoch: 2, Batch 0, Loss: 1.2870961427688599\n",
      "Epoch: 2, Batch 100, Loss: 1.267344355583191\n",
      "Epoch: 2, Batch 200, Loss: 1.3669427633285522\n",
      "Epoch: 2, Batch 300, Loss: 1.325300693511963\n",
      "Epoch: 2, Batch 400, Loss: 1.2432754039764404\n",
      "Epoch: 2, Batch 500, Loss: 1.165804147720337\n",
      "Epoch: 2, Batch 600, Loss: 1.6288259029388428\n",
      "Epoch: 2, Batch 700, Loss: 1.156908631324768\n",
      "Epoch: 2, Batch 800, Loss: 1.2382615804672241\n",
      "Epoch: 2, Batch 900, Loss: 1.0099117755889893\n",
      "Epoch: 2, Batch 1000, Loss: 1.1737334728240967\n",
      "Epoch: 2, Batch 1100, Loss: 1.447678565979004\n",
      "Epoch: 2, Batch 1200, Loss: 1.0271985530853271\n",
      "Epoch: 2, Batch 1300, Loss: 1.1323310136795044\n",
      "Epoch: 2, Batch 1400, Loss: 1.4000681638717651\n",
      "Epoch: 2, Batch 1500, Loss: 1.1153459548950195\n",
      "Epoch: 3, Batch 0, Loss: 1.060943841934204\n",
      "Epoch: 3, Batch 100, Loss: 0.7014942765235901\n",
      "Epoch: 3, Batch 200, Loss: 1.0343592166900635\n",
      "Epoch: 3, Batch 300, Loss: 1.3541566133499146\n",
      "Epoch: 3, Batch 400, Loss: 1.5358668565750122\n",
      "Epoch: 3, Batch 500, Loss: 1.012894868850708\n",
      "Epoch: 3, Batch 600, Loss: 1.4065980911254883\n",
      "Epoch: 3, Batch 700, Loss: 0.9851276874542236\n",
      "Epoch: 3, Batch 800, Loss: 1.2351889610290527\n",
      "Epoch: 3, Batch 900, Loss: 0.9932319521903992\n",
      "Epoch: 3, Batch 1000, Loss: 1.2908375263214111\n",
      "Epoch: 3, Batch 1100, Loss: 0.9160767197608948\n",
      "Epoch: 3, Batch 1200, Loss: 1.285638451576233\n",
      "Epoch: 3, Batch 1300, Loss: 1.1624267101287842\n",
      "Epoch: 3, Batch 1400, Loss: 1.5223023891448975\n",
      "Epoch: 3, Batch 1500, Loss: 1.3001867532730103\n",
      "Epoch: 4, Batch 0, Loss: 0.9132130146026611\n",
      "Epoch: 4, Batch 100, Loss: 0.845802366733551\n",
      "Epoch: 4, Batch 200, Loss: 1.091241478919983\n",
      "Epoch: 4, Batch 300, Loss: 0.7766786813735962\n",
      "Epoch: 4, Batch 400, Loss: 0.9866692423820496\n",
      "Epoch: 4, Batch 500, Loss: 0.8143962621688843\n",
      "Epoch: 4, Batch 600, Loss: 0.9542103409767151\n",
      "Epoch: 4, Batch 700, Loss: 0.8250712752342224\n",
      "Epoch: 4, Batch 800, Loss: 1.1771219968795776\n",
      "Epoch: 4, Batch 900, Loss: 0.908402681350708\n",
      "Epoch: 4, Batch 1000, Loss: 0.5994538068771362\n",
      "Epoch: 4, Batch 1100, Loss: 0.9318981766700745\n",
      "Epoch: 4, Batch 1200, Loss: 1.4235481023788452\n",
      "Epoch: 4, Batch 1300, Loss: 0.7287770509719849\n",
      "Epoch: 4, Batch 1400, Loss: 1.2437702417373657\n",
      "Epoch: 4, Batch 1500, Loss: 1.1303982734680176\n",
      "Epoch: 5, Batch 0, Loss: 1.0944316387176514\n",
      "Epoch: 5, Batch 100, Loss: 0.6609628200531006\n",
      "Epoch: 5, Batch 200, Loss: 0.9037396907806396\n",
      "Epoch: 5, Batch 300, Loss: 1.1094120740890503\n",
      "Epoch: 5, Batch 400, Loss: 1.028737187385559\n",
      "Epoch: 5, Batch 500, Loss: 0.9454236030578613\n",
      "Epoch: 5, Batch 600, Loss: 0.9019254446029663\n",
      "Epoch: 5, Batch 700, Loss: 0.5247771739959717\n",
      "Epoch: 5, Batch 800, Loss: 0.9778274893760681\n",
      "Epoch: 5, Batch 900, Loss: 1.0692124366760254\n",
      "Epoch: 5, Batch 1000, Loss: 1.0379705429077148\n",
      "Epoch: 5, Batch 1100, Loss: 1.3421694040298462\n",
      "Epoch: 5, Batch 1200, Loss: 0.6767202615737915\n",
      "Epoch: 5, Batch 1300, Loss: 0.6985637545585632\n",
      "Epoch: 5, Batch 1400, Loss: 0.6743367314338684\n",
      "Epoch: 5, Batch 1500, Loss: 0.7753691673278809\n",
      "Epoch: 6, Batch 0, Loss: 1.0448100566864014\n",
      "Epoch: 6, Batch 100, Loss: 0.897909939289093\n",
      "Epoch: 6, Batch 200, Loss: 0.7183297276496887\n",
      "Epoch: 6, Batch 300, Loss: 0.728122353553772\n",
      "Epoch: 6, Batch 400, Loss: 1.2145754098892212\n",
      "Epoch: 6, Batch 500, Loss: 0.9485419392585754\n",
      "Epoch: 6, Batch 600, Loss: 0.8836126327514648\n",
      "Epoch: 6, Batch 700, Loss: 0.9887258410453796\n",
      "Epoch: 6, Batch 800, Loss: 0.9240283966064453\n",
      "Epoch: 6, Batch 900, Loss: 1.096321702003479\n",
      "Epoch: 6, Batch 1000, Loss: 0.8600579500198364\n",
      "Epoch: 6, Batch 1100, Loss: 0.7867465019226074\n",
      "Epoch: 6, Batch 1200, Loss: 0.5940696597099304\n",
      "Epoch: 6, Batch 1300, Loss: 1.0049827098846436\n",
      "Epoch: 6, Batch 1400, Loss: 0.6168011426925659\n",
      "Epoch: 6, Batch 1500, Loss: 0.958250105381012\n",
      "Epoch: 7, Batch 0, Loss: 0.42601725459098816\n",
      "Epoch: 7, Batch 100, Loss: 0.577393651008606\n",
      "Epoch: 7, Batch 200, Loss: 0.6757772564888\n",
      "Epoch: 7, Batch 300, Loss: 0.8397324085235596\n",
      "Epoch: 7, Batch 400, Loss: 0.6927837133407593\n",
      "Epoch: 7, Batch 500, Loss: 0.4764164984226227\n",
      "Epoch: 7, Batch 600, Loss: 0.4845016300678253\n",
      "Epoch: 7, Batch 700, Loss: 0.3578128516674042\n",
      "Epoch: 7, Batch 800, Loss: 0.8045006990432739\n",
      "Epoch: 7, Batch 900, Loss: 0.6353157162666321\n",
      "Epoch: 7, Batch 1000, Loss: 0.4462592303752899\n",
      "Epoch: 7, Batch 1100, Loss: 0.7010040879249573\n",
      "Epoch: 7, Batch 1200, Loss: 0.8254308104515076\n",
      "Epoch: 7, Batch 1300, Loss: 0.83530193567276\n",
      "Epoch: 7, Batch 1400, Loss: 1.0213677883148193\n",
      "Epoch: 7, Batch 1500, Loss: 0.929815948009491\n",
      "Epoch: 8, Batch 0, Loss: 0.5524708032608032\n",
      "Epoch: 8, Batch 100, Loss: 0.5194327235221863\n",
      "Epoch: 8, Batch 200, Loss: 0.7341266870498657\n",
      "Epoch: 8, Batch 300, Loss: 0.38911154866218567\n",
      "Epoch: 8, Batch 400, Loss: 0.794087290763855\n",
      "Epoch: 8, Batch 500, Loss: 0.5801321268081665\n",
      "Epoch: 8, Batch 600, Loss: 0.49673372507095337\n",
      "Epoch: 8, Batch 700, Loss: 0.9460161328315735\n",
      "Epoch: 8, Batch 800, Loss: 0.4415128827095032\n",
      "Epoch: 8, Batch 900, Loss: 0.7186995148658752\n",
      "Epoch: 8, Batch 1000, Loss: 0.8117449283599854\n",
      "Epoch: 8, Batch 1100, Loss: 0.448393315076828\n",
      "Epoch: 8, Batch 1200, Loss: 0.509706974029541\n",
      "Epoch: 8, Batch 1300, Loss: 1.1180543899536133\n",
      "Epoch: 8, Batch 1400, Loss: 0.455692857503891\n",
      "Epoch: 8, Batch 1500, Loss: 0.5889124274253845\n",
      "Epoch: 9, Batch 0, Loss: 0.317281037569046\n",
      "Epoch: 9, Batch 100, Loss: 0.3677072823047638\n",
      "Epoch: 9, Batch 200, Loss: 0.745567262172699\n",
      "Epoch: 9, Batch 300, Loss: 0.601424515247345\n",
      "Epoch: 9, Batch 400, Loss: 0.4216260015964508\n",
      "Epoch: 9, Batch 500, Loss: 0.5631809234619141\n",
      "Epoch: 9, Batch 600, Loss: 0.7397580742835999\n",
      "Epoch: 9, Batch 700, Loss: 0.49027448892593384\n",
      "Epoch: 9, Batch 800, Loss: 0.753913402557373\n",
      "Epoch: 9, Batch 900, Loss: 0.654664933681488\n",
      "Epoch: 9, Batch 1000, Loss: 0.7438989281654358\n",
      "Epoch: 9, Batch 1100, Loss: 0.5822337865829468\n",
      "Epoch: 9, Batch 1200, Loss: 0.5789794921875\n",
      "Epoch: 9, Batch 1300, Loss: 0.38366037607192993\n",
      "Epoch: 9, Batch 1400, Loss: 0.4398897886276245\n",
      "Epoch: 9, Batch 1500, Loss: 0.9403244256973267\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "logger = SummaryWriter(log_dir = 'log')\n",
    "\n",
    "\n",
    "model = AlexNet_CryptGPU(num_classes=10)\n",
    "summary(model, (3, 32, 32))\n",
    "\n",
    "\n",
    "# GPU run\n",
    "gpu_list = [0, 1, 2, 3]\n",
    "gpu_list_str = ','.join(map(str, gpu_list))\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1) \n",
    "# AlexNet Official\n",
    "# lr=0.001, momentum=0.9 converges about 2~3 epoch. Without momentum, it does not converge.\n",
    "# lr=0.01 (remove Dropout) converges about 3~4 epoch.\n",
    "\n",
    "# AlexNet Official Modified: MaxPooling --> AvgPooing, remove Dropout\n",
    "# lr=0.01 converges\n",
    "\n",
    "# AlexNet CryptGPU\n",
    "# lr=0.01. momentum=0.9 not converge. No BN, remove the last avgpooling layer.\n",
    "\n",
    "# AlexNet Falcon\n",
    "# lr=0.01 converges\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        output = model(images.to(device))\n",
    "\n",
    "        # MSEloss\n",
    "        # one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch: {}, Batch {}, Loss: {}'.format(epoch, batch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sss = iter(testloader)\n",
    "# images, labels = test_sss.next()\n",
    "# images, labels = test_sss.next()\n",
    "# output = model(images)\n",
    "# _, output = torch.max(output, 1)\n",
    "\n",
    "# correct = 0\n",
    "# for i in range(len(output)):\n",
    "#     if output[i] == labels[i]:\n",
    "#         correct += 1\n",
    "# print(f\"acc: {correct * 1. / 128}\")\n",
    "# print(f\"Output: ${output}\")\n",
    "# print(f\"Target: ${labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pretrained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "Layer features.0, type weight, shape (96, 3, 11, 11)\n",
      "features.0.bias\n",
      "Layer features.0, type bias, shape (96,)\n",
      "features.3.weight\n",
      "Layer features.3, type weight, shape (96,)\n",
      "features.3.bias\n",
      "Layer features.3, type bias, shape (96,)\n",
      "features.4.weight\n",
      "Layer features.4, type weight, shape (256, 96, 5, 5)\n",
      "features.4.bias\n",
      "Layer features.4, type bias, shape (256,)\n",
      "features.7.weight\n",
      "Layer features.7, type weight, shape (256,)\n",
      "features.7.bias\n",
      "Layer features.7, type bias, shape (256,)\n",
      "features.8.weight\n",
      "Layer features.8, type weight, shape (384, 256, 3, 3)\n",
      "features.8.bias\n",
      "Layer features.8, type bias, shape (384,)\n",
      "features.10.weight\n",
      "Layer features.10, type weight, shape (384, 384, 3, 3)\n",
      "features.10.bias\n",
      "Layer features.10, type bias, shape (384,)\n",
      "features.12.weight\n",
      "Layer features.12, type weight, shape (256, 384, 3, 3)\n",
      "features.12.bias\n",
      "Layer features.12, type bias, shape (256,)\n",
      "fc_layers.1.weight\n",
      "Layer fc_layers.1, type weight, shape (256, 256)\n",
      "fc_layers.1.bias\n",
      "Layer fc_layers.1, type bias, shape (256,)\n",
      "fc_layers.3.weight\n",
      "Layer fc_layers.3, type weight, shape (256, 256)\n",
      "fc_layers.3.bias\n",
      "Layer fc_layers.3, type bias, shape (256,)\n",
      "fc_layers.5.weight\n",
      "Layer fc_layers.5, type weight, shape (10, 256)\n",
      "fc_layers.5.bias\n",
      "Layer fc_layers.5, type bias, shape (10,)\n"
     ]
    }
   ],
   "source": [
    "params = [(name, p.data.cpu().numpy()) for (name, p) in model.named_parameters()]\n",
    "\n",
    "for (name, p) in params:\n",
    "    print(name)\n",
    "    print(f\"Layer {str(name.split('.')[0])+'.'+str(name.split('.')[1])}, type {name.split('.')[2]}, shape {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save parameters to /home/haoqi.whq/output/trained/AlexNet/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "subdir = 'trained' if epochs != 0 else 'init'\n",
    "path = f\"{os.path.expanduser('~')}/output/{subdir}/AlexNet/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "print(f'Save parameters to {path}')\n",
    "\n",
    "np.savetxt(fname=path+\"cnn1_weight_0\", delimiter=\" \", X=params[0][1].reshape(3*11*11, 96).tolist())\n",
    "np.savetxt(fname=path+\"cnn1_bias_0\", delimiter=\" \",  X=params[1][1].tolist())\n",
    "np.savetxt(fname=path+\"bn1_gamma_0\", delimiter=\" \",  X=params[2][1].tolist())\n",
    "np.savetxt(fname=path+\"bn1_beta_0\", delimiter=\" \",  X=params[3][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn2_weight_0\", delimiter=\" \", X=params[4][1].reshape(96*5*5, 256).tolist())\n",
    "np.savetxt(fname=path+\"cnn2_bias_0\", delimiter=\" \", X=params[5][1].tolist())\n",
    "np.savetxt(fname=path+\"bn2_gamma_0\", delimiter=\" \",  X=params[6][1].tolist())\n",
    "np.savetxt(fname=path+\"bn2_beta_0\", delimiter=\" \",  X=params[7][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn3_weight_0\", delimiter=\" \", X=params[8][1].reshape(256*3*3, 384).tolist())\n",
    "np.savetxt(fname=path+\"cnn3_bias_0\", delimiter=\" \", X=params[9][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn4_weight_0\", delimiter=\" \", X=params[10][1].reshape(384*3*3, 384).tolist())\n",
    "np.savetxt(fname=path+\"cnn4_bias_0\", delimiter=\" \", X=params[11][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn5_weight_0\", delimiter=\" \", X=params[12][1].reshape(384*3*3, 256).tolist())\n",
    "np.savetxt(fname=path+\"cnn5_bias_0\", delimiter=\" \", X=params[13][1].tolist())\n",
    "# FC\n",
    "np.savetxt(fname=path+\"fc1_weight_0\", delimiter=\" \", X=params[14][1].tolist())\n",
    "np.savetxt(fname=path+\"fc1_bias_0\", delimiter=\" \", X=params[15][1].tolist())\n",
    "np.savetxt(fname=path+\"fc2_weight_0\", delimiter=\" \", X=params[16][1].tolist())\n",
    "np.savetxt(fname=path+\"fc2_bias_0\", delimiter=\" \", X=params[17][1].tolist())\n",
    "np.savetxt(fname=path+\"fc3_weight_0\", delimiter=\" \", X=params[18][1].tolist())\n",
    "np.savetxt(fname=path+\"fc3_bias_0\", delimiter=\" \", X=params[19][1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN\n",
      "forward\n",
      "[316.22778      2.3093395    1.2060366    0.89442366   0.9176591 ]\n",
      "[[ 0.         -0.5773349  -0.9045274  -1.3416355  -1.1470739 ]\n",
      " [ 0.          1.7320046   1.5075457   1.3416355   1.6059034 ]\n",
      " [ 0.         -0.5773349  -0.9045274   0.44721183 -0.22941478]\n",
      " [ 0.         -0.5773349   0.30150914 -0.44721183 -0.22941478]]\n",
      "[[ 0.         -0.5773349  -0.9045274  -1.3416355  -1.1470739 ]\n",
      " [ 0.          1.7320046   1.5075457   1.3416355   1.6059034 ]\n",
      " [ 0.         -0.5773349  -0.9045274   0.44721183 -0.22941478]\n",
      " [ 0.         -0.5773349   0.30150914 -0.44721183 -0.22941478]]\n",
      "backward\n",
      "++++++++++++++++++++\n",
      "[[ 4.  8. 12. 16. 20.]\n",
      " [ 4. 12. 20. 28. 32.]\n",
      " [ 4.  8. 12. 24. 24.]\n",
      " [ 4.  8. 16. 20. 24.]]\n",
      "[[ 0.0000000e+00 -5.3286552e-05 -4.3869019e-05 -4.8160553e-05\n",
      "  -4.1961670e-05]\n",
      " [ 0.0000000e+00  1.5997887e-04  7.2956085e-05  4.8160553e-05\n",
      "   5.9127808e-05]\n",
      " [ 0.0000000e+00 -5.3286552e-05 -4.3869019e-05  1.5974045e-05\n",
      "  -8.4042549e-06]\n",
      " [ 0.0000000e+00 -5.3286552e-05  1.4603138e-05 -1.5974045e-05\n",
      "  -8.4042549e-06]]\n",
      "(array([[ 0.0000000e+00, -3.0764186e-05, -1.3226911e-05, -1.0768985e-05,\n",
      "        -9.6266267e-06],\n",
      "       [ 0.0000000e+00,  9.2361377e-05,  2.1996926e-05,  1.0768985e-05,\n",
      "         1.3564792e-05],\n",
      "       [ 0.0000000e+00, -3.0764186e-05, -1.3226911e-05,  3.5718908e-06,\n",
      "        -1.9280603e-06],\n",
      "       [ 0.0000000e+00, -3.0764186e-05,  4.4029798e-06, -3.5718908e-06,\n",
      "        -1.9280603e-06]], dtype=float32), array([0.       , 1.7320046, 3.3166006, 4.472118 , 4.3588805],\n",
      "      dtype=float32), array([ 4.,  9., 15., 22., 25.], dtype=float32))\n",
      "[-0.05037197 -0.1         5.11753794]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "IT_N = 3\n",
    "def inverse_sqrt(x):\n",
    "    init_g = np.exp(-(x/2+0.2))*2 + 0\n",
    "    init_g -= x/1024\n",
    "\n",
    "    for i in range(IT_N):\n",
    "        init_g = init_g*(3-x * init_g * init_g)/2\n",
    "    return init_g\n",
    "    \n",
    "class BN:\n",
    "    def __init__(self, dims, gamma=0, beta=0):\n",
    "        print('BN')\n",
    "        self.eps = 1e-5\n",
    "        self.gamma = np.ones((dims, ), dtype=\"float32\")\n",
    "        self.beta = np.zeros((dims, ), dtype=\"float32\")\n",
    "        \n",
    "        self.inv_sqrt = None\n",
    "        self.norm_x = None\n",
    "\n",
    "        self.beta_grad = None\n",
    "        self.gamma_grad = None\n",
    "        self.act_grad = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=0)   # 1 truncation by batchSize [1, D]\n",
    "        x_mean = x - mean   # [B, D]\n",
    "        var = np.mean(x_mean * x_mean, axis=0)  # 1 multiplication, 1 truncation by batchsize [1, D]\n",
    "        var_eps = var + self.eps\n",
    "\n",
    "        # protocol inv_sqrt\n",
    "        self.inv_sqrt = 1. / np.sqrt(var_eps)   # 1 inverse sqrt [1, D]\n",
    "        # print(\"======\")\n",
    "        # print(var_eps)\n",
    "        # print(self.inv_sqrt)\n",
    "        # print(inverse_sqrt(var_eps))\n",
    "        self.norm_x = x_mean * self.inv_sqrt    # 1 multiplication [B, D] * [1, D]. Falcon here has bug.\n",
    "        print(self.inv_sqrt)\n",
    "        print(self.norm_x)\n",
    "\n",
    "        return self.gamma * self.norm_x + self.beta     # 1 multiplication\n",
    "\n",
    "    def backward(self, grad):\n",
    "        B, D = grad.shape\n",
    "        self.beta_grad = np.sum(grad, axis=0)\n",
    "        self.gamma_grad = np.sum(self.norm_x * grad, axis=0)    # 1 multiplication\n",
    "\n",
    "        dxhat = grad * self.gamma   # 1 multiplication\n",
    "\n",
    "        print('+' * 20)\n",
    "        print(B*dxhat)\n",
    "        # print(np.sum(dxhat, axis=0))\n",
    "        # print(np.sum(dxhat * self.norm_x, axis=0))\n",
    "        # print(self.norm_x * np.sum(dxhat * self.norm_x, axis=0))\n",
    "        print((B*dxhat - np.sum(dxhat, axis=0) - self.norm_x * np.sum(dxhat * self.norm_x, axis=0)))\n",
    "        self.act_grad = self.inv_sqrt * \\\n",
    "                        (B*dxhat - np.sum(dxhat, axis=0) - self.norm_x * np.sum(dxhat * self.norm_x, axis=0)) \\\n",
    "                        / B # 3 multiplication, 1 truncation\n",
    "\n",
    "        return self.act_grad, self.gamma_grad, self.beta_grad\n",
    "\n",
    "bn = BN(5)\n",
    "\n",
    "data = np.array([[1, 2, 3, 4, 5],\n",
    "                 [1, 3, 5, 7, 8],\n",
    "                 [1, 2, 3, 6, 6],\n",
    "                 [1, 2, 4, 5, 6]]).astype(np.float32)\n",
    "x_raw = torch.from_numpy(data)\n",
    "x = x_raw.numpy()\n",
    "\n",
    "grad = np.array([[1, 2, 3, 4, 5],\n",
    "                 [1, 3, 5, 7, 8],\n",
    "                 [1, 2, 3, 6, 6],\n",
    "                 [1, 2, 4, 5, 6]]).astype(np.float32)\n",
    "# grad = grad * 1024\n",
    "\n",
    "print(\"forward\")\n",
    "f_o = bn.forward(x)\n",
    "print(f_o)\n",
    "\n",
    "print(\"backward\")\n",
    "b_o = bn.backward(grad)\n",
    "print(b_o)\n",
    "\n",
    "print(inverse_sqrt(np.array([16, 100, 0.01])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.57733488 -0.90452743 -1.34163547 -1.14707386]\n",
      " [ 0.          1.73200464  1.50754571  1.34163547  1.60590339]\n",
      " [ 0.         -0.57733488 -0.90452743  0.44721183 -0.22941478]\n",
      " [ 0.         -0.57733488  0.30150914 -0.44721183 -0.22941478]]\n"
     ]
    }
   ],
   "source": [
    "class MyBN:\n",
    "    def __init__(self, momentum, eps, num_features):\n",
    "        \"\"\"\n",
    "        初始化参数值\n",
    "        :param momentum: 追踪样本整体均值和方差的动量\n",
    "        :param eps: 防止数值计算错误\n",
    "        :param num_features: 特征数量\n",
    "        \"\"\"\n",
    "        # 对每个batch的mean和var进行追踪统计\n",
    "        self._running_mean = 0\n",
    "        self._running_var = 1\n",
    "        # 更新self._running_xxx时的动量\n",
    "        self._momentum = momentum\n",
    "        # 防止分母计算为0\n",
    "        self._eps = eps\n",
    "        # 对应论文中需要更新的beta和gamma，采用pytorch文档中的初始化值\n",
    "        self._beta = np.zeros(shape=(num_features, ))\n",
    "        self._gamma = np.ones(shape=(num_features, ))\n",
    "\n",
    "    def batch_norm(self, x):\n",
    "        \"\"\"\n",
    "        BN向传播\n",
    "        :param x: 数据\n",
    "        :return: BN输出\n",
    "        \"\"\"\n",
    "        x_mean = x.mean(axis=0)\n",
    "        x_var = x.var(axis=0)\n",
    "        # 对应running_mean的更新公式\n",
    "        self._running_mean = (1-self._momentum)*x_mean + self._momentum*self._running_mean\n",
    "        self._running_var = (1-self._momentum)*x_var + self._momentum*self._running_var\n",
    "        # 对应论文中计算BN的公式\n",
    "        x_hat = (x-x_mean)/np.sqrt(x_var+self._eps)\n",
    "        y = self._gamma*x_hat + self._beta\n",
    "        return y\n",
    "\n",
    "my_bn = MyBN(momentum=0.01, eps=1e-5, num_features=5)\n",
    "bn_output = my_bn.batch_norm(x)\n",
    "print(bn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward [[ 0.         -0.5773349  -0.9045274  -1.3416355  -1.1470739 ]\n",
      " [ 0.          1.7320046   1.5075457   1.3416355   1.6059034 ]\n",
      " [ 0.         -0.5773349  -0.9045274   0.44721183 -0.22941478]\n",
      " [ 0.         -0.5773349   0.30150914 -0.44721183 -0.22941478]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,5) (3,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m'\u001b[39m, f_o)\n\u001b[1;32m     37\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m dx, dgamma, dbeta \u001b[39m=\u001b[39m batchnorm_backward(grad, cache)\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(dx)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(dgamma)\n",
      "Cell \u001b[0;32mIn[40], line 23\u001b[0m, in \u001b[0;36mbatchnorm_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     20\u001b[0m x_, gamma, x_minus_mean, var_plus_eps \u001b[39m=\u001b[39m cache\n\u001b[1;32m     22\u001b[0m \u001b[39m# calculate gradients\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m dgamma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(x_ \u001b[39m*\u001b[39;49m dout, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m dbeta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dout, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m dx_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(np\u001b[39m.\u001b[39mones((N,\u001b[39m1\u001b[39m)), gamma\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m dout\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,5) (3,5) "
     ]
    }
   ],
   "source": [
    "\n",
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "    # read some useful parameter\n",
    "    N, D = x.shape\n",
    "\n",
    "    # BN forward pass\n",
    "    sample_mean = x.mean(axis=0)\n",
    "    sample_var = x.var(axis=0)\n",
    "    x_ = (x - sample_mean) / np.sqrt(sample_var + eps)\n",
    "    out = gamma * x_ + beta\n",
    "\n",
    "    # storage variables for backward pass\n",
    "    cache = (x_, gamma, x - sample_mean, sample_var + eps)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    # extract variables\n",
    "    N, D = dout.shape\n",
    "    x_, gamma, x_minus_mean, var_plus_eps = cache\n",
    "\n",
    "    # calculate gradients\n",
    "    dgamma = np.sum(x_ * dout, axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "    dx_ = np.matmul(np.ones((N,1)), gamma.reshape((1, -1))) * dout\n",
    "    dx = N * dx_ - np.sum(dx_, axis=0) - x_ * np.sum(dx_ * x_, axis=0)\n",
    "    dx *= (1.0/N) / np.sqrt(var_plus_eps)\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "gamma = np.ones((5, ), dtype=\"float32\")\n",
    "beta = np.zeros((5, ), dtype=\"float32\")\n",
    "f_o, cache = batchnorm_forward(x, gamma, beta, 1e-5)\n",
    "print('forward', f_o)\n",
    "\n",
    "grad = np.ones((3, 5), dtype='float32')\n",
    "dx, dgamma, dbeta = batchnorm_backward(grad, cache)\n",
    "print(dx)\n",
    "print(dgamma)\n",
    "print(dbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61762686, 0.0342001 , 0.02221724, 0.07593597, 0.02961045,\n",
       "        0.02518505, 0.07787013, 0.04264032, 0.03133982, 0.04337406],\n",
       "       [0.07585606, 0.19268342, 0.05034063, 0.08103407, 0.09004115,\n",
       "        0.03654442, 0.09652534, 0.10700318, 0.0868849 , 0.18308683],\n",
       "       [0.13332502, 0.11849893, 0.0567541 , 0.10613299, 0.10576185,\n",
       "        0.04624202, 0.1092097 , 0.1020745 , 0.07403796, 0.14796295]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "x = np.array([\n",
    "    [2.60365, -0.290006, -0.721366, 0.507656, -0.434107, -0.595984, 0.532808, -0.0694342, -0.377345, -0.0523729],\n",
    "    [-0.124932, 0.807279, -0.534957, -0.0588999, 0.0464973, -0.855241, 0.116036, 0.219089, 0.0108147, 0.756191],\n",
    "    [0.41907, 0.301184, -0.434992, 0.190973, 0.18747, -0.639831, 0.21955, 0.151983, -0.169142, 0.523242]\n",
    "    ])\n",
    "\n",
    "m = softmax(x, axis=1)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de2997ba25086c1e82cd70d564ea0c2d9f27788cf106070172defeb4a05974d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
