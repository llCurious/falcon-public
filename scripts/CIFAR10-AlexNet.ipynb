{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import random\n",
    "\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_transform = True\n",
    "is_shuffle = False\n",
    "batch_size = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     ]) if not is_transform else transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "path = f'{os.path.expanduser(\"~\")}/Downloads'\n",
    "trainset = datasets.CIFAR10(root=path, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=is_shuffle)\n",
    "\n",
    "testset = datasets.CIFAR10(root=path, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "tensor([-0.5373, -0.6627, -0.6078, -0.4667, -0.2314, -0.0667,  0.0902,  0.1373,\n",
      "         0.1686,  0.1686,  0.0275, -0.0196,  0.1137,  0.1294,  0.0745,  0.0118,\n",
      "         0.0745,  0.0510, -0.0275,  0.0902,  0.0902,  0.0431,  0.0667,  0.0902,\n",
      "         0.1922,  0.2784,  0.3176,  0.2471,  0.2392,  0.2392,  0.1922,  0.1608])\n",
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = next(dataiter)\n",
    "# print(images[0]*255.0)\n",
    "\n",
    "# # get some random training images\n",
    "for i, x in enumerate(trainloader, 0):\n",
    "    img, label = x\n",
    "    print(img.shape)\n",
    "    print(img[0][0][0])\n",
    "    print(label[0])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_CryptGPU(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_CryptGPU, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=96, momentum=0),\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.BatchNorm2d(num_features=256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=256, momentum=0),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "class AlexNet_Falcon(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_Falcon, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 10),\n",
    "                # nn.ReLU(inplace=True),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "class AlexNet_Official(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_Official, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "class AlexNet_Official_modify(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_Official_modify, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=7, stride=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 2 * 2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AlexNet_SPDZ(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet_SPDZ, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(num_features=96, momentum=1),\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=256, momentum=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.BatchNorm2d(num_features=256),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 256, 1, 1]           --\n",
      "|    └─Conv2d: 2-1                       [-1, 96, 10, 10]          34,944\n",
      "|    └─ReLU: 2-2                         [-1, 96, 10, 10]          --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 96, 4, 4]            --\n",
      "|    └─BatchNorm2d: 2-4                  [-1, 96, 4, 4]            192\n",
      "|    └─Conv2d: 2-5                       [-1, 256, 2, 2]           614,656\n",
      "|    └─ReLU: 2-6                         [-1, 256, 2, 2]           --\n",
      "|    └─BatchNorm2d: 2-7                  [-1, 256, 2, 2]           512\n",
      "|    └─MaxPool2d: 2-8                    [-1, 256, 1, 1]           --\n",
      "|    └─Conv2d: 2-9                       [-1, 384, 1, 1]           885,120\n",
      "|    └─ReLU: 2-10                        [-1, 384, 1, 1]           --\n",
      "|    └─Conv2d: 2-11                      [-1, 384, 1, 1]           1,327,488\n",
      "|    └─ReLU: 2-12                        [-1, 384, 1, 1]           --\n",
      "|    └─Conv2d: 2-13                      [-1, 256, 1, 1]           884,992\n",
      "|    └─ReLU: 2-14                        [-1, 256, 1, 1]           --\n",
      "├─Sequential: 1-2                        [-1, 10]                  --\n",
      "|    └─Flatten: 2-15                     [-1, 256]                 --\n",
      "|    └─Linear: 2-16                      [-1, 256]                 65,792\n",
      "|    └─ReLU: 2-17                        [-1, 256]                 --\n",
      "|    └─Linear: 2-18                      [-1, 256]                 65,792\n",
      "|    └─ReLU: 2-19                        [-1, 256]                 --\n",
      "|    └─Linear: 2-20                      [-1, 10]                  2,570\n",
      "==========================================================================================\n",
      "Total params: 3,882,058\n",
      "Trainable params: 3,882,058\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 13.05\n",
      "==========================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 14.81\n",
      "Estimated Total Size (MB): 14.93\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "logger = SummaryWriter(log_dir = 'log')\n",
    "\n",
    "USE_SPDZ = True\n",
    "\n",
    "model_fn = AlexNet_SPDZ if USE_SPDZ else AlexNet_CryptGPU\n",
    "\n",
    "model = model_fn(num_classes=10)\n",
    "summary(model, (3, 32, 32))\n",
    "\n",
    "# GPU run\n",
    "gpu_list = [0, 1, 2, 3]\n",
    "gpu_list_str = ','.join(map(str, gpu_list))\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "def evaluate(model):\n",
    "    # model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch, data in enumerate(testloader, 0):\n",
    "        images, labels = data\n",
    "        output = model(images.to(device))\n",
    "        \n",
    "        correct += torch.sum(torch.argmax(output, dim=1) == labels.to(device))\n",
    "        total += len(images)\n",
    "    return correct * 1.0 / total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "epochs = 10\n",
    "\n",
    "lr = 1. / (1 << 4)\n",
    "use_softmax = False\n",
    "use_MSE = False\n",
    "\n",
    "momentum = 0.8\n",
    "# CE This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "criterion = nn.MSELoss() if use_MSE else nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "debug_epochs = 1\n",
    "debug_max_iter = 1\n",
    "\n",
    "# AlexNet Official\n",
    "# lr=0.001, momentum=0.9 converges about 2~3 epoch. Without momentum, it does not converge.\n",
    "# lr=0.01 (remove Dropout) converges about 3~4 epoch.\n",
    "\n",
    "# AlexNet Official Modified: MaxPooling --> AvgPooing, remove Dropout\n",
    "# lr=0.01 converges\n",
    "\n",
    "# AlexNet CryptGPU\n",
    "# lr=0.01. momentum=0.9 not converge. No BN, remove the last avgpooling layer.\n",
    "\n",
    "# AlexNet Falcon\n",
    "# lr=0.01 converges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "tensor([-0.5373, -0.6627, -0.6078, -0.4667, -0.2314, -0.0667,  0.0902,  0.1373,\n",
      "         0.1686,  0.1686,  0.0275, -0.0196,  0.1137,  0.1294,  0.0745,  0.0118,\n",
      "         0.0745,  0.0510, -0.0275,  0.0902,  0.0902,  0.0431,  0.0667,  0.0902,\n",
      "         0.1922,  0.2784,  0.3176,  0.2471,  0.2392,  0.2392,  0.1922,  0.1608])\n",
      "tensor(6)\n",
      "Use shuffle: False, use transform: True, batch size: 32, lr: 0.0625, model_SPDZ: True\n",
      "---------------0\n",
      "Batch 0, Loss: 2.290346145629883, Acc: 0.1875\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(trainloader, 0):\n",
    "    img, label = x\n",
    "    print(img.shape)\n",
    "    print(img[0][0][0])\n",
    "    print(label[0])\n",
    "    break\n",
    "\n",
    "print(f'Use shuffle: {is_shuffle}, use transform: {is_transform}, batch size: {batch_size}, lr: {lr}, model_SPDZ: {USE_SPDZ}')\n",
    "\n",
    "countMax = 0\n",
    "outputs = []\n",
    "for epoch in range(debug_epochs):\n",
    "    for batch, data in enumerate(trainloader, 0):\n",
    "        print(f'---------------{countMax}')\n",
    "        images, labels = data\n",
    "\n",
    "        x = images.to(device)\n",
    "        for i in range(len(model.features)):\n",
    "            # if isinstance(model.features[i], nn.Conv2d):\n",
    "            #     print(f'Conv2d input: {x[0]}')\n",
    "                # print(f'Conv2d weights: {model.features[i]}')\n",
    "            # elif isinstance(model.features[i], nn.ReLU):\n",
    "            #     print(f'ReLU input: {x[0]}')\n",
    "            # if isinstance(model.features[i], nn.MaxPool2d):\n",
    "            #     print(f'MaxPool2d input: {x[0]}')\n",
    "            # elif isinstance(model.features[i], nn.BatchNorm2d):\n",
    "            #     print(f'BatchNorm2d input: {x[0]}')\n",
    "            if i == 7:\n",
    "                maxPool_input = x\n",
    "                x = model.features[i](maxPool_input)\n",
    "            elif i == 6:\n",
    "                bn_input = x\n",
    "                x = model.features[i](bn_input)\n",
    "            elif i == 5:\n",
    "                relu_input_feature_2 = x\n",
    "                x = model.features[i](relu_input_feature_2)\n",
    "            elif i == 8:\n",
    "                cnn_input_3 = x\n",
    "                # print(f'CNN input: {cnn_input}')\n",
    "                x = model.features[i](cnn_input_3)\n",
    "                # print(f'CNN output: {x}')\n",
    "            elif i == 0:\n",
    "                cnn_input_1 = x\n",
    "                # print(f'CNN input: {cnn_input}')\n",
    "                x = model.features[i](cnn_input_1)\n",
    "                # print(f'CNN output: {x}')\n",
    "            elif i == 4:\n",
    "                cnn_input_2 = x\n",
    "                # print(f'CNN input: {cnn_input}')\n",
    "                x = model.features[i](cnn_input_2)\n",
    "                # print(f'CNN output: {x}')\n",
    "            elif i == 1:\n",
    "                relu_input_feature_1 = x\n",
    "                # print(f'CNN input: {cnn_input}')\n",
    "                x = model.features[i](relu_input_feature_1)\n",
    "                # print(f'CNN output: {x}')\n",
    "            elif i == 3:\n",
    "                bn_input_1 = x\n",
    "                x = model.features[i](bn_input_1)\n",
    "            elif i == 9:\n",
    "                relu_input_feature_3 = x\n",
    "                # print(f'ReLU input: {relu_input}')\n",
    "                x = model.features[i](relu_input_feature_3)\n",
    "                # print(f'ReLU output: {x}')\n",
    "            elif i == 11:\n",
    "                relu_input_11 = x\n",
    "                # print(f'ReLU input: {relu_input}')\n",
    "                x = model.features[i](relu_input_11)\n",
    "                # print(f'ReLU output: {x}')\n",
    "            elif i == 13:\n",
    "                relu_input_13 = x\n",
    "                # print(f'ReLU input: {relu_input}')\n",
    "                x = model.features[i](relu_input_13)\n",
    "                # print(f'ReLU output: {x}')\n",
    "            else:\n",
    "                x = model.features[i](x)\n",
    "            # if isinstance(model.features[i], nn.Conv2d):\n",
    "            #     print(f'Conv2d output: {x[0]}')\n",
    "            # elif isinstance(model.features[i], nn.ReLU):\n",
    "            #     print(f'ReLU output: {x[0]}')\n",
    "            # if isinstance(model.features[i], nn.MaxPool2d):\n",
    "            #     print(f'MaxPool2d output: {x[0]}')\n",
    "            # elif isinstance(model.features[i], nn.BatchNorm2d):\n",
    "            #     print(f'BatchNorm2d mean: {model.features[i].running_mean.data}')\n",
    "            #     print(f'BatchNorm2d var: {model.features[i].running_var.data}')\n",
    "            #     print(f'BatchNorm2d output: {x[0]}')\n",
    "        for i in range(len(model.fc_layers)):\n",
    "            # if isinstance(model.fc_layers[i], nn.Conv2d):\n",
    "            #     print(f'Conv2d input: {x[0]}')\n",
    "            #     print(f'Conv2d weights: {model.fc_layers[i]}')\n",
    "            # elif isinstance(model.fc_layers[i], nn.ReLU):\n",
    "            #     print(f'ReLU input: {x[0]}')\n",
    "            # elif isinstance(model.fc_layers[i], nn.MaxPool2d):\n",
    "            #     print(f'MaxPool2d input: {x[0]}')\n",
    "            # elif isinstance(model.fc_layers[i], nn.BatchNorm2d):\n",
    "            #     print(f'BatchNorm2d input: {x[0]}')\n",
    "            # if isinstance(model.fc_layers[i], nn.Linear):\n",
    "            #     print(f'Linear input: {x[0]}')\n",
    "            # 多了一层nn.Flatten()\n",
    "            if i == 2:\n",
    "                relu_input_1 = x\n",
    "                # print(f'Relu-1 input: {relu_input_1}')\n",
    "                x = model.fc_layers[i](relu_input_1)\n",
    "            elif i == 3:\n",
    "                fc_input_2 = x\n",
    "                # print(f'Relu-1 input: {relu_input_1}')\n",
    "                x = model.fc_layers[i](fc_input_2)\n",
    "            elif i == 1:\n",
    "                fc_input_1 = x\n",
    "                # print(f'Relu-1 input: {relu_input_1}')\n",
    "                x = model.fc_layers[i](fc_input_1)\n",
    "            # elif i == 5:\n",
    "            #     fc_input = x\n",
    "            #     x = model.fc_layers[i](fc_input)\n",
    "            elif i == 4:\n",
    "                relu_input_2 = x\n",
    "                # print(f'Relu-2 input: {relu_input_2}')\n",
    "                x = model.fc_layers[i](relu_input_2)\n",
    "                # print(f'Relu-2 output: {x}')\n",
    "            else:\n",
    "                x = model.fc_layers[i](x)\n",
    "            # if isinstance(model.fc_layers[i], nn.Conv2d):\n",
    "            #     print(f'Conv2d output: {x[0]}')\n",
    "            # elif isinstance(model.fc_layers[i], nn.ReLU):\n",
    "            #     print(f'ReLU output: {x[0]}')\n",
    "            # elif isinstance(model.fc_layers[i], nn.MaxPool2d):\n",
    "            #     print(f'MaxPool2d output: {x[0]}')\n",
    "            # elif isinstance(model.fc_layers[i], nn.BatchNorm2d):\n",
    "            #     print(f'BatchNorm2d mean: {model.features[i].running_mean.data}')\n",
    "            #     print(f'BatchNorm2d var: {model.features[i].running_var.data}')\n",
    "            #     print(f'BatchNorm2d output: {x[0]}')\n",
    "            if isinstance(model.fc_layers[i], nn.Linear):\n",
    "                logger.add_histogram(f'FC-Linear-output-{i}', x, countMax)\n",
    "                logger.add_histogram(f'FC-Linear-weight{i}', model.fc_layers[i].weight, countMax)\n",
    "                logger.add_histogram(f'FC-Linear-bias{i}', model.fc_layers[i].bias, countMax)\n",
    "                # print(f'Linear output: {x[0]}')\n",
    "        \n",
    "        # print('output: ', x.cpu()[0])\n",
    "        # print(\"softmax output: \", torch.nn.functional.softmax(x.cpu(), dim=1)[0])\n",
    "        # print(f'BN var: {bn_input.var([0, 2, 3])}')\n",
    "        # print(\"loss: \", (torch.nn.functional.softmax(x.cpu(), dim=1) - torch.nn.functional.one_hot(labels, num_classes=10).float())[0])\n",
    "        # print(\"grad: \", torch.sum(torch.nn.functional.softmax(x.cpu(), dim=1) - torch.nn.functional.one_hot(labels, num_classes=10).float(), dim=0))\n",
    "        outputs.append(x.cpu())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        maxPool_input.retain_grad()\n",
    "        cnn_input_3.retain_grad()\n",
    "        # fc_input.retain_grad()\n",
    "        relu_input_feature_3.retain_grad()\n",
    "        # BN_input.retain_grad()\n",
    "        # relu_input_2.retain_grad()\n",
    "        # relu_input_1.retain_grad()\n",
    "        # relu_input_11.retain_grad()\n",
    "        # relu_input_13.retain_grad()\n",
    "        # bn_input.retain_grad()\n",
    "        # fc_input_2.retain_grad()\n",
    "        # relu_input_feature_2.retain_grad()\n",
    "        bn_input_1.retain_grad()\n",
    "        cnn_input_2.retain_grad()\n",
    "        # output = x\n",
    "        # output.retain_grad()\n",
    "        loss = criterion(x, labels.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        # print(f'BN-2 delta: {(maxPool_input.grad*batch_size)[0]}')\n",
    "        # print(f'maxpool-2 delta: {(cnn_input.grad*batch_size)[0]}')\n",
    "        # print(f'Relu-6 delta: {BN_input.grad}')\n",
    "        # print(f'relu-4 grad: {fc_input.grad}')  # correct\n",
    "        # print(f'fc-3 grad {relu_input_4.grad.shape}: {relu_input_4.grad}')\n",
    "        # print(f'cnn-5 grad: {relu_input_13.grad.shape} {(relu_input_13.grad[0]*batch_size)}')\n",
    "        # print(f'cnn-4 grad: {relu_input_11.grad.shape} {(relu_input_11.grad[0]*batch_size)}')\n",
    "        # print(f'cnn-3 grad: {relu_input.grad.shape} {(relu_input.grad[0]*batch_size)}')\n",
    "        # print(f'relu-2 delta: {(bn_input.grad*batch_size)[0]}')\n",
    "        # print(f'fc-1 delta: {(relu_input_1.grad*batch_size)[0]}')\n",
    "        # print(f'fc-2 delta: {(relu_input_2.grad*batch_size)[0]}')\n",
    "        # print(f'relu-1 delta: {(fc_input_2.grad*batch_size)[0]}')\n",
    "        \n",
    "\n",
    "        # print(f'Theo output grad{torch.nn.functional.softmax(output) - torch.nn.functional.one_hot(labels.to(device), num_classes=10)}')\n",
    "        # print(f'Actual output grad: {output.grad}')\n",
    "\n",
    "        # for i in range(len(model.features)):\n",
    "        #     if isinstance(model.features[i], nn.Conv2d):\n",
    "        #         print(f'Conv2d weights grad: {model.features[i].weight.grad[0]}')\n",
    "        #         print(f'Conv2d bias grad: {model.features[i].bias.shape}, {model.features[i].bias.grad}')\n",
    "            \n",
    "        #     if isinstance(model.features[i], nn.BatchNorm2d):\n",
    "        #         print(f'BatchNorm2d weights grad: {model.features[i].weight.grad[0]}')\n",
    "        #         print(f'BatchNorm2d bias grad: {model.features[i].bias.shape}, {model.features[i].bias.grad}')\n",
    "        # print(f'BN weights grad: {(model.features[6].weight.grad/lr)}')\n",
    "        # print(f'BN bias grad: {(model.features[6].bias.grad*batch_size)}')\n",
    "\n",
    "        # print(f'CNN weights grad: {model.features[8].weight.grad}')\n",
    "        # for idx in range(3):\n",
    "        #     print(f'cnn-4 channel-{idx} grad: {(relu_input_11.grad*batch_size)[:, idx, :,:]}')\n",
    "\n",
    "        #     print(f'cnn-4 channel-{idx} grad sum: {(relu_input_11.grad*batch_size).sum([0, 2, 3])[idx]}')\n",
    "        # print(f'CNN-5 bias grad: {model.features[12].bias.grad.shape}, {model.features[12].bias.grad*batch_size}')\n",
    "        # print(f'CNN-4 bias grad: {model.features[10].bias.grad.shape}, {model.features[10].bias.grad*batch_size}')\n",
    "        # print(f'CNN-3 bias grad: {model.features[8].bias.grad*batch_size}')\n",
    "        \n",
    "\n",
    "        # for i in range(len(model.fc_layers)):\n",
    "        #     if isinstance(model.fc_layers[i], nn.Linear):\n",
    "        #         # print(f'Linear weights grad: {model.fc_layers[i].weight.grad[0]}')\n",
    "        #         print(f'Linear bias grad: {model.fc_layers[i].bias.grad}')\n",
    "        # print(f'Linear weights grad: {model.fc_layers[5].weight.grad}')\n",
    "        # print(f'Linear bias grad: {model.fc_layers[5].bias.grad}')\n",
    "        # print(f'Expect bias grad: {torch.sum(output.grad, dim=0)}')\n",
    "\n",
    "        \n",
    "        acc = torch.sum(torch.argmax(x, dim=1) == labels.to(device)) * 1.0 / len(labels.to(device))\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Batch {}, Loss: {}, Acc: {}'.format(countMax, loss.item(), acc))\n",
    "        # print(f'Input: {images[0]}')\n",
    "        # print(f'Output: {x[0]}')\n",
    "        countMax += 1\n",
    "        if countMax >= debug_max_iter:\n",
    "            break\n",
    "# torch.save((relu_input_feature_2.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn2-delta.pt\")\n",
    "# torch.save((relu_input_11.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn4-delta.pt\")\n",
    "# torch.save((relu_input_13.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn5-delta.pt\")\n",
    "torch.save((relu_input_feature_3.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn3-delta.pt\")\n",
    "# torch.save((relu_input_1.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-fc1-delta.pt\")\n",
    "# torch.save((fc_input_2.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-relu1-delta.pt\")\n",
    "torch.save((bn_input_1.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-mp1-delta.pt\")\n",
    "torch.save((cnn_input_2.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn1-delta.pt\")\n",
    "torch.save((maxPool_input.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn2-delta.pt\")\n",
    "# torch.save((fc_input_2), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-relu1-output.pt\")\n",
    "# torch.save((relu_input_1), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-relu1-input.pt\")\n",
    "torch.save((fc_input_1), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-fc1-input.pt\")\n",
    "# torch.save((cnn_input_1), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn1-input.pt\")\n",
    "torch.save((cnn_input_3), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn3-input.pt\")\n",
    "torch.save((cnn_input_2), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn2-input.pt\")\n",
    "torch.save((bn_input), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn2-input.pt\")\n",
    "torch.save((maxPool_input), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn2-output.pt\")\n",
    "torch.save((maxPool_input), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-mp2-input.pt\")\n",
    "torch.save((cnn_input_3), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-mp2-output.pt\")\n",
    "torch.save((bn_input_1), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn1-input.pt\")\n",
    "torch.save((cnn_input_2), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn1-output.pt\")\n",
    "torch.save((relu_input_feature_3), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn3-output.pt\")\n",
    "# torch.save((relu_input_feature_1), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-cnn1-output.pt\")\n",
    "# torch.save((relu_input_2.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-fc2-delta.pt\")\n",
    "torch.save((model.features[6].bias.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn2-bias_grad.pt\")\n",
    "torch.save((model.features[6].weight.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn2-weight_grad.pt\")\n",
    "torch.save((model.features[3].bias.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn1-bias_grad.pt\")\n",
    "torch.save((model.features[3].bias.grad*batch_size), os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-bn1-weight_grad.pt\")\n",
    "torch.save(outputs, os.path.expanduser(\"~\")+ \"/DNN/output/plaintext-log.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "tensor([-0.5373, -0.6627, -0.6078, -0.4667, -0.2314, -0.0667,  0.0902,  0.1373,\n",
      "         0.1686,  0.1686,  0.0275, -0.0196,  0.1137,  0.1294,  0.0745,  0.0118,\n",
      "         0.0745,  0.0510, -0.0275,  0.0902,  0.0902,  0.0431,  0.0667,  0.0902,\n",
      "         0.1922,  0.2784,  0.3176,  0.2471,  0.2392,  0.2392,  0.1922,  0.1608])\n",
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(trainloader, 0):\n",
    "    img, label = x\n",
    "    print(img.shape)\n",
    "    print(img[0][0][0])\n",
    "    print(label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch 0, Loss: 2.2982542514801025, Acc: 0.125\n",
      "Epoch: 0, Batch 100, Loss: 2.3020567893981934, Acc: 0.09375\n",
      "Epoch: 0, Batch 200, Loss: 2.304307222366333, Acc: 0.078125\n",
      "Epoch: 0, Batch 300, Loss: 2.300875663757324, Acc: 0.0625\n",
      "Epoch: 0, Batch 400, Loss: 2.302187204360962, Acc: 0.125\n",
      "Epoch: 0, Batch 500, Loss: 2.3046960830688477, Acc: 0.109375\n",
      "Epoch: 0, Batch 600, Loss: 2.3027138710021973, Acc: 0.125\n",
      "Epoch: 0, Batch 700, Loss: 2.296396017074585, Acc: 0.125\n",
      "Epoch: 0, Test Acc: 0.16089999675750732\n",
      "Epoch: 1, Batch 0, Loss: 2.297051429748535, Acc: 0.09375\n",
      "Epoch: 1, Batch 100, Loss: 2.1815133094787598, Acc: 0.1875\n",
      "Epoch: 1, Batch 200, Loss: 1.87067711353302, Acc: 0.25\n",
      "Epoch: 1, Batch 300, Loss: 2.122049331665039, Acc: 0.15625\n",
      "Epoch: 1, Batch 400, Loss: 1.917684555053711, Acc: 0.1875\n",
      "Epoch: 1, Batch 500, Loss: 1.837367296218872, Acc: 0.234375\n",
      "Epoch: 1, Batch 600, Loss: 1.8541369438171387, Acc: 0.1875\n",
      "Epoch: 1, Batch 700, Loss: 1.6632658243179321, Acc: 0.359375\n",
      "Epoch: 1, Test Acc: 0.2969000041484833\n",
      "Epoch: 2, Batch 0, Loss: 1.9081592559814453, Acc: 0.3125\n",
      "Epoch: 2, Batch 100, Loss: 1.698997139930725, Acc: 0.40625\n",
      "Epoch: 2, Batch 200, Loss: 1.4553673267364502, Acc: 0.375\n",
      "Epoch: 2, Batch 300, Loss: 1.7117071151733398, Acc: 0.296875\n",
      "Epoch: 2, Batch 400, Loss: 1.561631202697754, Acc: 0.484375\n",
      "Epoch: 2, Batch 500, Loss: 1.6978403329849243, Acc: 0.328125\n",
      "Epoch: 2, Batch 600, Loss: 1.6733300685882568, Acc: 0.265625\n",
      "Epoch: 2, Batch 700, Loss: 1.4755007028579712, Acc: 0.390625\n",
      "Epoch: 2, Test Acc: 0.3814999759197235\n",
      "Epoch: 3, Batch 0, Loss: 1.8549636602401733, Acc: 0.328125\n",
      "Epoch: 3, Batch 100, Loss: 1.4512052536010742, Acc: 0.46875\n",
      "Epoch: 3, Batch 200, Loss: 1.1593717336654663, Acc: 0.515625\n",
      "Epoch: 3, Batch 300, Loss: 1.6576720476150513, Acc: 0.28125\n",
      "Epoch: 3, Batch 400, Loss: 1.332352638244629, Acc: 0.578125\n",
      "Epoch: 3, Batch 500, Loss: 1.5214577913284302, Acc: 0.46875\n",
      "Epoch: 3, Batch 600, Loss: 1.3382618427276611, Acc: 0.5\n",
      "Epoch: 3, Batch 700, Loss: 1.2664225101470947, Acc: 0.53125\n",
      "Epoch: 3, Test Acc: 0.4431999921798706\n",
      "Epoch: 4, Batch 0, Loss: 1.4858886003494263, Acc: 0.515625\n",
      "Epoch: 4, Batch 100, Loss: 1.341946005821228, Acc: 0.515625\n",
      "Epoch: 4, Batch 200, Loss: 0.9226527214050293, Acc: 0.609375\n",
      "Epoch: 4, Batch 300, Loss: 1.4387013912200928, Acc: 0.328125\n",
      "Epoch: 4, Batch 400, Loss: 1.114928126335144, Acc: 0.59375\n",
      "Epoch: 4, Batch 500, Loss: 1.3551878929138184, Acc: 0.5\n",
      "Epoch: 4, Batch 600, Loss: 1.1505508422851562, Acc: 0.53125\n",
      "Epoch: 4, Batch 700, Loss: 1.1349893808364868, Acc: 0.625\n",
      "Epoch: 4, Test Acc: 0.49079999327659607\n",
      "Epoch: 5, Batch 0, Loss: 1.4031189680099487, Acc: 0.484375\n",
      "Epoch: 5, Batch 100, Loss: 1.2268106937408447, Acc: 0.53125\n",
      "Epoch: 5, Batch 200, Loss: 0.8432091474533081, Acc: 0.65625\n",
      "Epoch: 5, Batch 300, Loss: 1.1885908842086792, Acc: 0.484375\n",
      "Epoch: 5, Batch 400, Loss: 0.9633173942565918, Acc: 0.703125\n",
      "Epoch: 5, Batch 500, Loss: 1.1835299730300903, Acc: 0.5625\n",
      "Epoch: 5, Batch 600, Loss: 0.9418758749961853, Acc: 0.671875\n",
      "Epoch: 5, Batch 700, Loss: 1.026254415512085, Acc: 0.671875\n",
      "Epoch: 5, Test Acc: 0.5483999848365784\n",
      "Epoch: 6, Batch 0, Loss: 1.0340349674224854, Acc: 0.609375\n",
      "Epoch: 6, Batch 100, Loss: 1.0828897953033447, Acc: 0.65625\n",
      "Epoch: 6, Batch 200, Loss: 0.7275926470756531, Acc: 0.703125\n",
      "Epoch: 6, Batch 300, Loss: 1.0210258960723877, Acc: 0.578125\n",
      "Epoch: 6, Batch 400, Loss: 0.7875598073005676, Acc: 0.765625\n",
      "Epoch: 6, Batch 500, Loss: 1.024299144744873, Acc: 0.6875\n",
      "Epoch: 6, Batch 600, Loss: 0.8345880508422852, Acc: 0.703125\n",
      "Epoch: 6, Batch 700, Loss: 0.8284821510314941, Acc: 0.734375\n",
      "Epoch: 6, Test Acc: 0.5270000100135803\n",
      "Epoch: 7, Batch 0, Loss: 1.079429268836975, Acc: 0.671875\n",
      "Epoch: 7, Batch 100, Loss: 1.006909966468811, Acc: 0.6875\n",
      "Epoch: 7, Batch 200, Loss: 0.6553019285202026, Acc: 0.765625\n",
      "Epoch: 7, Batch 300, Loss: 1.0092427730560303, Acc: 0.546875\n",
      "Epoch: 7, Batch 400, Loss: 0.5917503833770752, Acc: 0.84375\n",
      "Epoch: 7, Batch 500, Loss: 0.9865456223487854, Acc: 0.71875\n",
      "Epoch: 7, Batch 600, Loss: 0.7268434166908264, Acc: 0.78125\n",
      "Epoch: 7, Batch 700, Loss: 0.8633319735527039, Acc: 0.796875\n",
      "Epoch: 7, Test Acc: 0.5715000033378601\n",
      "Epoch: 8, Batch 0, Loss: 1.1815568208694458, Acc: 0.65625\n",
      "Epoch: 8, Batch 100, Loss: 0.8518145084381104, Acc: 0.703125\n",
      "Epoch: 8, Batch 200, Loss: 0.596203088760376, Acc: 0.859375\n",
      "Epoch: 8, Batch 300, Loss: 0.8718895316123962, Acc: 0.65625\n",
      "Epoch: 8, Batch 400, Loss: 0.5086626410484314, Acc: 0.859375\n",
      "Epoch: 8, Batch 500, Loss: 0.8113255500793457, Acc: 0.671875\n",
      "Epoch: 8, Batch 600, Loss: 0.6562212705612183, Acc: 0.78125\n",
      "Epoch: 8, Batch 700, Loss: 0.8105847239494324, Acc: 0.765625\n",
      "Epoch: 8, Test Acc: 0.5884999632835388\n",
      "Epoch: 9, Batch 0, Loss: 0.7491979002952576, Acc: 0.75\n",
      "Epoch: 9, Batch 100, Loss: 0.7240690588951111, Acc: 0.734375\n",
      "Epoch: 9, Batch 200, Loss: 0.5124155282974243, Acc: 0.78125\n",
      "Epoch: 9, Batch 300, Loss: 0.6291036009788513, Acc: 0.75\n",
      "Epoch: 9, Batch 400, Loss: 0.43985462188720703, Acc: 0.84375\n",
      "Epoch: 9, Batch 500, Loss: 0.7736740708351135, Acc: 0.71875\n",
      "Epoch: 9, Batch 600, Loss: 0.6597946882247925, Acc: 0.8125\n",
      "Epoch: 9, Batch 700, Loss: 0.6625857353210449, Acc: 0.78125\n",
      "Epoch: 9, Test Acc: 0.6044999957084656\n"
     ]
    }
   ],
   "source": [
    "acc_file_path = (\n",
    "    os.path.expanduser(\"~\")\n",
    "    + \"/DNN/output/\"\n",
    "    + \"AlexNet_\"\n",
    "    + (\"SPDZ\" if USE_SPDZ else \"Falcon\")\n",
    "    + (\"_train_GPU\" if True else \"_train_CPU\")\n",
    "    + \"_\"\n",
    "    + \"CIFAR10\"\n",
    "    + \"_\"\n",
    "    + (\"Transform_\" if is_transform else \"\")\n",
    "    + (\"Shuffle_\" if is_shuffle else \"\")\n",
    "    + str(batch_size) + \"_\"\n",
    "    + str(lr) + \"_\"\n",
    "    + str(momentum) + \"_\"\n",
    "    + (\"MSE_\" if use_MSE else \"CE_\")\n",
    "    + \"acc\"\n",
    "    + \"_\" + str(epochs)\n",
    "    + '-epoch'\n",
    "    + \".txt\"\n",
    ")\n",
    "\n",
    "loss_file_path = (\n",
    "    os.path.expanduser(\"~\")\n",
    "    + \"/DNN/output/\"\n",
    "    + \"AlexNet_\"\n",
    "    + (\"SPDZ\" if USE_SPDZ else \"Falcon\")\n",
    "    + (\"_train_GPU\" if True else \"_train_CPU\")\n",
    "    + \"_\"\n",
    "    + \"CIFAR10\"\n",
    "    + \"_\"\n",
    "    + (\"Transform_\" if is_transform else \"\")\n",
    "    + (\"Shuffle_\" if is_shuffle else \"\")\n",
    "    + str(batch_size) + \"_\"\n",
    "    + str(lr) + \"_\"\n",
    "    + str(momentum) + \"_\"\n",
    "    + (\"MSE_\" if use_MSE else \"CE_\")\n",
    "    + \"loss\"\n",
    "    + \"_\" + str(epochs)\n",
    "    + '-epoch'\n",
    "    + \".txt\"\n",
    ")\n",
    "\n",
    "accF = open(acc_file_path, 'w')\n",
    "lossF = open(loss_file_path, 'w')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        output = model(images.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # MSEloss\n",
    "        if use_MSE:\n",
    "            t_labels = torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "        else:\n",
    "            t_labels = labels\n",
    "        loss = criterion(output, t_labels.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        acc = torch.sum(torch.argmax(output, dim=1) == labels.to(device)) * 1.0 / len(labels.to(device))\n",
    "        optimizer.step()\n",
    "\n",
    "        accF.write(f'{epoch * len(trainloader) + batch}\\t{acc*100.0}\\n')\n",
    "        lossF.write(f'{epoch * len(trainloader) + batch}\\t{loss.item()}\\n')\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch: {}, Batch {}, Loss: {}, Acc: {}'.format(epoch, batch, loss.item(), acc))\n",
    "        \n",
    "            # print(model.features[6].running_var.data[0:2])\n",
    "\n",
    "    test_acc = evaluate(model)\n",
    "    print('Epoch: {}, Test Acc: {}'.format(epoch, test_acc))\n",
    "accF.close()\n",
    "lossF.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sss = iter(testloader)\n",
    "# images, labels = test_sss.next()\n",
    "# images, labels = test_sss.next()\n",
    "# output = model(images)\n",
    "# _, output = torch.max(output, 1)\n",
    "\n",
    "# correct = 0\n",
    "# for i in range(len(output)):\n",
    "#     if output[i] == labels[i]:\n",
    "#         correct += 1\n",
    "# print(f\"acc: {correct * 1. / 128}\")\n",
    "# print(f\"Output: ${output}\")\n",
    "# print(f\"Target: ${labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Pretrained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "Layer features.0, type weight, shape (96, 3, 11, 11)\n",
      "features.0.bias\n",
      "Layer features.0, type bias, shape (96,)\n",
      "features.3.weight\n",
      "Layer features.3, type weight, shape (96,)\n",
      "features.3.bias\n",
      "Layer features.3, type bias, shape (96,)\n",
      "features.4.weight\n",
      "Layer features.4, type weight, shape (256, 96, 5, 5)\n",
      "features.4.bias\n",
      "Layer features.4, type bias, shape (256,)\n",
      "features.6.weight\n",
      "Layer features.6, type weight, shape (256,)\n",
      "features.6.bias\n",
      "Layer features.6, type bias, shape (256,)\n",
      "features.8.weight\n",
      "Layer features.8, type weight, shape (384, 256, 3, 3)\n",
      "features.8.bias\n",
      "Layer features.8, type bias, shape (384,)\n",
      "features.10.weight\n",
      "Layer features.10, type weight, shape (384, 384, 3, 3)\n",
      "features.10.bias\n",
      "Layer features.10, type bias, shape (384,)\n",
      "features.12.weight\n",
      "Layer features.12, type weight, shape (256, 384, 3, 3)\n",
      "features.12.bias\n",
      "Layer features.12, type bias, shape (256,)\n",
      "fc_layers.1.weight\n",
      "Layer fc_layers.1, type weight, shape (256, 256)\n",
      "fc_layers.1.bias\n",
      "Layer fc_layers.1, type bias, shape (256,)\n",
      "fc_layers.3.weight\n",
      "Layer fc_layers.3, type weight, shape (256, 256)\n",
      "fc_layers.3.bias\n",
      "Layer fc_layers.3, type bias, shape (256,)\n",
      "fc_layers.5.weight\n",
      "Layer fc_layers.5, type weight, shape (10, 256)\n",
      "fc_layers.5.bias\n",
      "Layer fc_layers.5, type bias, shape (10,)\n"
     ]
    }
   ],
   "source": [
    "params = [(name, p.data.cpu().numpy()) for (name, p) in model.named_parameters()]\n",
    "\n",
    "for (name, p) in params:\n",
    "    print(name)\n",
    "    print(f\"Layer {str(name.split('.')[0])+'.'+str(name.split('.')[1])}, type {name.split('.')[2]}, shape {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save parameters to /home/haoqi.whq/DNN/params/init/AlexNet/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "subdir = 'trained' if epochs != 0 else 'init'\n",
    "path = f\"{os.path.expanduser('~')}/DNN/params/{subdir}/AlexNet/\"\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "print(f'Save parameters to {path}')\n",
    "\n",
    "# print(f'cnn1_weight_0: {params[0][1].reshape(3*11*11, 96).tolist()}')\n",
    "np.savetxt(fname=path+\"cnn1_weight_0\", delimiter=\" \", X=params[0][1].reshape(3*11*11, 96).tolist())\n",
    "np.savetxt(fname=path+\"cnn1_bias_0\", delimiter=\" \",  X=params[1][1].tolist())\n",
    "np.savetxt(fname=path+\"bn1_gamma_0\", delimiter=\" \",  X=params[2][1].tolist())\n",
    "np.savetxt(fname=path+\"bn1_beta_0\", delimiter=\" \",  X=params[3][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn2_weight_0\", delimiter=\" \", X=params[4][1].reshape(96*5*5, 256).tolist())\n",
    "np.savetxt(fname=path+\"cnn2_bias_0\", delimiter=\" \", X=params[5][1].tolist())\n",
    "np.savetxt(fname=path+\"bn2_gamma_0\", delimiter=\" \",  X=params[6][1].tolist())\n",
    "np.savetxt(fname=path+\"bn2_beta_0\", delimiter=\" \",  X=params[7][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn3_weight_0\", delimiter=\" \", X=params[8][1].reshape(256*3*3, 384).tolist())\n",
    "np.savetxt(fname=path+\"cnn3_bias_0\", delimiter=\" \", X=params[9][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn4_weight_0\", delimiter=\" \", X=params[10][1].reshape(384*3*3, 384).tolist())\n",
    "np.savetxt(fname=path+\"cnn4_bias_0\", delimiter=\" \", X=params[11][1].tolist())\n",
    "np.savetxt(fname=path+\"cnn5_weight_0\", delimiter=\" \", X=params[12][1].reshape(384*3*3, 256).tolist())\n",
    "np.savetxt(fname=path+\"cnn5_bias_0\", delimiter=\" \", X=params[13][1].tolist())\n",
    "# FC\n",
    "np.savetxt(fname=path+\"fc1_weight_0\", delimiter=\" \", X=params[14][1].tolist())\n",
    "np.savetxt(fname=path+\"fc1_bias_0\", delimiter=\" \", X=params[15][1].tolist())\n",
    "np.savetxt(fname=path+\"fc2_weight_0\", delimiter=\" \", X=params[16][1].tolist())\n",
    "np.savetxt(fname=path+\"fc2_bias_0\", delimiter=\" \", X=params[17][1].tolist())\n",
    "np.savetxt(fname=path+\"fc3_weight_0\", delimiter=\" \", X=params[18][1].tolist())\n",
    "np.savetxt(fname=path+\"fc3_bias_0\", delimiter=\" \", X=params[19][1].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization Pseudocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BN\n",
      "forward\n",
      "[1.2499999e-04 1.8750000e-01 6.8750000e-01 1.2500000e+00 3.6187500e+01]\n",
      "Target:  [89.442726    2.309401    1.2060454   0.8944272   0.16623433]\n",
      "Compute:  [6.744339   2.3093176  1.2058791  0.8931092  0.16623433]\n",
      "[[-0.03372169 -0.5773294  -0.9044093  -1.3396637  -0.78961307]\n",
      " [ 0.0337217   1.7319882   1.5073489   1.3396637  -0.29091007]\n",
      " [ 0.10116509 -0.5773294  -0.9044093   0.4465546  -0.62337875]\n",
      " [-0.10116508 -0.5773294   0.30146977 -0.4465546   1.7039019 ]]\n",
      "[[-0.03372169 -0.5773294  -0.9044093  -1.3396637  -0.78961307]\n",
      " [ 0.0337217   1.7319882   1.5073489   1.3396637  -0.29091007]\n",
      " [ 0.10116509 -0.5773294  -0.9044093   0.4465546  -0.62337875]\n",
      " [-0.10116508 -0.5773294   0.30146977 -0.4465546   1.7039019 ]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "IT_N = 3\n",
    "def inverse_sqrt(x):\n",
    "    init_g = np.exp(-(x/2+0.2))*2.2 + 0.2\n",
    "    init_g -= x/1024\n",
    "\n",
    "    for i in range(IT_N):\n",
    "        init_g = init_g*(3-x * init_g * init_g)/2\n",
    "    return init_g\n",
    "    \n",
    "class BN:\n",
    "    def __init__(self, dims, gamma=0, beta=0):\n",
    "        print('BN')\n",
    "        self.dims = dims\n",
    "        self.eps = 0\n",
    "        self.gamma = np.ones((dims, ), dtype=\"float32\")\n",
    "        self.beta = np.zeros((dims, ), dtype=\"float32\")\n",
    "        \n",
    "        self.inv_sqrt = None\n",
    "        self.norm_x = None\n",
    "\n",
    "        self.beta_grad = None\n",
    "        self.gamma_grad = None\n",
    "        self.act_grad = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=0)   # 1 truncation by batchSize [1, D]\n",
    "        x_mean = x - mean   # [B, D]\n",
    "        var = np.mean(x_mean * x_mean, axis=0)  # 1 multiplication, 1 truncation by batchsize [1, D]\n",
    "        var_eps = var + self.eps\n",
    "\n",
    "        # protocol inv_sqrt\n",
    "        self.inv_sqrt = 1. / np.sqrt(var_eps)   # 1 inverse sqrt [1, D]\n",
    "        self.inv_sqrt = inverse_sqrt(var_eps)   # 1 inverse sqrt [1, D]\n",
    "        # print(\"======\")\n",
    "        print(var_eps)\n",
    "        print('Target: ', 1. / np.sqrt(var_eps))\n",
    "        print('Compute: ', inverse_sqrt(var_eps))\n",
    "        self.norm_x = x_mean * self.inv_sqrt    # 1 multiplication [B, D] * [1, D]. Falcon here has bug.\n",
    "        # print(self.inv_sqrt)\n",
    "        print(self.norm_x)\n",
    "\n",
    "        return self.gamma * self.norm_x + self.beta     # 1 multiplication\n",
    "\n",
    "    def backward(self, grad):\n",
    "        B, D = grad.shape\n",
    "        self.beta_grad = np.sum(grad, axis=0)\n",
    "        self.gamma_grad = np.sum(self.norm_x * grad, axis=0)    # 1 multiplication\n",
    "\n",
    "        dxhat = grad * self.gamma   # 1 multiplication\n",
    "\n",
    "        print('+' * 20)\n",
    "        print(B*dxhat)\n",
    "        # print(np.sum(dxhat, axis=0))\n",
    "        # print(np.sum(dxhat * self.norm_x, axis=0))\n",
    "        # print(self.norm_x * np.sum(dxhat * self.norm_x, axis=0))\n",
    "        print((B*dxhat - np.sum(dxhat, axis=0) - self.norm_x * np.sum(dxhat * self.norm_x, axis=0)))\n",
    "        self.act_grad = self.inv_sqrt * \\\n",
    "                        (B*dxhat - np.sum(dxhat, axis=0) - self.norm_x * np.sum(dxhat * self.norm_x, axis=0)) \\\n",
    "                        / B # 3 multiplication, 1 truncation\n",
    "\n",
    "        return self.act_grad, self.gamma_grad, self.beta_grad\n",
    "\n",
    "bn = BN(5)\n",
    "\n",
    "data = np.array([[0.02, 2, 3, 4, 5],\n",
    "                 [0.03, 3, 5, 7, 8],\n",
    "                 [0.04, 2, 3, 6, 6],\n",
    "                 [0.01, 2, 4, 5, 20]]).astype(np.float32)\n",
    "x_raw = torch.from_numpy(data)\n",
    "x = x_raw.numpy()\n",
    "\n",
    "grad = np.array([[1, 2, 3, 4, 5],\n",
    "                 [1, 3, 5, 7, 8],\n",
    "                 [1, 2, 3, 6, 6],\n",
    "                 [1, 2, 4, 5, 6]]).astype(np.float32)\n",
    "# grad = grad * 1024\n",
    "\n",
    "print(\"forward\")\n",
    "f_o = bn.forward(x)\n",
    "print(f_o)\n",
    "\n",
    "# print(\"backward\")\n",
    "# b_o = bn.backward(grad)\n",
    "# print(b_o)\n",
    "\n",
    "# print(inverse_sqrt(np.array([16, 100, 0.01])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.43033141 -0.57733488 -0.90452743 -1.34163547 -0.78961295]\n",
      " [ 0.43033159  1.73200464  1.50754571  1.34163547 -0.29091004]\n",
      " [ 1.29099452 -0.57733488 -0.90452743  0.44721183 -0.62337863]\n",
      " [-1.29099441 -0.57733488  0.30150914 -0.44721183  1.70390165]]\n"
     ]
    }
   ],
   "source": [
    "class MyBN:\n",
    "    def __init__(self, momentum, eps, num_features):\n",
    "        \"\"\"\n",
    "        初始化参数值\n",
    "        :param momentum: 追踪样本整体均值和方差的动量\n",
    "        :param eps: 防止数值计算错误\n",
    "        :param num_features: 特征数量\n",
    "        \"\"\"\n",
    "        # 对每个batch的mean和var进行追踪统计\n",
    "        self._running_mean = 0\n",
    "        self._running_var = 1\n",
    "        # 更新self._running_xxx时的动量\n",
    "        self._momentum = momentum\n",
    "        # 防止分母计算为0\n",
    "        self._eps = eps\n",
    "        # 对应论文中需要更新的beta和gamma，采用pytorch文档中的初始化值\n",
    "        self._beta = np.zeros(shape=(num_features, ))\n",
    "        self._gamma = np.ones(shape=(num_features, ))\n",
    "\n",
    "    def batch_norm(self, x):\n",
    "        \"\"\"\n",
    "        BN向传播\n",
    "        :param x: 数据\n",
    "        :return: BN输出\n",
    "        \"\"\"\n",
    "        x_mean = x.mean(axis=0)\n",
    "        x_var = x.var(axis=0)\n",
    "        # 对应running_mean的更新公式\n",
    "        self._running_mean = (1-self._momentum)*x_mean + self._momentum*self._running_mean\n",
    "        self._running_var = (1-self._momentum)*x_var + self._momentum*self._running_var\n",
    "        # 对应论文中计算BN的公式\n",
    "        x_hat = (x-x_mean)/np.sqrt(x_var+self._eps)\n",
    "        y = self._gamma*x_hat + self._beta\n",
    "        return y\n",
    "\n",
    "my_bn = MyBN(momentum=0.01, eps=1e-5, num_features=5)\n",
    "bn_output = my_bn.batch_norm(x)\n",
    "print(bn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.9448,  1.1164]],\n",
      "\n",
      "         [[-0.6701, -0.7322]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4516,  0.5333]],\n",
      "\n",
      "         [[ 1.6078, -0.7906]]]])\n",
      "tensor([ 0.7615, -0.1463])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.1026, 1.3700])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 2, 1, 2)\n",
    "print(input)\n",
    "print(torch.mean(input, [0, 2, 3]))\n",
    "\n",
    "input.var([0,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward [[-0.4303314  -0.5773349  -0.9045274  -1.3416355  -0.78961295]\n",
      " [ 0.4303316   1.7320046   1.5075457   1.3416355  -0.29091004]\n",
      " [ 1.2909945  -0.5773349  -0.9045274   0.44721183 -0.62337863]\n",
      " [-1.2909944  -0.5773349   0.30150914 -0.44721183  1.7039016 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,5) (3,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m'\u001b[39m, f_o)\n\u001b[1;32m     37\u001b[0m grad \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m3\u001b[39m, \u001b[39m5\u001b[39m), dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m dx, dgamma, dbeta \u001b[39m=\u001b[39m batchnorm_backward(grad, cache)\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(dx)\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(dgamma)\n",
      "Cell \u001b[0;32mIn[254], line 23\u001b[0m, in \u001b[0;36mbatchnorm_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     20\u001b[0m x_, gamma, x_minus_mean, var_plus_eps \u001b[39m=\u001b[39m cache\n\u001b[1;32m     22\u001b[0m \u001b[39m# calculate gradients\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m dgamma \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(x_ \u001b[39m*\u001b[39;49m dout, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m dbeta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(dout, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     26\u001b[0m dx_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(np\u001b[39m.\u001b[39mones((N,\u001b[39m1\u001b[39m)), gamma\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))) \u001b[39m*\u001b[39m dout\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,5) (3,5) "
     ]
    }
   ],
   "source": [
    "\n",
    "def batchnorm_forward(x, gamma, beta, eps):\n",
    "    # read some useful parameter\n",
    "    N, D = x.shape\n",
    "\n",
    "    # BN forward pass\n",
    "    sample_mean = x.mean(axis=0)\n",
    "    sample_var = x.var(axis=0)\n",
    "    x_ = (x - sample_mean) / np.sqrt(sample_var + eps)\n",
    "    out = gamma * x_ + beta\n",
    "\n",
    "    # storage variables for backward pass\n",
    "    cache = (x_, gamma, x - sample_mean, sample_var + eps)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def batchnorm_backward(dout, cache):\n",
    "    # extract variables\n",
    "    N, D = dout.shape\n",
    "    x_, gamma, x_minus_mean, var_plus_eps = cache\n",
    "\n",
    "    # calculate gradients\n",
    "    dgamma = np.sum(x_ * dout, axis=0)\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "\n",
    "    dx_ = np.matmul(np.ones((N,1)), gamma.reshape((1, -1))) * dout\n",
    "    dx = N * dx_ - np.sum(dx_, axis=0) - x_ * np.sum(dx_ * x_, axis=0)\n",
    "    dx *= (1.0/N) / np.sqrt(var_plus_eps)\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "gamma = np.ones((5, ), dtype=\"float32\")\n",
    "beta = np.zeros((5, ), dtype=\"float32\")\n",
    "f_o, cache = batchnorm_forward(x, gamma, beta, 1e-5)\n",
    "print('forward', f_o)\n",
    "\n",
    "grad = np.ones((3, 5), dtype='float32')\n",
    "dx, dgamma, dbeta = batchnorm_backward(grad, cache)\n",
    "print(dx)\n",
    "print(dgamma)\n",
    "print(dbeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_batchnorm2d(input, output, grad_output, layer):\n",
    "    gamma = layer.weight\n",
    "    gamma = gamma.view(1,-1,1,1) # edit\n",
    "    # beta = layer.bias\n",
    "    # avg = layer.running_mean\n",
    "    # var = layer.running_var\n",
    "    eps = layer.eps\n",
    "    B = input.shape[0] * input.shape[2] * input.shape[3] # edit\n",
    "\n",
    "    # add new\n",
    "    mean = input.mean(dim = (0,2,3), keepdim = True)\n",
    "    variance = input.var(dim = (0,2,3), unbiased=False, keepdim = True)\n",
    "    x_hat = (input - mean)/(torch.sqrt(variance + eps))\n",
    "    \n",
    "    inv_sqrt = 1.0 / torch.sqrt(variance + eps)\n",
    "\n",
    "    dL_dxi_hat = grad_output * gamma\n",
    "    # dL_dvar = (-0.5 * dL_dxi_hat * (input - avg) / ((var + eps) ** 1.5)).sum((0, 2, 3), keepdim=True) \n",
    "    # dL_davg = (-1.0 / torch.sqrt(var + eps) * dL_dxi_hat).sum((0, 2, 3), keepdim=True) + dL_dvar * (-2.0 * (input - avg)).sum((0, 2, 3), keepdim=True) / B\n",
    "    dL_dvar = (-0.5 * dL_dxi_hat * (input - mean)).sum((0, 2, 3), keepdim=True)  * ((variance + eps) ** -1.5) # edit\n",
    "    dL_davg = (-inv_sqrt * dL_dxi_hat).sum((0, 2, 3), keepdim=True) + (dL_dvar * (-2.0 * (input - mean)).sum((0, 2, 3), keepdim=True) / B) #edit\n",
    "    \n",
    "    dL_dxi = (dL_dxi_hat / torch.sqrt(variance + eps)) + (2.0 * dL_dvar * (input - mean) / B) + (dL_davg / B) # dL_dxi_hat / sqrt()\n",
    "    # dL_dgamma = (grad_output * output).sum((0, 2, 3), keepdim=True) \n",
    "    dL_dgamma = (grad_output * x_hat).sum((0, 2, 3), keepdim=True) # edit\n",
    "    dL_dbeta = (grad_output).sum((0, 2, 3), keepdim=True)\n",
    "    return dL_dxi, dL_dgamma, dL_dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61762686, 0.0342001 , 0.02221724, 0.07593597, 0.02961045,\n",
       "        0.02518505, 0.07787013, 0.04264032, 0.03133982, 0.04337406],\n",
       "       [0.07585606, 0.19268342, 0.05034063, 0.08103407, 0.09004115,\n",
       "        0.03654442, 0.09652534, 0.10700318, 0.0868849 , 0.18308683],\n",
       "       [0.13332502, 0.11849893, 0.0567541 , 0.10613299, 0.10576185,\n",
       "        0.04624202, 0.1092097 , 0.1020745 , 0.07403796, 0.14796295]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "x = np.array([\n",
    "    [2.60365, -0.290006, -0.721366, 0.507656, -0.434107, -0.595984, 0.532808, -0.0694342, -0.377345, -0.0523729],\n",
    "    [-0.124932, 0.807279, -0.534957, -0.0588999, 0.0464973, -0.855241, 0.116036, 0.219089, 0.0108147, 0.756191],\n",
    "    [0.41907, 0.301184, -0.434992, 0.190973, 0.18747, -0.639831, 0.21955, 0.151983, -0.169142, 0.523242]\n",
    "    ])\n",
    "\n",
    "m = softmax(x, axis=1)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_N = 3\n",
    "def inverse_sqrt(x):\n",
    "    init_g = np.exp(-(x/2+0.2))*2.2 + 0.2\n",
    "    init_g -= x/1024\n",
    "\n",
    "    for i in range(IT_N):\n",
    "        init_g = init_g*(3-x * init_g * init_g)/2\n",
    "    return init_g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "falcon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de2997ba25086c1e82cd70d564ea0c2d9f27788cf106070172defeb4a05974d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
