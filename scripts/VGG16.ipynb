{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.vgg = models.vgg16(pretrained=False)\n",
    "        \n",
    "        self.vgg.features[4] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False)\n",
    "        self.vgg.features[9] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0,  ceil_mode=False)\n",
    "        self.vgg.features[16] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0,  ceil_mode=False)\n",
    "        self.vgg.features[23] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0,  ceil_mode=False)\n",
    "        self.vgg.features[30] = nn.AvgPool2d(kernel_size=2, stride=2, padding=0,  ceil_mode=False)\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(4608, 4096),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.Linear(4096, 1000)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class VGG16_Falcon(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16_Falcon, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(4608, 4096),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.Linear(4096, 1000)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch 0, Loss: 2.3094210624694824\n",
      "Epoch: 0, Batch 100, Loss: 2.297933340072632\n",
      "Epoch: 0, Batch 200, Loss: 2.304176092147827\n",
      "Epoch: 0, Batch 300, Loss: 2.304649591445923\n",
      "Epoch: 0, Batch 400, Loss: 2.3056483268737793\n",
      "Epoch: 0, Batch 500, Loss: 2.3006937503814697\n",
      "Epoch: 0, Batch 600, Loss: 2.2962560653686523\n",
      "Epoch: 0, Batch 700, Loss: 2.297899007797241\n",
      "Epoch: 0, Batch 800, Loss: 2.3032522201538086\n",
      "Epoch: 0, Batch 900, Loss: 2.302910327911377\n",
      "Epoch: 0, Batch 1000, Loss: 2.3034443855285645\n",
      "Epoch: 0, Batch 1100, Loss: 2.2999305725097656\n",
      "Epoch: 0, Batch 1200, Loss: 2.3051376342773438\n",
      "Epoch: 0, Batch 1300, Loss: 2.303309202194214\n",
      "Epoch: 0, Batch 1400, Loss: 2.300313949584961\n",
      "Epoch: 0, Batch 1500, Loss: 2.304264545440674\n",
      "Epoch: 1, Batch 0, Loss: 2.303295850753784\n",
      "Epoch: 1, Batch 100, Loss: 2.301085948944092\n",
      "Epoch: 1, Batch 200, Loss: 2.3017776012420654\n",
      "Epoch: 1, Batch 300, Loss: 2.3042843341827393\n",
      "Epoch: 1, Batch 400, Loss: 2.3008060455322266\n",
      "Epoch: 1, Batch 500, Loss: 2.304126262664795\n",
      "Epoch: 1, Batch 600, Loss: 2.302398204803467\n",
      "Epoch: 1, Batch 700, Loss: 2.306032657623291\n",
      "Epoch: 1, Batch 800, Loss: 2.3029627799987793\n",
      "Epoch: 1, Batch 900, Loss: 2.303208112716675\n",
      "Epoch: 1, Batch 1000, Loss: 2.3041911125183105\n",
      "Epoch: 1, Batch 1100, Loss: 2.302325487136841\n",
      "Epoch: 1, Batch 1200, Loss: 2.30220103263855\n",
      "Epoch: 1, Batch 1300, Loss: 2.307149887084961\n",
      "Epoch: 1, Batch 1400, Loss: 2.3032076358795166\n",
      "Epoch: 1, Batch 1500, Loss: 2.3010048866271973\n",
      "Epoch: 2, Batch 0, Loss: 2.3028714656829834\n",
      "Epoch: 2, Batch 100, Loss: 2.301422595977783\n",
      "Epoch: 2, Batch 200, Loss: 2.2998790740966797\n",
      "Epoch: 2, Batch 300, Loss: 2.3029022216796875\n",
      "Epoch: 2, Batch 400, Loss: 2.301372528076172\n",
      "Epoch: 2, Batch 500, Loss: 2.3048665523529053\n",
      "Epoch: 2, Batch 600, Loss: 2.301272392272949\n",
      "Epoch: 2, Batch 700, Loss: 2.2995693683624268\n",
      "Epoch: 2, Batch 800, Loss: 2.3027377128601074\n",
      "Epoch: 2, Batch 900, Loss: 2.303652286529541\n",
      "Epoch: 2, Batch 1000, Loss: 2.304652214050293\n",
      "Epoch: 2, Batch 1100, Loss: 2.300554037094116\n",
      "Epoch: 2, Batch 1200, Loss: 2.3012328147888184\n",
      "Epoch: 2, Batch 1300, Loss: 2.3039064407348633\n",
      "Epoch: 2, Batch 1400, Loss: 2.3018054962158203\n",
      "Epoch: 2, Batch 1500, Loss: 2.3036246299743652\n",
      "Epoch: 3, Batch 0, Loss: 2.304886817932129\n",
      "Epoch: 3, Batch 100, Loss: 2.302375555038452\n",
      "Epoch: 3, Batch 200, Loss: 2.300593137741089\n",
      "Epoch: 3, Batch 300, Loss: 2.3036396503448486\n",
      "Epoch: 3, Batch 400, Loss: 2.308330535888672\n",
      "Epoch: 3, Batch 500, Loss: 2.3027374744415283\n",
      "Epoch: 3, Batch 600, Loss: 2.300783157348633\n",
      "Epoch: 3, Batch 700, Loss: 2.2955496311187744\n",
      "Epoch: 3, Batch 800, Loss: 2.3027899265289307\n",
      "Epoch: 3, Batch 900, Loss: 2.302305221557617\n",
      "Epoch: 3, Batch 1000, Loss: 2.304854154586792\n",
      "Epoch: 3, Batch 1100, Loss: 2.302621841430664\n",
      "Epoch: 3, Batch 1200, Loss: 2.3021397590637207\n",
      "Epoch: 3, Batch 1300, Loss: 2.3045828342437744\n",
      "Epoch: 3, Batch 1400, Loss: 2.3057239055633545\n",
      "Epoch: 3, Batch 1500, Loss: 2.304276943206787\n",
      "Epoch: 4, Batch 0, Loss: 2.3028528690338135\n",
      "Epoch: 4, Batch 100, Loss: 2.301762580871582\n",
      "Epoch: 4, Batch 200, Loss: 2.304373025894165\n",
      "Epoch: 4, Batch 300, Loss: 2.2994041442871094\n",
      "Epoch: 4, Batch 400, Loss: 2.303619861602783\n",
      "Epoch: 4, Batch 500, Loss: 2.301020383834839\n",
      "Epoch: 4, Batch 600, Loss: 2.3049967288970947\n",
      "Epoch: 4, Batch 700, Loss: 2.302729845046997\n",
      "Epoch: 4, Batch 800, Loss: 2.304490804672241\n",
      "Epoch: 4, Batch 900, Loss: 2.3011863231658936\n",
      "Epoch: 4, Batch 1000, Loss: 2.303534746170044\n",
      "Epoch: 4, Batch 1100, Loss: 2.3039560317993164\n",
      "Epoch: 4, Batch 1200, Loss: 2.302171230316162\n",
      "Epoch: 4, Batch 1300, Loss: 2.302718162536621\n",
      "Epoch: 4, Batch 1400, Loss: 2.3023269176483154\n",
      "Epoch: 4, Batch 1500, Loss: 2.3027632236480713\n",
      "Epoch: 5, Batch 0, Loss: 2.302321434020996\n",
      "Epoch: 5, Batch 100, Loss: 2.3015189170837402\n",
      "Epoch: 5, Batch 200, Loss: 2.3025994300842285\n",
      "Epoch: 5, Batch 300, Loss: 2.302067756652832\n",
      "Epoch: 5, Batch 400, Loss: 2.298746347427368\n",
      "Epoch: 5, Batch 500, Loss: 2.302302360534668\n",
      "Epoch: 5, Batch 600, Loss: 2.3077595233917236\n",
      "Epoch: 5, Batch 700, Loss: 2.300311803817749\n",
      "Epoch: 5, Batch 800, Loss: 2.300114154815674\n",
      "Epoch: 5, Batch 900, Loss: 2.3023698329925537\n",
      "Epoch: 5, Batch 1000, Loss: 2.304206371307373\n",
      "Epoch: 5, Batch 1100, Loss: 2.2996654510498047\n",
      "Epoch: 5, Batch 1200, Loss: 2.301020860671997\n",
      "Epoch: 5, Batch 1300, Loss: 2.304640531539917\n",
      "Epoch: 5, Batch 1400, Loss: 2.301212787628174\n",
      "Epoch: 5, Batch 1500, Loss: 2.3009021282196045\n",
      "Epoch: 6, Batch 0, Loss: 2.3040623664855957\n",
      "Epoch: 6, Batch 100, Loss: 2.3033547401428223\n",
      "Epoch: 6, Batch 200, Loss: 2.3022725582122803\n",
      "Epoch: 6, Batch 300, Loss: 2.3052213191986084\n",
      "Epoch: 6, Batch 400, Loss: 2.3016796112060547\n",
      "Epoch: 6, Batch 500, Loss: 2.297196865081787\n",
      "Epoch: 6, Batch 600, Loss: 2.301884651184082\n",
      "Epoch: 6, Batch 700, Loss: 2.3053863048553467\n",
      "Epoch: 6, Batch 800, Loss: 2.3016905784606934\n",
      "Epoch: 6, Batch 900, Loss: 2.303040027618408\n",
      "Epoch: 6, Batch 1000, Loss: 2.302703857421875\n",
      "Epoch: 6, Batch 1100, Loss: 2.304318904876709\n",
      "Epoch: 6, Batch 1200, Loss: 2.3003785610198975\n",
      "Epoch: 6, Batch 1300, Loss: 2.3011579513549805\n",
      "Epoch: 6, Batch 1400, Loss: 2.303492307662964\n",
      "Epoch: 6, Batch 1500, Loss: 2.300830364227295\n",
      "Epoch: 7, Batch 0, Loss: 2.3021469116210938\n",
      "Epoch: 7, Batch 100, Loss: 2.3020355701446533\n",
      "Epoch: 7, Batch 200, Loss: 2.303729772567749\n",
      "Epoch: 7, Batch 300, Loss: 2.304446220397949\n",
      "Epoch: 7, Batch 400, Loss: 2.3012702465057373\n",
      "Epoch: 7, Batch 500, Loss: 2.303882360458374\n",
      "Epoch: 7, Batch 600, Loss: 2.3020684719085693\n",
      "Epoch: 7, Batch 700, Loss: 2.3023369312286377\n",
      "Epoch: 7, Batch 800, Loss: 2.3010916709899902\n",
      "Epoch: 7, Batch 900, Loss: 2.302917957305908\n",
      "Epoch: 7, Batch 1000, Loss: 2.30322265625\n",
      "Epoch: 7, Batch 1100, Loss: 2.3021373748779297\n",
      "Epoch: 7, Batch 1200, Loss: 2.302419662475586\n",
      "Epoch: 7, Batch 1300, Loss: 2.301534652709961\n",
      "Epoch: 7, Batch 1400, Loss: 2.3040924072265625\n",
      "Epoch: 7, Batch 1500, Loss: 2.302173137664795\n",
      "Epoch: 8, Batch 0, Loss: 2.299781084060669\n",
      "Epoch: 8, Batch 100, Loss: 2.3032357692718506\n",
      "Epoch: 8, Batch 200, Loss: 2.301248788833618\n",
      "Epoch: 8, Batch 300, Loss: 2.3049070835113525\n",
      "Epoch: 8, Batch 400, Loss: 2.302238941192627\n",
      "Epoch: 8, Batch 500, Loss: 2.3032562732696533\n",
      "Epoch: 8, Batch 600, Loss: 2.300133466720581\n",
      "Epoch: 8, Batch 700, Loss: 2.302960157394409\n",
      "Epoch: 8, Batch 800, Loss: 2.3031458854675293\n",
      "Epoch: 8, Batch 900, Loss: 2.3046257495880127\n",
      "Epoch: 8, Batch 1000, Loss: 2.304513454437256\n",
      "Epoch: 8, Batch 1100, Loss: 2.3011157512664795\n",
      "Epoch: 8, Batch 1200, Loss: 2.3006393909454346\n",
      "Epoch: 8, Batch 1300, Loss: 2.3048079013824463\n",
      "Epoch: 8, Batch 1400, Loss: 2.3019959926605225\n",
      "Epoch: 8, Batch 1500, Loss: 2.302981376647949\n",
      "Epoch: 9, Batch 0, Loss: 2.303706645965576\n",
      "Epoch: 9, Batch 100, Loss: 2.3016912937164307\n",
      "Epoch: 9, Batch 200, Loss: 2.3027658462524414\n",
      "Epoch: 9, Batch 300, Loss: 2.301114559173584\n",
      "Epoch: 9, Batch 400, Loss: 2.301964282989502\n",
      "Epoch: 9, Batch 500, Loss: 2.3013620376586914\n",
      "Epoch: 9, Batch 600, Loss: 2.3009164333343506\n",
      "Epoch: 9, Batch 700, Loss: 2.306180715560913\n",
      "Epoch: 9, Batch 800, Loss: 2.3072996139526367\n",
      "Epoch: 9, Batch 900, Loss: 2.307016611099243\n",
      "Epoch: 9, Batch 1000, Loss: 2.302168130874634\n",
      "Epoch: 9, Batch 1100, Loss: 2.3014607429504395\n",
      "Epoch: 9, Batch 1200, Loss: 2.30373215675354\n",
      "Epoch: 9, Batch 1300, Loss: 2.3034331798553467\n",
      "Epoch: 9, Batch 1400, Loss: 2.3019022941589355\n",
      "Epoch: 9, Batch 1500, Loss: 2.300661325454712\n",
      "Output: $tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Target: $tensor([2, 0, 8, 8, 1, 6, 2, 3, 5, 4, 0, 8, 9, 3, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "logger = SummaryWriter(log_dir = 'log')\n",
    "\n",
    "\n",
    "model = VGG16()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) \n",
    "# VGG16 CryptGPU\n",
    "# lr=0.001, momentum=0.9 converges about 2~3 epoch. Without momentum, it does not converge.\n",
    "# lr=0.01 (remove Dropout) converges about 3~4 epoch.\n",
    "\n",
    "# VGG16 Official\n",
    "# lr=0.01 \n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "it = 10\n",
    "for epoch in range(it):\n",
    "    for batch, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        output = model(images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch: {}, Batch {}, Loss: {}'.format(epoch, batch, loss.item()))\n",
    "\n",
    "output = model(images)\n",
    "_, output = torch.max(output, 1)\n",
    "print(f\"Output: ${output}\")\n",
    "print(f\"Target: ${labels}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8e35784da7b5309346b9948007e2c8e432cfca57e220859a198433798687959"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
