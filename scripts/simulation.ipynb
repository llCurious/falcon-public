{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f886b557a50>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training configs\n",
    "batch_size = 64\n",
    "bitw_forward = 32\n",
    "bitf_forward = 12\n",
    "\n",
    "bitw_backward = 64\n",
    "bitf_backward = 20\n",
    "\n",
    "re_scale = False\n",
    "\n",
    "use_LN = False\n",
    "\n",
    "quant = True\n",
    "w_linear = True\n",
    "w_conv = True\n",
    "\n",
    "# SecureML, MiniONN, LeNet, AlexNet, VGG16\n",
    "model_name = \"VGG16\"\n",
    "\n",
    "shifting_scale = 1.\n",
    "\n",
    "# ds = 'CIFAR-10'\n",
    "ds = 'CIFAR-10' if model_name in ['AlexNet', 'VGG16'] else 'MNIST'\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\"\"\"\n",
    "make sure all the random initiation of parameters are the same in different runs\n",
    "\"\"\"\n",
    "torch.manual_seed(0)\n",
    "# print(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Quantized Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overflow_validate(val, bitw, location):\n",
    "    if val.abs().max() >= float(2**(bitw-1)):\n",
    "        raise Exception(\"Overflow {4}! Val: {0}, Bit-width: {1}, val: [-{2}, {3}]\".format(val.abs().max(), bitw, 1 << (bitw-1), (1 << (bitw-1)) - 1, location))\n",
    "\n",
    "class FxInput(InplaceFunction):\n",
    "    \"\"\"\n",
    "    Quantize w.r.t. qmax in forward pass; use STE in backward pass\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitw_backward, bitf_forward, bitf_backward, inplace=False, trunc=False):\n",
    "        ctx.inplace = inplace\n",
    "        if ctx.inplace:\n",
    "            ctx.mark_dirty(input)\n",
    "            output = input\n",
    "\n",
    "        else:\n",
    "            output = input.clone()\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow FxInput forward! Val: {0}, Bit-width: {1}, val: {2}\".format(val.max(), bitw, 1 << bitw))\n",
    "\n",
    "        if trunc:   # 2f-bit precision, require truncation by 2^f\n",
    "            output = (output * (1 << bitf_backward)).floor()\n",
    "            overflow_validate(output, bitw_backward, \"FxInput forward\")\n",
    "            output = (output / (1 << (bitf_backward - bitf_forward))).floor()     # FX-representation\n",
    "            overflow_validate(output, bitw_forward, \"FxInput forward\")\n",
    "            output = output / (1 << bitf_forward)                         # translate to FP\n",
    "            # if output.max() >= (1 << (bitw - bitf_after)):                                 # overflow judgement\n",
    "            #     print(\"overflow\")\n",
    "        else:       # 0-bit precision, require multiply by 2^f, currently not used\n",
    "            output = (output * (1 << bitf_forward)).floor()              # FX-representation\n",
    "            overflow_validate(output, bitw_forward, 'FxInput forward')\n",
    "            output = output / (1 << bitf_backward)                         # FP\n",
    "            # if output.max() >= (1 << (bitw - bitf_before)):                                 # overflow judgement\n",
    "            #     print(\"overflow\")\n",
    "        return output\n",
    "    \n",
    "    \"\"\"\n",
    "    Follow the STE method. Such that the gradient of Quant(x) is 1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output\n",
    "        return grad_input, None, None, None, None, None, None\n",
    "\n",
    "class FxLinear(Function):\n",
    "    \"\"\"\n",
    "    FX multiplication for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    weight: (64, 16)\n",
    "    bias: (64, 16)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            detached_weight = weight.detach()\n",
    "            detached_weight.requires_grad_(True)\n",
    "\n",
    "            detached_bias = bias.detach()\n",
    "            detached_bias.requires_grad_(True)\n",
    "            \"\"\"\n",
    "            7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "            \"\"\"\n",
    "\n",
    "            qweight = FxInput.apply(detached_weight, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            qbias = FxInput.apply(detached_bias, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Print histogram of weights before and after quantization\n",
    "            \"\"\"\n",
    "            if not verbose:\n",
    "                n, bins, patches = plt.hist(detached_weight.data, 50)\n",
    "                plt.show()\n",
    "\n",
    "                n, bins, patches = plt.hist(qweight.data, 50)\n",
    "                plt.show()\n",
    "            \n",
    "\n",
    "            _output = F.linear(detached_input, qweight, qbias)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.saved_param = detached_weight, detached_bias\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'Linear Forward')\n",
    "        # if output.abs().max() >= float(2**(bitw_forward-1)):\n",
    "        #     raise Exception(\"Overflow Linear Forward! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _weight, _bias = ctx.saved_param\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_weight, grad_bias = _weight.grad, _bias.grad   # backward\n",
    "        grad_input = _input.grad\n",
    "        # print(\"ss: \", grad_input)\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow Linear Backward! Val: {0}, Bit-width: {1}\".format(val.max(), bitw))\n",
    "\n",
    "        grad_weight = (grad_weight * (1 << (bitf_forward + bitf_backward))).floor()     #FX\n",
    "        overflow_validate(grad_weight, bitw_backward, 'Linear Backward Weight')\n",
    "        grad_weight = (grad_weight / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_bias = (grad_bias * (1 << (bitf_forward + bitf_backward))).floor()         #FX\n",
    "        overflow_validate(grad_bias, bitw_backward, 'Linear Backward Bias')\n",
    "        grad_bias = (grad_bias / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Linear Backward Activation')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None\n",
    "\n",
    "class FxConv2D(Function):\n",
    "    \"\"\"\n",
    "    FX Convolution for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    weight: (64, 16)\n",
    "    bias: (64, 16)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, stride, padding, dilation, groups, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            detached_weight = weight.detach()\n",
    "            detached_weight.requires_grad_(True)\n",
    "\n",
    "            detached_bias = bias.detach()\n",
    "            detached_bias.requires_grad_(True)\n",
    "            \"\"\"\n",
    "            7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "            \"\"\"\n",
    "            qweight = FxInput.apply(detached_weight, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            qbias = FxInput.apply(detached_bias, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            \n",
    "            _output = F.conv2d(detached_input, qweight, qbias, stride, padding, dilation, groups)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.saved_param = detached_weight, detached_bias\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() * (1 << (2*bitf_forward))).floor()           # FX\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'Conv2D Forward')\n",
    "        # if output.abs().max() >= float(2**(bitw_forward-1)):\n",
    "        #     raise Exception(\"Overflow Conv2D Forward! Val: {0}, Bit-width: {1}, val: {2}\".format(output.max(), bitw_forward, 1 << bitw_forward))\n",
    "            \n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX truncate by bitf_forward bits\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _weight, _bias = ctx.saved_param\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_weight, grad_bias = _weight.grad, _bias.grad   # backward\n",
    "        grad_input = _input.grad\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow Conv2D Backward! Val: {0}, Bit-width: {1}, ring: {2}\".format(val.max(), bitw, 1 << bitw))\n",
    "\n",
    "        grad_weight = (grad_weight * (1 << (bitf_forward + bitf_backward))).floor()     #FX\n",
    "        overflow_validate(grad_weight, bitw_backward, 'Conv2D Backward Weight')\n",
    "        grad_weight = (grad_weight / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_bias = (grad_bias * (1 << (bitf_forward + bitf_backward))).floor()         #FX\n",
    "        overflow_validate(grad_bias, bitw_backward, 'Conv2D Backward Bias')\n",
    "        grad_bias = (grad_bias / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Conv2D Backward Activation')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None, None, None\n",
    "\n",
    "class FxLayerNorm(Function):\n",
    "    \"\"\"\n",
    "    FX LayerNorm for normalization activation outputs.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def fx_mean(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        tar_mean = input.mean(-1, keepdim = True)\n",
    "\n",
    "        # fx mean imp\n",
    "        sum = input.sum(-1, keepdim = True)\n",
    "        divisor = input.shape[-1]\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)      # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        mean = sum * recp\n",
    "\n",
    "        # Fx --> Fp\n",
    "        mean_fx = (mean * (1 << (2 * bitf_forward))).floor()\n",
    "        mean = (mean_fx / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "        # definitely not overflow, so here pass the overflow validate\n",
    "\n",
    "        # print(\"tar mean: {0}, output_mean: {1}, diff: {2}\".format(tar_mean, mean, mean - tar_mean))\n",
    "        return mean\n",
    "\n",
    "    @staticmethod\n",
    "    def fx_var(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        tar_var = input.var(-1, keepdim = True)\n",
    "\n",
    "        # fx var imp\n",
    "        mean = FxLayerNorm.fx_mean(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "        diff = (input - mean)\n",
    "        diff = diff ** 2\n",
    "        diff_sum = diff.sum(-1, keepdim = True)\n",
    "        diff_sum = (diff_sum * (1 << (2*bitf_forward))).floor()\n",
    "        overflow_validate(diff_sum, bitw_forward, \"FX Variance\")\n",
    "\n",
    "        diff_sum = (diff_sum / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "\n",
    "        divisor = input.shape[-1]\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        var = diff_sum * recp\n",
    "\n",
    "        # Fx --> Fp\n",
    "        var_fx = (var * (1 << (2 * bitf_forward))).floor()\n",
    "        var = (var_fx / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "        # print(\"tar var: {0}, output var: {1}, diff: {2}\".format(tar_var, var, var - tar_var))\n",
    "        return mean, var\n",
    "\n",
    "    @staticmethod\n",
    "    def fx_layer_norm(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        mean, var = FxLayerNorm.fx_var(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "\n",
    "        dividend = input - mean\n",
    "\n",
    "        divisor = torch.sqrt(var)\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        \n",
    "        norm_output = dividend * recp\n",
    "        return norm_output\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "            print(detached_input)\n",
    "            fx_output = FxLayerNorm.fx_layer_norm(detached_input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "            \n",
    "            _output = F.layer_norm(detached_input, (detached_input.shape[-1],))\n",
    "\n",
    "            print(\"tar norm: {0}, output norm: {1}, diff: {2}\".format(_output, fx_output, fx_output - _output))\n",
    "        \n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "        print(output)\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'LayerNorm Forward')\n",
    "        # if output.abs().max() >= 2 **(bitw_forward-1):\n",
    "        #     raise Exception(\"Overflow! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_input = _input.grad\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'LayerNorm Backward')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "class FxSoftmax(Function):\n",
    "    \"\"\"\n",
    "    FX softmax for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            \n",
    "            # TODO: modified to softmax. I think this should be done using polynomial approximation or iterative method\n",
    "            _output = F.softmax(detached_input)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "        overflow_validate(output, bitw_forward, 'Softmax Forward')\n",
    "        # if output.max() >= 2 **bitw_forward:\n",
    "        #     raise Exception(\"Overflow! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_input = _input.grad\n",
    "        # print(\"ss: \", grad_input)\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     print(val.dtype)\n",
    "        #     if val.abs().max() >= 2 ** (bitw-1):\n",
    "        #         raise Exception(\"Overflow Conv2D! Val: {0}, Bit-width: {1}\".format(val.max(), bitw))\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Softmax Backward')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fx Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2500)\n",
      "tensor(15.2500)\n",
      "tensor(7.6250, grad_fn=<FxInputBackward>)\n",
      "====================  FX Linear  ====================\n",
      "tensor(66.4225, grad_fn=<MseLossBackward0>)\n",
      "tensor(58.2500, grad_fn=<FxLinearBackward>)\n",
      "tensor(16.3000)\n",
      "tensor([124.2500])\n",
      "====================  LayerNorm  ====================\n",
      "tensor([[0.0496, 0.0768, 0.0088],\n",
      "        [0.0132, 0.0307, 0.0634]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.0496, 0.0768, 0.0088],\n",
      "        [0.0132, 0.0307, 0.0634]], requires_grad=True)\n",
      "tar norm: tensor([[ 0.1610,  1.1284, -1.2895],\n",
      "        [-1.0731, -0.2396,  1.3127]], grad_fn=<NativeLayerNormBackward0>), output norm: tensor([[ 0.1622,  1.1365, -1.2987],\n",
      "        [-1.0864, -0.2426,  1.3290]], grad_fn=<MulBackward0>), diff: tensor([[ 0.0012,  0.0081, -0.0092],\n",
      "        [-0.0133, -0.0029,  0.0163]], grad_fn=<SubBackward0>)\n",
      "tensor([[ 1.7706e+11,  1.2407e+12, -1.4178e+12],\n",
      "        [-1.1799e+12, -2.6346e+11,  1.4434e+12]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-268-09750be26726>:292: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1610,  1.1284, -1.2895],\n",
       "        [-1.0731, -0.2396,  1.3127]], grad_fn=<FxLayerNormBackward>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(7.6875, requires_grad=True)\n",
    "\n",
    "y = FxInput.apply(x, 16, 32, 3, 4, False, True)\n",
    "# y = FxInput.apply(x, 5, 32, 3, 4, False, False)\n",
    "y.retain_grad()\n",
    "loss = y * y\n",
    "loss.backward()\n",
    "\n",
    "print(y.grad)\n",
    "print(x.grad)\n",
    "print(y)\n",
    "\n",
    "\n",
    "print(\"=\"*20, \" FX Linear \", \"=\"*20)\n",
    "x = torch.full([1], 7.625, requires_grad=True)  # 3-bit\n",
    "w = torch.full([1], 7.6875, requires_grad=True) # 4-bit to 3-bit qweight = 7.625\n",
    "b = torch.tensor(0.2, requires_grad=True)\n",
    "z = FxLinear.apply(x, w, b, 16, 3, 32, 4)       # 58.25\n",
    "z.retain_grad()\n",
    "loss = F.mse_loss(z, torch.tensor(50.1))\n",
    "loss.backward()\n",
    "\n",
    "print(loss)\n",
    "print(z)\n",
    "print(z.grad)\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=\"*20, \" LayerNorm \", \"=\"*20)\n",
    "x = torch.rand([2, 3], requires_grad=True)  # 3-bit\n",
    "func = FxLayerNorm.apply\n",
    "print(x / 10)\n",
    "func(x / 10, 64, 20, 32, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(nn.Linear):\n",
    "    \"\"\"QLinear using FX.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \"\"\"\n",
    "        Print histogram of initialized weights using different f-bits quantization\n",
    "        \"\"\"\n",
    "        if not verbose:\n",
    "            n, bins, patches = plt.hist(self.weight.data, 20)\n",
    "            plt.show()\n",
    "\n",
    "            f_bits = [20, 12, 7, 6, 5]\n",
    "            for i in f_bits:\n",
    "                tmp_weight =  nn.Parameter(((self.weight / shifting_scale * (1 << i))).round() / (1 << i))    # FX64 copy\n",
    "                n, bins, patches = plt.hist(tmp_weight.data, 20)\n",
    "                plt.show()\n",
    "            \n",
    "        self.weight =  nn.Parameter(((self.weight / shifting_scale * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        self.bias =  nn.Parameter(((self.bias / shifting_scale * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Hack the weight initialitization\n",
    "    TODO: modify the kaiming init. Since the backend PyTorch init follows the uniform(-1/sqrt(in_features), 1/sqrt(in_features)) \n",
    "    https://github.com/pytorch/pytorch/issues/57109\n",
    "    \"\"\"\n",
    "    def reset_parameters(self) -> None:\n",
    "        return super().reset_parameters()\n",
    "    # we here assume all the input are already quantized to fixed-point numbers\n",
    "    def forward(self, input):\n",
    "        output = FxLinear.apply(input, self.weight, self.bias, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "\n",
    "        # qoutput = FxSimulation.apply(output, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)      # truncation by 2^f\n",
    "\n",
    "        return output\n",
    "\n",
    "class QConv2d(nn.Conv2d):\n",
    "    \"\"\"QConv2d using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding, dilation, groups, bias)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "\n",
    "        self.weight =  nn.Parameter(((self.weight * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        self.bias =  nn.Parameter(((self.bias * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxConv2D.apply(input, self.weight, self.bias, \n",
    "                            self.stride, self.padding, self.dilation, self.groups, \n",
    "                            self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        # output = F.conv2d(input, qweight, self.bias, self.stride,\n",
    "        #                     self.padding, self.dilation, self.groups)\n",
    "        return output\n",
    "\n",
    "class QLayerNorm(nn.LayerNorm):\n",
    "    \"\"\"QLayerNorm using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QLayerNorm, self).__init__(input)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxSoftmax.apply(input, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        return output\n",
    "\n",
    "class QSoftmax(nn.Softmax):\n",
    "    \"\"\"QSoftmax using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QSoftmax, self).__init__(input)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxSoftmax.apply(input, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "This two functions seems does not need to separately implement FX-version, since the precision does not change during computation\n",
    "\"\"\"\n",
    "class QReLU(nn.ReLU):\n",
    "    \"\"\"QReLU using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, num_bits=8, quant=False, quant_scale=False):\n",
    "        super(QReLU, self).__init__(input)\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "class QMaxPooling(nn.MaxPool2d):\n",
    "    \"\"\"\n",
    "    FX for MaxPooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride, padding = 0, dilation = 1, return_indices = False, ceil_mode = False, num_bits=8, quant=False, quant_scale=False) -> None:\n",
    "        super().__init__(kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # assert(input.dtype == torch.int32)\n",
    "        output = F.max_pool2d(input, self.kernel_size, self.stride, self.padding, self.dilation, self.ceil_mode)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SecureML(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(28*28, 128) if quant else Linear(28*28, 128),\n",
    "            # nn.LayerNorm(128, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            QLinear(128, 128) if quant else Linear(128, 128),\n",
    "            # nn.LayerNorm(128, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Sequential(\n",
    "            QLinear(128, 10) if quant else Linear(128, 10),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "class MiniONN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(4*4*16, 100) if quant and w_linear else Linear(4*4*16, 100),\n",
    "            # nn.LayerNorm(100, elementwise_affine=False),\n",
    "            # nn.BatchNorm1d(100, affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            QLinear(100, 10) if quant and w_linear else Linear(100, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 50, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(4*4*50, 500) if quant and w_linear else Linear(4*4*50, 500),\n",
    "            nn.ReLU(),\n",
    "            QLinear(500, 10) if quant and w_linear else Linear(500, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Softmax()\n",
    "        ) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "AlexNet for CIFAR-10\n",
    "https://github.com/soapisnotfat/pytorch-cifar10/blob/master/models/AlexNet.py\n",
    "\"\"\"\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "            \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "        )\n",
    "\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.cnn3(out)\n",
    "        out = self.cnn4(out)\n",
    "        out = self.cnn5(out)\n",
    "        out = self.fc_layers(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Standard implementation of VGG16 for ImageNet\n",
    "\"\"\"\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),  # Conv1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),  # Conv2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool1\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # Conv3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),  # Conv4\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool2\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  # Conv5\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  # Conv6\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  # Conv7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool3\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # Conv8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv10\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool4\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv11\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv12\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv13\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    "Standard implementation of VGG16 for CIFAR-10\n",
    "https://www.kaggle.com/willzy/vgg16-with-cifar10\n",
    "\"\"\"\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(4608, 4096),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.Linear(4096, 1000)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    #     self.features = nn.Sequential(\n",
    "    #         nn.Conv2d(3, 32, 3, padding=1),  # Conv1\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(32, 32, 3, padding=1),  # Conv2\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool1\n",
    "\n",
    "    #         nn.Conv2d(32, 64, 3, padding=1),  # Conv3\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(64, 64, 3, padding=1),  # Conv4\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool2\n",
    "\n",
    "    #         nn.Conv2d(64, 128, 3, padding=1),  # Conv5\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(128, 128, 3, padding=1),  # Conv6\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(128, 128, 3, padding=1),  # Conv7\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool3\n",
    "\n",
    "    #         nn.Conv2d(128, 256, 3, padding=1),  # Conv8\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv9\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv10\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool4\n",
    "\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv11\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv12\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv13\n",
    "    #         nn.ReLU(),\n",
    "    #         # nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
    "    #     )\n",
    "\n",
    "    #     self.classifier = nn.Sequential(\n",
    "    #         nn.Flatten(),\n",
    "    #         QLinear(2 * 2 * 256, 512) if quant and w_linear else Linear(2 * 2 * 256, 512) ,\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Dropout(),\n",
    "    #         QLinear(512, 512) if quant and w_linear else Linear(512, 512),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Dropout(),\n",
    "    #         QLinear(512, 10) if quant and w_linear else Linear(512, 10),\n",
    "    #     )\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     out = self.features(x)\n",
    "    #     out = self.classifier(out)\n",
    "    #     return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# MNIST Dataset\n",
    "if ds == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=True,\n",
    "                                transform=transforms.ToTensor(),\n",
    "                                download=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "elif ds == 'CIFAR-10':\n",
    "    # CIFAR-10 Dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyTransformer.transformers.torchTransformer import TorchTransformer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if model_name == 'SecureML':\n",
    "    model = SecureML()\n",
    "elif model_name == \"MiniONN\":\n",
    "    model = MiniONN()\n",
    "elif model_name == \"LeNet\":\n",
    "    model = LeNet()\n",
    "elif model_name == \"AlexNet\":    \n",
    "    model = AlexNet()\n",
    "# model = VGG16()\n",
    "\n",
    "if quant and w_conv:\n",
    "    transformer = TorchTransformer()\n",
    "    transformer.register(torch.nn.Conv2d, QConv2d)\n",
    "    # transformer.register(torch.nn.Linear, QLinear)        # TODO: currently, there is a bug.\n",
    "    model = transformer.trans_layers(model)\n",
    "# for name, modules in model.named_modules():\n",
    "#     print(name)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "logger = SummaryWriter(log_dir = \"log\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (feature, label) in enumerate(train_loader):\n",
    "        feature, label = Variable(feature), Variable(label)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # re-scale from [0,1] to [-1,1]\n",
    "        if re_scale: \n",
    "            feature = 2 * feature - 1\n",
    "        output = model(feature)\n",
    "        label_raw = label\n",
    "        label = F.one_hot(label, num_classes=10).type(torch.float)\n",
    "        loss = F.cross_entropy(output, label_raw)\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        global_iter_num = epoch * len(train_loader) + batch_idx + 1\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(feature), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), total_loss/100))\n",
    "            total_loss = 0\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(label_raw.data.view_as(pred)).cpu().sum()\n",
    "            train_acc = 100. * correct / len(feature)\n",
    "\n",
    "            acc = test()\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            log into tensorboard\n",
    "            https://stackoverflow.com/questions/37304461/tensorflow-importing-data-from-a-tensorboard-tfevent-file\n",
    "            \"\"\"\n",
    "            logger.add_scalar(\"train loss\", loss.item(), global_iter_num)\n",
    "            logger.add_scalar(\"train accuracy\", train_acc, global_iter_num)\n",
    "            logger.add_scalar(\"test accuracy\", acc, global_iter_num)\n",
    "            for name, param in model.named_parameters():\n",
    "                logger.add_histogram(name, param.data.numpy(), global_iter_num)\n",
    "                logger.add_histogram(name+\"_grad\", param.grad.numpy(), global_iter_num)\n",
    "            for name, module in model.named_modules():\n",
    "                logger.add_histogram(name, activations[name], global_iter_num)\n",
    "                \n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if re_scale:\n",
    "            data = 2 * data -1\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook for logging activation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" hook for activation \"\"\"\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "for name, module in model.named_modules():\n",
    "    module.register_forward_hook(get_activation(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Training Setting ====================\n",
      "Batch size:  32\n",
      "Dataset:  CIFAR-10\n",
      "Data re-scale:  False\n",
      "Model: \n",
      "AlexNet(\n",
      "  (cnn1): Sequential(\n",
      "    (0): QConv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(9, 9))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (cnn2): Sequential(\n",
      "    (0): QConv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (cnn3): Sequential(\n",
      "    (0): QConv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (cnn4): Sequential(\n",
      "    (0): QConv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (cnn5): Sequential(\n",
      "    (0): QConv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "==================== Quantization Setting ====================\n",
      "Quant: False, \tw_linear: True,  \tw_conv: True\n",
      "\tForward: \t32\t12\n",
      "\tBackward: \t64\t20\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20, \"Training Setting\", \"=\"*20)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Dataset: \", ds)\n",
    "print(\"Data re-scale: \", re_scale)\n",
    "print(\"Model: \")\n",
    "print(model)\n",
    "\n",
    "print(\"=\"*20, \"Quantization Setting\", \"=\"*20)\n",
    "print(\"Quant: {0}, \\tw_linear: {1},  \\tw_conv: {2}\".format(quant, w_linear, w_conv))\n",
    "print(\"\\tForward: \\t{0}\\t{1}\\n\\tBackward: \\t{2}\\t{3}\".format(bitw_forward, bitf_forward,  bitw_backward, bitf_backward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.014094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-274-f6cc16486685>:56: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "/home/whq/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.1736, Accuracy: 5931/10000 (59%)\n",
      "\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 1.021468\n",
      "\n",
      "Test set: Average loss: 1.1794, Accuracy: 5908/10000 (59%)\n",
      "\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.979371\n",
      "\n",
      "Test set: Average loss: 1.1786, Accuracy: 5984/10000 (60%)\n",
      "\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 1.025116\n",
      "\n",
      "Test set: Average loss: 1.1459, Accuracy: 6048/10000 (60%)\n",
      "\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.995657\n",
      "\n",
      "Test set: Average loss: 1.1762, Accuracy: 5987/10000 (60%)\n",
      "\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.019940\n",
      "\n",
      "Test set: Average loss: 1.1857, Accuracy: 5935/10000 (59%)\n",
      "\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.012566\n",
      "\n",
      "Test set: Average loss: 1.1554, Accuracy: 6019/10000 (60%)\n",
      "\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 0.994827\n",
      "\n",
      "Test set: Average loss: 1.1488, Accuracy: 6033/10000 (60%)\n",
      "\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.011436\n",
      "\n",
      "Test set: Average loss: 1.0946, Accuracy: 6253/10000 (63%)\n",
      "\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 1.022133\n",
      "\n",
      "Test set: Average loss: 1.1177, Accuracy: 6190/10000 (62%)\n",
      "\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.034598\n",
      "\n",
      "Test set: Average loss: 1.1286, Accuracy: 6146/10000 (61%)\n",
      "\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 0.987964\n",
      "\n",
      "Test set: Average loss: 1.1000, Accuracy: 6224/10000 (62%)\n",
      "\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.978820\n",
      "\n",
      "Test set: Average loss: 1.1147, Accuracy: 6252/10000 (63%)\n",
      "\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.008927\n",
      "\n",
      "Test set: Average loss: 1.0903, Accuracy: 6256/10000 (63%)\n",
      "\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.011015\n",
      "\n",
      "Test set: Average loss: 1.0919, Accuracy: 6263/10000 (63%)\n",
      "\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 0.964217\n",
      "\n",
      "Test set: Average loss: 1.1052, Accuracy: 6187/10000 (62%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0814, Accuracy: 6358/10000 (64%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.007868\n",
      "\n",
      "Test set: Average loss: 1.0878, Accuracy: 6327/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 0.878767\n",
      "\n",
      "Test set: Average loss: 1.0961, Accuracy: 6273/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.889354\n",
      "\n",
      "Test set: Average loss: 1.0892, Accuracy: 6271/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 0.902053\n",
      "\n",
      "Test set: Average loss: 1.1689, Accuracy: 6105/10000 (61%)\n",
      "\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.893745\n",
      "\n",
      "Test set: Average loss: 1.1593, Accuracy: 6173/10000 (62%)\n",
      "\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 0.879787\n",
      "\n",
      "Test set: Average loss: 1.1308, Accuracy: 6256/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.895230\n",
      "\n",
      "Test set: Average loss: 1.1092, Accuracy: 6220/10000 (62%)\n",
      "\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 0.899901\n",
      "\n",
      "Test set: Average loss: 1.1248, Accuracy: 6257/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.909905\n",
      "\n",
      "Test set: Average loss: 1.1643, Accuracy: 6116/10000 (61%)\n",
      "\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 0.911487\n",
      "\n",
      "Test set: Average loss: 1.0880, Accuracy: 6291/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.897937\n",
      "\n",
      "Test set: Average loss: 1.0722, Accuracy: 6386/10000 (64%)\n",
      "\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 0.904833\n",
      "\n",
      "Test set: Average loss: 1.0616, Accuracy: 6489/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.892805\n",
      "\n",
      "Test set: Average loss: 1.1001, Accuracy: 6340/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 0.905535\n",
      "\n",
      "Test set: Average loss: 1.1150, Accuracy: 6265/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.900244\n",
      "\n",
      "Test set: Average loss: 1.0431, Accuracy: 6494/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 0.917903\n",
      "\n",
      "Test set: Average loss: 1.1203, Accuracy: 6191/10000 (62%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0490, Accuracy: 6537/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.009737\n",
      "\n",
      "Test set: Average loss: 1.0430, Accuracy: 6555/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [3200/50000 (6%)]\tLoss: 0.771498\n",
      "\n",
      "Test set: Average loss: 1.1000, Accuracy: 6463/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.804410\n",
      "\n",
      "Test set: Average loss: 1.0765, Accuracy: 6416/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [9600/50000 (19%)]\tLoss: 0.776915\n",
      "\n",
      "Test set: Average loss: 1.0986, Accuracy: 6362/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.797648\n",
      "\n",
      "Test set: Average loss: 1.1482, Accuracy: 6210/10000 (62%)\n",
      "\n",
      "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 0.819311\n",
      "\n",
      "Test set: Average loss: 1.0856, Accuracy: 6362/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.786340\n",
      "\n",
      "Test set: Average loss: 1.0966, Accuracy: 6316/10000 (63%)\n",
      "\n",
      "Train Epoch: 3 [22400/50000 (45%)]\tLoss: 0.832915\n",
      "\n",
      "Test set: Average loss: 1.0767, Accuracy: 6352/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.811534\n",
      "\n",
      "Test set: Average loss: 1.0625, Accuracy: 6458/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [28800/50000 (58%)]\tLoss: 0.835284\n",
      "\n",
      "Test set: Average loss: 1.0699, Accuracy: 6375/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.822886\n",
      "\n",
      "Test set: Average loss: 1.0726, Accuracy: 6474/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [35200/50000 (70%)]\tLoss: 0.816827\n",
      "\n",
      "Test set: Average loss: 1.0528, Accuracy: 6499/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.836466\n",
      "\n",
      "Test set: Average loss: 1.0275, Accuracy: 6544/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [41600/50000 (83%)]\tLoss: 0.812815\n",
      "\n",
      "Test set: Average loss: 1.0915, Accuracy: 6445/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.813705\n",
      "\n",
      "Test set: Average loss: 1.0878, Accuracy: 6429/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 0.826092\n",
      "\n",
      "Test set: Average loss: 1.0815, Accuracy: 6467/10000 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.1311, Accuracy: 6310/10000 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.007154\n",
      "\n",
      "Test set: Average loss: 1.1262, Accuracy: 6375/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [3200/50000 (6%)]\tLoss: 0.724850\n",
      "\n",
      "Test set: Average loss: 1.0913, Accuracy: 6456/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.678280\n",
      "\n",
      "Test set: Average loss: 1.0492, Accuracy: 6521/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [9600/50000 (19%)]\tLoss: 0.739089\n",
      "\n",
      "Test set: Average loss: 1.0975, Accuracy: 6344/10000 (63%)\n",
      "\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.720386\n",
      "\n",
      "Test set: Average loss: 1.1070, Accuracy: 6401/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 0.731801\n",
      "\n",
      "Test set: Average loss: 1.0980, Accuracy: 6418/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.700987\n",
      "\n",
      "Test set: Average loss: 1.1233, Accuracy: 6357/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [22400/50000 (45%)]\tLoss: 0.734260\n",
      "\n",
      "Test set: Average loss: 1.0998, Accuracy: 6463/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.728791\n",
      "\n",
      "Test set: Average loss: 1.0688, Accuracy: 6544/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [28800/50000 (58%)]\tLoss: 0.727071\n",
      "\n",
      "Test set: Average loss: 1.0339, Accuracy: 6566/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.722710\n",
      "\n",
      "Test set: Average loss: 1.0627, Accuracy: 6632/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [35200/50000 (70%)]\tLoss: 0.748887\n",
      "\n",
      "Test set: Average loss: 1.0595, Accuracy: 6502/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.758480\n",
      "\n",
      "Test set: Average loss: 1.1145, Accuracy: 6397/10000 (64%)\n",
      "\n",
      "Train Epoch: 4 [41600/50000 (83%)]\tLoss: 0.754424\n",
      "\n",
      "Test set: Average loss: 1.0330, Accuracy: 6534/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.717807\n",
      "\n",
      "Test set: Average loss: 1.0639, Accuracy: 6530/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 0.754798\n",
      "\n",
      "Test set: Average loss: 1.0287, Accuracy: 6655/10000 (67%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0814, Accuracy: 6506/10000 (65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/VGG16/full-precision/cnn1.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn1.0.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn1.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn1.0.bias_grad.pdf\n",
      "log/VGG16/full-precision/cnn1.3.weight.pdf\n",
      "log/VGG16/full-precision/cnn1.3.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn1.3.bias.pdf\n",
      "log/VGG16/full-precision/cnn1.3.bias_grad.pdf\n",
      "log/VGG16/full-precision/cnn2.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn2.0.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn2.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn2.0.bias_grad.pdf\n",
      "log/VGG16/full-precision/cnn2.3.weight.pdf\n",
      "log/VGG16/full-precision/cnn2.3.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn2.3.bias.pdf\n",
      "log/VGG16/full-precision/cnn2.3.bias_grad.pdf\n",
      "log/VGG16/full-precision/cnn3.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn3.0.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn3.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn3.0.bias_grad.pdf\n",
      "log/VGG16/full-precision/cnn4.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn4.0.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn4.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn4.0.bias_grad.pdf\n",
      "log/VGG16/full-precision/cnn5.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn5.0.weight_grad.pdf\n",
      "log/VGG16/full-precision/cnn5.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn5.0.bias_grad.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.weight.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.weight_grad.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.bias.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.bias_grad.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.weight.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.weight_grad.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.bias.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.bias_grad.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.weight.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.weight_grad.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.bias.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.bias_grad.pdf\n",
      "log/VGG16/full-precision/.act.pdf\n",
      "log/VGG16/full-precision/cnn1.act.pdf\n",
      "log/VGG16/full-precision/cnn1.0.act.pdf\n",
      "log/VGG16/full-precision/cnn1.1.act.pdf\n",
      "log/VGG16/full-precision/cnn1.2.act.pdf\n",
      "log/VGG16/full-precision/cnn1.3.act.pdf\n",
      "log/VGG16/full-precision/cnn2.act.pdf\n",
      "log/VGG16/full-precision/cnn2.0.act.pdf\n",
      "log/VGG16/full-precision/cnn2.1.act.pdf\n",
      "log/VGG16/full-precision/cnn2.2.act.pdf\n",
      "log/VGG16/full-precision/cnn2.3.act.pdf\n",
      "log/VGG16/full-precision/cnn3.act.pdf\n",
      "log/VGG16/full-precision/cnn3.0.act.pdf\n",
      "log/VGG16/full-precision/cnn3.1.act.pdf\n",
      "log/VGG16/full-precision/cnn4.act.pdf\n",
      "log/VGG16/full-precision/cnn4.0.act.pdf\n",
      "log/VGG16/full-precision/cnn4.1.act.pdf\n",
      "log/VGG16/full-precision/cnn5.act.pdf\n",
      "log/VGG16/full-precision/cnn5.0.act.pdf\n",
      "log/VGG16/full-precision/cnn5.1.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.0.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.2.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.4.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.act.pdf\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlabel = \"Expectation\"\n",
    "ylable = \"Density\"\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set()\n",
    "\n",
    "font_family = {'family': 'Times New Roman', 'size': 14}\n",
    "font_size = {'ax_label': 24, 'ax_tick': 22, 'inner_tick': 14, 'text': 15, 'legend': 12}\n",
    "line_args = {'marker': 'o', 'markevery': 10, 'color': 'red'}\n",
    "\n",
    "sufix = 'quant' if quant else 'full-precision'\n",
    "base_path = 'log/' + model_name + '/' + sufix + \"/\"\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "def draw_handle(name, data):\n",
    "    plt.clf()\n",
    "    # plt.hist(data)\n",
    "    # ax = plt.subplot()\n",
    "    sns.histplot(data.flatten(), kde=True, fill=True, stat='probability', line_kws=line_args)\n",
    "    # sns.displot(data.flatten(), kde=True)\n",
    "    # sns.kdeplot(data.flatten(), ls=':')\n",
    "    # sns.distplot(data.flatten(), kde=True, hist=False, norm_hist=True)\n",
    "    # plt.hist(data.flatten(), density=True)\n",
    "    # plt.xlabel(xlabel, fontdict=font_family)\n",
    "    # plt.ylabel(ylable, fontdict=font_family)\n",
    "    plt.grid(True, alpha=0.5, linestyle='--')\n",
    "    plt.title(name, fontdict=font_family)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    print(base_path + name + '.pdf')\n",
    "    plt.savefig(base_path + name + '.pdf')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # draw_handle(name.split('.')[0] + '.weight', param.data.numpy())\n",
    "    # draw_handle(name.split('.')[0] + '.weight_grad', param.grad.numpy())\n",
    "\n",
    "    draw_handle(name, param.data.numpy())\n",
    "    draw_handle(name + '_grad', param.grad.numpy())\n",
    "\n",
    "    # logger.add_histogram(name, param.data.numpy(), global_iter_num)\n",
    "    # logger.add_histogram(name+\"_grad\", param.grad.numpy(), global_iter_num)\n",
    "for name, module in model.named_modules():\n",
    "    draw_handle(name + '.act', activations[name])\n",
    "\n",
    "    # logger.add_histogram(name, activations[name], global_iter_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8e35784da7b5309346b9948007e2c8e432cfca57e220859a198433798687959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
