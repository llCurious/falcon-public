{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc5e2890fd0>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training configs\n",
    "batch_size = 64\n",
    "bitw_forward = 32\n",
    "bitf_forward = 12\n",
    "\n",
    "bitw_backward = 64\n",
    "bitf_backward = 20\n",
    "\n",
    "re_scale = False\n",
    "\n",
    "use_LN = False\n",
    "\n",
    "log_acc = True\n",
    "quant = True\n",
    "w_linear = True\n",
    "w_conv = True\n",
    "\n",
    "# SecureML, MiniONN, LeNet, AlexNet, VGG16\n",
    "model_name = \"VGG16\"\n",
    "\n",
    "shifting_scale = 1.\n",
    "\n",
    "# ds = 'CIFAR-10'\n",
    "ds = 'CIFAR-10' if model_name in ['AlexNet', 'VGG16'] else 'MNIST'\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\"\"\"\n",
    "make sure all the random initiation of parameters are the same in different runs\n",
    "\"\"\"\n",
    "torch.manual_seed(0)\n",
    "# print(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Quantized Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overflow_validate(val, bitw, location):\n",
    "    if val.abs().max() >= float(2**(bitw-1)):\n",
    "        raise Exception(\"Overflow {4}! Val: {0}, Bit-width: {1}, val: [-{2}, {3}]\".format(val.abs().max(), bitw, 1 << (bitw-1), (1 << (bitw-1)) - 1, location))\n",
    "\n",
    "class FxInput(InplaceFunction):\n",
    "    \"\"\"\n",
    "    Quantize w.r.t. qmax in forward pass; use STE in backward pass\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitw_backward, bitf_forward, bitf_backward, inplace=False, trunc=False):\n",
    "        ctx.inplace = inplace\n",
    "        if ctx.inplace:\n",
    "            ctx.mark_dirty(input)\n",
    "            output = input\n",
    "\n",
    "        else:\n",
    "            output = input.clone()\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow FxInput forward! Val: {0}, Bit-width: {1}, val: {2}\".format(val.max(), bitw, 1 << bitw))\n",
    "\n",
    "        if trunc:   # 2f-bit precision, require truncation by 2^f\n",
    "            output = (output * (1 << bitf_backward)).floor()\n",
    "            overflow_validate(output, bitw_backward, \"FxInput forward\")\n",
    "            output = (output / (1 << (bitf_backward - bitf_forward))).floor()     # FX-representation\n",
    "            overflow_validate(output, bitw_forward, \"FxInput forward\")\n",
    "            output = output / (1 << bitf_forward)                         # translate to FP\n",
    "            # if output.max() >= (1 << (bitw - bitf_after)):                                 # overflow judgement\n",
    "            #     print(\"overflow\")\n",
    "        else:       # 0-bit precision, require multiply by 2^f, currently not used\n",
    "            output = (output * (1 << bitf_forward)).floor()              # FX-representation\n",
    "            overflow_validate(output, bitw_forward, 'FxInput forward')\n",
    "            output = output / (1 << bitf_backward)                         # FP\n",
    "            # if output.max() >= (1 << (bitw - bitf_before)):                                 # overflow judgement\n",
    "            #     print(\"overflow\")\n",
    "        return output\n",
    "    \n",
    "    \"\"\"\n",
    "    Follow the STE method. Such that the gradient of Quant(x) is 1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output\n",
    "        return grad_input, None, None, None, None, None, None\n",
    "\n",
    "class FxLinear(Function):\n",
    "    \"\"\"\n",
    "    FX multiplication for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    weight: (64, 16)\n",
    "    bias: (64, 16)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            detached_weight = weight.detach()\n",
    "            detached_weight.requires_grad_(True)\n",
    "\n",
    "            detached_bias = bias.detach()\n",
    "            detached_bias.requires_grad_(True)\n",
    "            \"\"\"\n",
    "            7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "            \"\"\"\n",
    "\n",
    "            qweight = FxInput.apply(detached_weight, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            qbias = FxInput.apply(detached_bias, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Print histogram of weights before and after quantization\n",
    "            \"\"\"\n",
    "            if not verbose:\n",
    "                n, bins, patches = plt.hist(detached_weight.data, 50)\n",
    "                plt.show()\n",
    "\n",
    "                n, bins, patches = plt.hist(qweight.data, 50)\n",
    "                plt.show()\n",
    "            \n",
    "\n",
    "            _output = F.linear(detached_input, qweight, qbias)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.saved_param = detached_weight, detached_bias\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'Linear Forward')\n",
    "        # if output.abs().max() >= float(2**(bitw_forward-1)):\n",
    "        #     raise Exception(\"Overflow Linear Forward! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _weight, _bias = ctx.saved_param\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_weight, grad_bias = _weight.grad, _bias.grad   # backward\n",
    "        grad_input = _input.grad\n",
    "        # print(\"ss: \", grad_input)\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow Linear Backward! Val: {0}, Bit-width: {1}\".format(val.max(), bitw))\n",
    "\n",
    "        grad_weight = (grad_weight * (1 << (bitf_forward + bitf_backward))).floor()     #FX\n",
    "        overflow_validate(grad_weight, bitw_backward, 'Linear Backward Weight')\n",
    "        grad_weight = (grad_weight / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_bias = (grad_bias * (1 << (bitf_forward + bitf_backward))).floor()         #FX\n",
    "        overflow_validate(grad_bias, bitw_backward, 'Linear Backward Bias')\n",
    "        grad_bias = (grad_bias / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Linear Backward Activation')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None\n",
    "\n",
    "class FxConv2D(Function):\n",
    "    \"\"\"\n",
    "    FX Convolution for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    weight: (64, 16)\n",
    "    bias: (64, 16)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, stride, padding, dilation, groups, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            detached_weight = weight.detach()\n",
    "            detached_weight.requires_grad_(True)\n",
    "\n",
    "            detached_bias = bias.detach()\n",
    "            detached_bias.requires_grad_(True)\n",
    "            \"\"\"\n",
    "            7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "            \"\"\"\n",
    "            qweight = FxInput.apply(detached_weight, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            qbias = FxInput.apply(detached_bias, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            \n",
    "            _output = F.conv2d(detached_input, qweight, qbias, stride, padding, dilation, groups)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.saved_param = detached_weight, detached_bias\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() * (1 << (2*bitf_forward))).floor()           # FX\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'Conv2D Forward')\n",
    "        # if output.abs().max() >= float(2**(bitw_forward-1)):\n",
    "        #     raise Exception(\"Overflow Conv2D Forward! Val: {0}, Bit-width: {1}, val: {2}\".format(output.max(), bitw_forward, 1 << bitw_forward))\n",
    "            \n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX truncate by bitf_forward bits\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _weight, _bias = ctx.saved_param\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_weight, grad_bias = _weight.grad, _bias.grad   # backward\n",
    "        grad_input = _input.grad\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow Conv2D Backward! Val: {0}, Bit-width: {1}, ring: {2}\".format(val.max(), bitw, 1 << bitw))\n",
    "\n",
    "        grad_weight = (grad_weight * (1 << (bitf_forward + bitf_backward))).floor()     #FX\n",
    "        overflow_validate(grad_weight, bitw_backward, 'Conv2D Backward Weight')\n",
    "        grad_weight = (grad_weight / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_bias = (grad_bias * (1 << (bitf_forward + bitf_backward))).floor()         #FX\n",
    "        overflow_validate(grad_bias, bitw_backward, 'Conv2D Backward Bias')\n",
    "        grad_bias = (grad_bias / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Conv2D Backward Activation')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None, None, None\n",
    "\n",
    "class FxLayerNorm(Function):\n",
    "    \"\"\"\n",
    "    FX LayerNorm for normalization activation outputs.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def fx_mean(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        tar_mean = input.mean(-1, keepdim = True)\n",
    "\n",
    "        # fx mean imp\n",
    "        sum = input.sum(-1, keepdim = True)\n",
    "        divisor = input.shape[-1]\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)      # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        mean = sum * recp\n",
    "\n",
    "        # Fx --> Fp\n",
    "        mean_fx = (mean * (1 << (2 * bitf_forward))).floor()\n",
    "        mean = (mean_fx / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "        # definitely not overflow, so here pass the overflow validate\n",
    "\n",
    "        # print(\"tar mean: {0}, output_mean: {1}, diff: {2}\".format(tar_mean, mean, mean - tar_mean))\n",
    "        return mean\n",
    "\n",
    "    @staticmethod\n",
    "    def fx_var(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        tar_var = input.var(-1, keepdim = True)\n",
    "\n",
    "        # fx var imp\n",
    "        mean = FxLayerNorm.fx_mean(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "        diff = (input - mean)\n",
    "        diff = diff ** 2\n",
    "        diff_sum = diff.sum(-1, keepdim = True)\n",
    "        diff_sum = (diff_sum * (1 << (2*bitf_forward))).floor()\n",
    "        overflow_validate(diff_sum, bitw_forward, \"FX Variance\")\n",
    "\n",
    "        diff_sum = (diff_sum / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "\n",
    "        divisor = input.shape[-1]\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        var = diff_sum * recp\n",
    "\n",
    "        # Fx --> Fp\n",
    "        var_fx = (var * (1 << (2 * bitf_forward))).floor()\n",
    "        var = (var_fx / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "        # print(\"tar var: {0}, output var: {1}, diff: {2}\".format(tar_var, var, var - tar_var))\n",
    "        return mean, var\n",
    "\n",
    "    @staticmethod\n",
    "    def fx_layer_norm(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        mean, var = FxLayerNorm.fx_var(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "\n",
    "        dividend = input - mean\n",
    "\n",
    "        divisor = torch.sqrt(var)\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        \n",
    "        norm_output = dividend * recp\n",
    "        return norm_output\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "            print(detached_input)\n",
    "            fx_output = FxLayerNorm.fx_layer_norm(detached_input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "            \n",
    "            _output = F.layer_norm(detached_input, (detached_input.shape[-1],))\n",
    "\n",
    "            print(\"tar norm: {0}, output norm: {1}, diff: {2}\".format(_output, fx_output, fx_output - _output))\n",
    "        \n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "        print(output)\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'LayerNorm Forward')\n",
    "        # if output.abs().max() >= 2 **(bitw_forward-1):\n",
    "        #     raise Exception(\"Overflow! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_input = _input.grad\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'LayerNorm Backward')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "class FxSoftmax(Function):\n",
    "    \"\"\"\n",
    "    FX softmax for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            \n",
    "            # TODO: modified to softmax. I think this should be done using polynomial approximation or iterative method\n",
    "            _output = F.softmax(detached_input)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "        overflow_validate(output, bitw_forward, 'Softmax Forward')\n",
    "        # if output.max() >= 2 **bitw_forward:\n",
    "        #     raise Exception(\"Overflow! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_input = _input.grad\n",
    "        # print(\"ss: \", grad_input)\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     print(val.dtype)\n",
    "        #     if val.abs().max() >= 2 ** (bitw-1):\n",
    "        #         raise Exception(\"Overflow Conv2D! Val: {0}, Bit-width: {1}\".format(val.max(), bitw))\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Softmax Backward')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fx Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2500)\n",
      "tensor(15.2500)\n",
      "tensor(7.6250, grad_fn=<FxInputBackward>)\n",
      "====================  FX Linear  ====================\n",
      "tensor(66.4225, grad_fn=<MseLossBackward0>)\n",
      "tensor(58.2500, grad_fn=<FxLinearBackward>)\n",
      "tensor(16.3000)\n",
      "tensor([124.2500])\n",
      "====================  LayerNorm  ====================\n",
      "tensor([[0.0496, 0.0768, 0.0088],\n",
      "        [0.0132, 0.0307, 0.0634]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.0496, 0.0768, 0.0088],\n",
      "        [0.0132, 0.0307, 0.0634]], requires_grad=True)\n",
      "tar norm: tensor([[ 0.1610,  1.1284, -1.2895],\n",
      "        [-1.0731, -0.2396,  1.3127]], grad_fn=<NativeLayerNormBackward0>), output norm: tensor([[ 0.1622,  1.1365, -1.2987],\n",
      "        [-1.0864, -0.2426,  1.3290]], grad_fn=<MulBackward0>), diff: tensor([[ 0.0012,  0.0081, -0.0092],\n",
      "        [-0.0133, -0.0029,  0.0163]], grad_fn=<SubBackward0>)\n",
      "tensor([[ 1.7706e+11,  1.2407e+12, -1.4178e+12],\n",
      "        [-1.1799e+12, -2.6346e+11,  1.4434e+12]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-120-09750be26726>:292: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1610,  1.1284, -1.2895],\n",
       "        [-1.0731, -0.2396,  1.3127]], grad_fn=<FxLayerNormBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(7.6875, requires_grad=True)\n",
    "\n",
    "y = FxInput.apply(x, 16, 32, 3, 4, False, True)\n",
    "# y = FxInput.apply(x, 5, 32, 3, 4, False, False)\n",
    "y.retain_grad()\n",
    "loss = y * y\n",
    "loss.backward()\n",
    "\n",
    "print(y.grad)\n",
    "print(x.grad)\n",
    "print(y)\n",
    "\n",
    "\n",
    "print(\"=\"*20, \" FX Linear \", \"=\"*20)\n",
    "x = torch.full([1], 7.625, requires_grad=True)  # 3-bit\n",
    "w = torch.full([1], 7.6875, requires_grad=True) # 4-bit to 3-bit qweight = 7.625\n",
    "b = torch.tensor(0.2, requires_grad=True)\n",
    "z = FxLinear.apply(x, w, b, 16, 3, 32, 4)       # 58.25\n",
    "z.retain_grad()\n",
    "loss = F.mse_loss(z, torch.tensor(50.1))\n",
    "loss.backward()\n",
    "\n",
    "print(loss)\n",
    "print(z)\n",
    "print(z.grad)\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=\"*20, \" LayerNorm \", \"=\"*20)\n",
    "x = torch.rand([2, 3], requires_grad=True)  # 3-bit\n",
    "func = FxLayerNorm.apply\n",
    "print(x / 10)\n",
    "func(x / 10, 64, 20, 32, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(nn.Linear):\n",
    "    \"\"\"QLinear using FX.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \"\"\"\n",
    "        Print histogram of initialized weights using different f-bits quantization\n",
    "        \"\"\"\n",
    "        if not verbose:\n",
    "            n, bins, patches = plt.hist(self.weight.data, 20)\n",
    "            plt.show()\n",
    "\n",
    "            f_bits = [20, 12, 7, 6, 5]\n",
    "            for i in f_bits:\n",
    "                tmp_weight =  nn.Parameter(((self.weight / shifting_scale * (1 << i))).round() / (1 << i))    # FX64 copy\n",
    "                n, bins, patches = plt.hist(tmp_weight.data, 20)\n",
    "                plt.show()\n",
    "            \n",
    "        self.weight =  nn.Parameter(((self.weight / shifting_scale * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        self.bias =  nn.Parameter(((self.bias / shifting_scale * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Hack the weight initialitization\n",
    "    TODO: modify the kaiming init. Since the backend PyTorch init follows the uniform(-1/sqrt(in_features), 1/sqrt(in_features)) \n",
    "    https://github.com/pytorch/pytorch/issues/57109\n",
    "    \"\"\"\n",
    "    def reset_parameters(self) -> None:\n",
    "        return super().reset_parameters()\n",
    "    # we here assume all the input are already quantized to fixed-point numbers\n",
    "    def forward(self, input):\n",
    "        output = FxLinear.apply(input, self.weight, self.bias, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "\n",
    "        # qoutput = FxSimulation.apply(output, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)      # truncation by 2^f\n",
    "\n",
    "        return output\n",
    "\n",
    "class QConv2d(nn.Conv2d):\n",
    "    \"\"\"QConv2d using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding, dilation, groups, bias)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "\n",
    "        self.weight =  nn.Parameter(((self.weight * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        self.bias =  nn.Parameter(((self.bias * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxConv2D.apply(input, self.weight, self.bias, \n",
    "                            self.stride, self.padding, self.dilation, self.groups, \n",
    "                            self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        # output = F.conv2d(input, qweight, self.bias, self.stride,\n",
    "        #                     self.padding, self.dilation, self.groups)\n",
    "        return output\n",
    "\n",
    "class QLayerNorm(nn.LayerNorm):\n",
    "    \"\"\"QLayerNorm using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QLayerNorm, self).__init__(input)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxSoftmax.apply(input, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        return output\n",
    "\n",
    "class QSoftmax(nn.Softmax):\n",
    "    \"\"\"QSoftmax using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QSoftmax, self).__init__(input)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxSoftmax.apply(input, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "This two functions seems does not need to separately implement FX-version, since the precision does not change during computation\n",
    "\"\"\"\n",
    "class QReLU(nn.ReLU):\n",
    "    \"\"\"QReLU using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, num_bits=8, quant=False, quant_scale=False):\n",
    "        super(QReLU, self).__init__(input)\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "class QMaxPooling(nn.MaxPool2d):\n",
    "    \"\"\"\n",
    "    FX for MaxPooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride, padding = 0, dilation = 1, return_indices = False, ceil_mode = False, num_bits=8, quant=False, quant_scale=False) -> None:\n",
    "        super().__init__(kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # assert(input.dtype == torch.int32)\n",
    "        output = F.max_pool2d(input, self.kernel_size, self.stride, self.padding, self.dilation, self.ceil_mode)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SecureML(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(28*28, 128) if quant else Linear(28*28, 128),\n",
    "            # nn.LayerNorm(128, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            QLinear(128, 128) if quant else Linear(128, 128),\n",
    "            # nn.LayerNorm(128, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Sequential(\n",
    "            QLinear(128, 10) if quant else Linear(128, 10),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "class MiniONN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(4*4*16, 100) if quant and w_linear else Linear(4*4*16, 100),\n",
    "            # nn.LayerNorm(100, elementwise_affine=False),\n",
    "            # nn.BatchNorm1d(100, affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            QLinear(100, 10) if quant and w_linear else Linear(100, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 50, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(4*4*50, 500) if quant and w_linear else Linear(4*4*50, 500),\n",
    "            nn.ReLU(),\n",
    "            QLinear(500, 10) if quant and w_linear else Linear(500, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Softmax()\n",
    "        ) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "AlexNet for CIFAR-10\n",
    "https://github.com/soapisnotfat/pytorch-cifar10/blob/master/models/AlexNet.py\n",
    "\"\"\"\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "            \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "        )\n",
    "\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                QLinear(256, 256) if quant and w_linear else Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                QLinear(256, 256) if quant and w_linear else Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                QLinear(256, 10) if quant and w_linear else Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.cnn3(out)\n",
    "        out = self.cnn4(out)\n",
    "        out = self.cnn5(out)\n",
    "        out = self.fc_layers(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Standard implementation of VGG16 for ImageNet\n",
    "\"\"\"\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),  # Conv1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),  # Conv2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool1\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # Conv3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),  # Conv4\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool2\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  # Conv5\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  # Conv6\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  # Conv7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool3\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # Conv8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv10\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool4\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv11\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv12\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv13\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    "Standard implementation of VGG16 for CIFAR-10\n",
    "https://www.kaggle.com/willzy/vgg16-with-cifar10\n",
    "\"\"\"\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(4608, 4096),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.Linear(4096, 1000)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    #     self.features = nn.Sequential(\n",
    "    #         nn.Conv2d(3, 32, 3, padding=1),  # Conv1\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(32, 32, 3, padding=1),  # Conv2\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool1\n",
    "\n",
    "    #         nn.Conv2d(32, 64, 3, padding=1),  # Conv3\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(64, 64, 3, padding=1),  # Conv4\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool2\n",
    "\n",
    "    #         nn.Conv2d(64, 128, 3, padding=1),  # Conv5\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(128, 128, 3, padding=1),  # Conv6\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(128, 128, 3, padding=1),  # Conv7\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool3\n",
    "\n",
    "    #         nn.Conv2d(128, 256, 3, padding=1),  # Conv8\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv9\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv10\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool4\n",
    "\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv11\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv12\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv13\n",
    "    #         nn.ReLU(),\n",
    "    #         # nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
    "    #     )\n",
    "\n",
    "    #     self.classifier = nn.Sequential(\n",
    "    #         nn.Flatten(),\n",
    "    #         QLinear(2 * 2 * 256, 512) if quant and w_linear else Linear(2 * 2 * 256, 512) ,\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Dropout(),\n",
    "    #         QLinear(512, 512) if quant and w_linear else Linear(512, 512),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Dropout(),\n",
    "    #         QLinear(512, 10) if quant and w_linear else Linear(512, 10),\n",
    "    #     )\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     out = self.features(x)\n",
    "    #     out = self.classifier(out)\n",
    "    #     return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# MNIST Dataset\n",
    "if ds == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=True,\n",
    "                                transform=transforms.ToTensor(),\n",
    "                                download=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "elif ds == 'CIFAR-10':\n",
    "    # CIFAR-10 Dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyTransformer.transformers.torchTransformer import TorchTransformer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if model_name == 'SecureML':\n",
    "    model = SecureML()\n",
    "elif model_name == \"MiniONN\":\n",
    "    model = MiniONN()\n",
    "elif model_name == \"LeNet\":\n",
    "    model = LeNet()\n",
    "elif model_name == \"AlexNet\":    \n",
    "    model = AlexNet()\n",
    "# model = VGG16()\n",
    "\n",
    "if quant and w_conv:\n",
    "    transformer = TorchTransformer()\n",
    "    transformer.register(torch.nn.Conv2d, QConv2d)\n",
    "    # transformer.register(torch.nn.Linear, QLinear)        # TODO: currently, there is a bug.\n",
    "    model = transformer.trans_layers(model)\n",
    "# for name, modules in model.named_modules():\n",
    "#     print(name)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "logger = SummaryWriter(log_dir = \"log\")\n",
    "\n",
    "sufix = 'quant' if quant else 'full-precision'\n",
    "base_path = 'log/' + model_name + '/' + sufix + \"/\"\n",
    "\n",
    "if log_acc:\n",
    "    lossF = open(base_path + \"loss.txt\", 'w')\n",
    "    trainAccF = open(base_path + \"train_acc.txt\", 'w')\n",
    "    testAccF = open(base_path + \"test_acc.txt\", 'w')\n",
    "\n",
    "def train(epoch):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (feature, label) in enumerate(train_loader):\n",
    "        feature, label = Variable(feature), Variable(label)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # re-scale from [0,1] to [-1,1]\n",
    "        if re_scale: \n",
    "            feature = 2 * feature - 1\n",
    "        output = model(feature)\n",
    "        label_raw = label\n",
    "        label = F.one_hot(label, num_classes=10).type(torch.float)\n",
    "        loss = F.cross_entropy(output, label_raw)\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        global_iter_num = epoch * len(train_loader) + batch_idx\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(feature), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), total_loss/100))\n",
    "            total_loss = 0\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(label_raw.data.view_as(pred)).cpu().sum()\n",
    "            train_acc = 100. * correct / len(feature)\n",
    "\n",
    "            acc = test()\n",
    "\n",
    "            if log_acc:\n",
    "                lossF.write(f'{global_iter_num}\\t{loss.item()}\\n')\n",
    "                testAccF.write(f'{global_iter_num}\\t{acc}\\n')\n",
    "                trainAccF.write(f'{global_iter_num}\\t{train_acc}\\n')\n",
    "\n",
    "            \"\"\"\n",
    "            log into tensorboard\n",
    "            https://stackoverflow.com/questions/37304461/tensorflow-importing-data-from-a-tensorboard-tfevent-file\n",
    "            \"\"\"\n",
    "            # logger.add_scalar(\"train loss\", loss.item(), global_iter_num)\n",
    "            # logger.add_scalar(\"train accuracy\", train_acc, global_iter_num)\n",
    "            # logger.add_scalar(\"test accuracy\", acc, global_iter_num)\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     logger.add_histogram(name, param.data.numpy(), global_iter_num)\n",
    "            #     logger.add_histogram(name+\"_grad\", param.grad.numpy(), global_iter_num)\n",
    "            # for name, module in model.named_modules():\n",
    "            #     logger.add_histogram(name, activations[name], global_iter_num)\n",
    "  \n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if re_scale:\n",
    "            data = 2 * data -1\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook for logging activation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" hook for activation \"\"\"\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "for name, module in model.named_modules():\n",
    "    module.register_forward_hook(get_activation(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Training Setting ====================\n",
      "Batch size:  32\n",
      "Dataset:  CIFAR-10\n",
      "Data re-scale:  False\n",
      "Model: \n",
      "AlexNet(\n",
      "  (cnn1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(9, 9))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (cnn2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (cnn3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (cnn4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (cnn5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "==================== Quantization Setting ====================\n",
      "Quant: False, \tw_linear: True,  \tw_conv: True\n",
      "\tForward: \t32\t12\n",
      "\tBackward: \t64\t20\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20, \"Training Setting\", \"=\"*20)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Dataset: \", ds)\n",
    "print(\"Data re-scale: \", re_scale)\n",
    "print(\"Model: \")\n",
    "print(model)\n",
    "\n",
    "print(\"=\"*20, \"Quantization Setting\", \"=\"*20)\n",
    "print(\"Quant: {0}, \\tw_linear: {1},  \\tw_conv: {2}\".format(quant, w_linear, w_conv))\n",
    "print(\"\\tForward: \\t{0}\\t{1}\\n\\tBackward: \\t{2}\\t{3}\".format(bitw_forward, bitf_forward,  bitw_backward, bitf_backward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 0.013266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-126-9220afa7df5a>:68: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0834, Accuracy: 6264/10000 (63%)\n",
      "\n",
      "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 0.904006\n",
      "\n",
      "Test set: Average loss: 1.1657, Accuracy: 6103/10000 (61%)\n",
      "\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 0.872559\n",
      "\n",
      "Test set: Average loss: 1.1283, Accuracy: 6236/10000 (62%)\n",
      "\n",
      "Train Epoch: 0 [9600/50000 (19%)]\tLoss: 0.923473\n",
      "\n",
      "Test set: Average loss: 1.1159, Accuracy: 6250/10000 (62%)\n",
      "\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 0.879536\n",
      "\n",
      "Test set: Average loss: 1.1186, Accuracy: 6231/10000 (62%)\n",
      "\n",
      "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 0.909734\n",
      "\n",
      "Test set: Average loss: 1.1810, Accuracy: 6063/10000 (61%)\n",
      "\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 0.920193\n",
      "\n",
      "Test set: Average loss: 1.0903, Accuracy: 6302/10000 (63%)\n",
      "\n",
      "Train Epoch: 0 [22400/50000 (45%)]\tLoss: 0.887454\n",
      "\n",
      "Test set: Average loss: 1.0832, Accuracy: 6285/10000 (63%)\n",
      "\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 0.901205\n",
      "\n",
      "Test set: Average loss: 1.0788, Accuracy: 6321/10000 (63%)\n",
      "\n",
      "Train Epoch: 0 [28800/50000 (58%)]\tLoss: 0.916437\n",
      "\n",
      "Test set: Average loss: 1.0901, Accuracy: 6373/10000 (64%)\n",
      "\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 0.950288\n",
      "\n",
      "Test set: Average loss: 1.0872, Accuracy: 6320/10000 (63%)\n",
      "\n",
      "Train Epoch: 0 [35200/50000 (70%)]\tLoss: 0.900704\n",
      "\n",
      "Test set: Average loss: 1.0472, Accuracy: 6504/10000 (65%)\n",
      "\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.884532\n",
      "\n",
      "Test set: Average loss: 1.1162, Accuracy: 6343/10000 (63%)\n",
      "\n",
      "Train Epoch: 0 [41600/50000 (83%)]\tLoss: 0.914962\n",
      "\n",
      "Test set: Average loss: 1.0630, Accuracy: 6416/10000 (64%)\n",
      "\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 0.926325\n",
      "\n",
      "Test set: Average loss: 1.0682, Accuracy: 6410/10000 (64%)\n",
      "\n",
      "Train Epoch: 0 [48000/50000 (96%)]\tLoss: 0.872917\n",
      "\n",
      "Test set: Average loss: 1.1334, Accuracy: 6304/10000 (63%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0859, Accuracy: 6360/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.006752\n",
      "\n",
      "Test set: Average loss: 1.0824, Accuracy: 6354/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 0.777683\n",
      "\n",
      "Test set: Average loss: 1.0709, Accuracy: 6454/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 0.790021\n",
      "\n",
      "Test set: Average loss: 1.0623, Accuracy: 6426/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 0.792320\n",
      "\n",
      "Test set: Average loss: 1.0883, Accuracy: 6475/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 0.793150\n",
      "\n",
      "Test set: Average loss: 1.1080, Accuracy: 6391/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 0.782513\n",
      "\n",
      "Test set: Average loss: 1.1179, Accuracy: 6426/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 0.824389\n",
      "\n",
      "Test set: Average loss: 1.0438, Accuracy: 6468/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 0.812213\n",
      "\n",
      "Test set: Average loss: 1.0750, Accuracy: 6471/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 0.814652\n",
      "\n",
      "Test set: Average loss: 1.0872, Accuracy: 6389/10000 (64%)\n",
      "\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 0.827472\n",
      "\n",
      "Test set: Average loss: 1.0361, Accuracy: 6575/10000 (66%)\n",
      "\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 0.810575\n",
      "\n",
      "Test set: Average loss: 1.0750, Accuracy: 6539/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 0.831958\n",
      "\n",
      "Test set: Average loss: 1.0442, Accuracy: 6551/10000 (66%)\n",
      "\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 0.797206\n",
      "\n",
      "Test set: Average loss: 1.1016, Accuracy: 6341/10000 (63%)\n",
      "\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 0.825442\n",
      "\n",
      "Test set: Average loss: 1.0650, Accuracy: 6485/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 0.828429\n",
      "\n",
      "Test set: Average loss: 1.0515, Accuracy: 6507/10000 (65%)\n",
      "\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 0.850389\n",
      "\n",
      "Test set: Average loss: 1.0757, Accuracy: 6431/10000 (64%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0592, Accuracy: 6515/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.011235\n",
      "\n",
      "Test set: Average loss: 1.0400, Accuracy: 6578/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 0.681074\n",
      "\n",
      "Test set: Average loss: 1.0798, Accuracy: 6567/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 0.740588\n",
      "\n",
      "Test set: Average loss: 1.1649, Accuracy: 6308/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 0.680485\n",
      "\n",
      "Test set: Average loss: 1.0907, Accuracy: 6523/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 0.734659\n",
      "\n",
      "Test set: Average loss: 1.0388, Accuracy: 6625/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 0.740368\n",
      "\n",
      "Test set: Average loss: 1.0374, Accuracy: 6545/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 0.698422\n",
      "\n",
      "Test set: Average loss: 1.0889, Accuracy: 6529/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 0.752004\n",
      "\n",
      "Test set: Average loss: 1.0303, Accuracy: 6608/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.724612\n",
      "\n",
      "Test set: Average loss: 1.0636, Accuracy: 6549/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 0.732977\n",
      "\n",
      "Test set: Average loss: 1.0560, Accuracy: 6558/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 0.750374\n",
      "\n",
      "Test set: Average loss: 1.0387, Accuracy: 6604/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 0.743993\n",
      "\n",
      "Test set: Average loss: 1.0623, Accuracy: 6573/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.765791\n",
      "\n",
      "Test set: Average loss: 1.0566, Accuracy: 6506/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 0.739333\n",
      "\n",
      "Test set: Average loss: 1.0599, Accuracy: 6556/10000 (66%)\n",
      "\n",
      "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 0.747258\n",
      "\n",
      "Test set: Average loss: 1.0426, Accuracy: 6546/10000 (65%)\n",
      "\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 0.732694\n",
      "\n",
      "Test set: Average loss: 1.0872, Accuracy: 6464/10000 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0527, Accuracy: 6566/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.004263\n",
      "\n",
      "Test set: Average loss: 1.0757, Accuracy: 6498/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [3200/50000 (6%)]\tLoss: 0.643114\n",
      "\n",
      "Test set: Average loss: 1.0570, Accuracy: 6703/10000 (67%)\n",
      "\n",
      "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 0.618461\n",
      "\n",
      "Test set: Average loss: 1.0339, Accuracy: 6657/10000 (67%)\n",
      "\n",
      "Train Epoch: 3 [9600/50000 (19%)]\tLoss: 0.651797\n",
      "\n",
      "Test set: Average loss: 1.0921, Accuracy: 6582/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.651011\n",
      "\n",
      "Test set: Average loss: 1.0731, Accuracy: 6546/10000 (65%)\n",
      "\n",
      "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 0.636902\n",
      "\n",
      "Test set: Average loss: 1.1071, Accuracy: 6607/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 0.638647\n",
      "\n",
      "Test set: Average loss: 1.0418, Accuracy: 6604/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [22400/50000 (45%)]\tLoss: 0.668543\n",
      "\n",
      "Test set: Average loss: 1.1335, Accuracy: 6447/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.646608\n",
      "\n",
      "Test set: Average loss: 1.0595, Accuracy: 6640/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [28800/50000 (58%)]\tLoss: 0.662627\n",
      "\n",
      "Test set: Average loss: 1.0624, Accuracy: 6689/10000 (67%)\n",
      "\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 0.663787\n",
      "\n",
      "Test set: Average loss: 1.0685, Accuracy: 6643/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [35200/50000 (70%)]\tLoss: 0.667962\n",
      "\n",
      "Test set: Average loss: 1.0669, Accuracy: 6642/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.697673\n",
      "\n",
      "Test set: Average loss: 1.0385, Accuracy: 6645/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [41600/50000 (83%)]\tLoss: 0.695090\n",
      "\n",
      "Test set: Average loss: 1.0651, Accuracy: 6603/10000 (66%)\n",
      "\n",
      "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 0.650382\n",
      "\n",
      "Test set: Average loss: 1.1221, Accuracy: 6436/10000 (64%)\n",
      "\n",
      "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 0.701363\n",
      "\n",
      "Test set: Average loss: 1.0190, Accuracy: 6660/10000 (67%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.1348, Accuracy: 6498/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.009411\n",
      "\n",
      "Test set: Average loss: 1.1193, Accuracy: 6502/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [3200/50000 (6%)]\tLoss: 0.524148\n",
      "\n",
      "Test set: Average loss: 1.1285, Accuracy: 6654/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 0.568315\n",
      "\n",
      "Test set: Average loss: 1.1880, Accuracy: 6594/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [9600/50000 (19%)]\tLoss: 0.569715\n",
      "\n",
      "Test set: Average loss: 1.1070, Accuracy: 6584/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.561284\n",
      "\n",
      "Test set: Average loss: 1.1961, Accuracy: 6590/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 0.591615\n",
      "\n",
      "Test set: Average loss: 1.0696, Accuracy: 6664/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 0.626549\n",
      "\n",
      "Test set: Average loss: 1.0735, Accuracy: 6538/10000 (65%)\n",
      "\n",
      "Train Epoch: 4 [22400/50000 (45%)]\tLoss: 0.626168\n",
      "\n",
      "Test set: Average loss: 1.0638, Accuracy: 6679/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.580767\n",
      "\n",
      "Test set: Average loss: 1.0895, Accuracy: 6591/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [28800/50000 (58%)]\tLoss: 0.636795\n",
      "\n",
      "Test set: Average loss: 1.0736, Accuracy: 6598/10000 (66%)\n",
      "\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 0.604722\n",
      "\n",
      "Test set: Average loss: 1.0787, Accuracy: 6682/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [35200/50000 (70%)]\tLoss: 0.615126\n",
      "\n",
      "Test set: Average loss: 1.0205, Accuracy: 6762/10000 (68%)\n",
      "\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.614558\n",
      "\n",
      "Test set: Average loss: 1.0534, Accuracy: 6666/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [41600/50000 (83%)]\tLoss: 0.612103\n",
      "\n",
      "Test set: Average loss: 1.0640, Accuracy: 6680/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 0.599627\n",
      "\n",
      "Test set: Average loss: 1.0660, Accuracy: 6678/10000 (67%)\n",
      "\n",
      "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 0.622143\n",
      "\n",
      "Test set: Average loss: 1.0775, Accuracy: 6703/10000 (67%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0346, Accuracy: 6754/10000 (68%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "if log_acc:\n",
    "    lossF.close()\n",
    "    testAccF.close()\n",
    "    trainAccF.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/VGG16/full-precision/cnn1.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn1.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn1.3.weight.pdf\n",
      "log/VGG16/full-precision/cnn1.3.bias.pdf\n",
      "log/VGG16/full-precision/cnn2.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn2.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn2.3.weight.pdf\n",
      "log/VGG16/full-precision/cnn2.3.bias.pdf\n",
      "log/VGG16/full-precision/cnn3.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn3.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn4.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn4.0.bias.pdf\n",
      "log/VGG16/full-precision/cnn5.0.weight.pdf\n",
      "log/VGG16/full-precision/cnn5.0.bias.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.weight.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.bias.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.weight.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.bias.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.weight.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-131-82dad0df18b2>:23: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/VGG16/full-precision/fc_layers.5.bias.pdf\n",
      "log/VGG16/full-precision/.act.pdf\n",
      "log/VGG16/full-precision/cnn1.act.pdf\n",
      "log/VGG16/full-precision/cnn1.0.act.pdf\n",
      "log/VGG16/full-precision/cnn1.1.act.pdf\n",
      "log/VGG16/full-precision/cnn1.2.act.pdf\n",
      "log/VGG16/full-precision/cnn1.3.act.pdf\n",
      "log/VGG16/full-precision/cnn2.act.pdf\n",
      "log/VGG16/full-precision/cnn2.0.act.pdf\n",
      "log/VGG16/full-precision/cnn2.1.act.pdf\n",
      "log/VGG16/full-precision/cnn2.2.act.pdf\n",
      "log/VGG16/full-precision/cnn2.3.act.pdf\n",
      "log/VGG16/full-precision/cnn3.act.pdf\n",
      "log/VGG16/full-precision/cnn3.0.act.pdf\n",
      "log/VGG16/full-precision/cnn3.1.act.pdf\n",
      "log/VGG16/full-precision/cnn4.act.pdf\n",
      "log/VGG16/full-precision/cnn4.0.act.pdf\n",
      "log/VGG16/full-precision/cnn4.1.act.pdf\n",
      "log/VGG16/full-precision/cnn5.act.pdf\n",
      "log/VGG16/full-precision/cnn5.0.act.pdf\n",
      "log/VGG16/full-precision/cnn5.1.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.0.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.1.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.2.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.3.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.4.act.pdf\n",
      "log/VGG16/full-precision/fc_layers.5.act.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlabel = \"Expectation\"\n",
    "ylable = \"Density\"\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set()\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "bin_num = 10\n",
    "\n",
    "\n",
    "font_family = {'family': 'Times New Roman', 'size': 14}\n",
    "font_size = {'ax_label': 24, 'ax_tick': 22, 'inner_tick': 14, 'text': 15, 'legend': 12}\n",
    "line_args = {'marker': 'o', 'markevery': bin_num, 'ls': '--', 'mfc': 'blue', 'mec': 'k'}\n",
    "\n",
    "sufix = 'quant' if quant else 'full-precision'\n",
    "base_path = 'log/' + model_name + '/' + sufix + \"/\"\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "def draw_handle(name, data):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    # plt.hist(data)\n",
    "    # ax = plt.subplot()\n",
    "    binw = 1\n",
    "    g = sns.histplot(data.flatten(), kde=True, fill=True, stat='probability', line_kws=line_args)\n",
    "    g.lines[0].set_color('crimson')\n",
    "\n",
    "    # STEM PLOT https://github.com/mwaskom/seaborn/issues/2344\n",
    "    # points = g.lines[0].get_path().vertices\n",
    "    # x, y = np.split(points, 2, 1)\n",
    "    # x = x[np.arange(0, len(x), int(len(x)/bin_num))]\n",
    "    # y = y[np.arange(0, len(y), int(len(y)/bin_num))]\n",
    "    # plt.stem(x, y, linefmt ='g--', markerfmt='b')\n",
    "    # STEM PLOT\n",
    "\n",
    "    # sns.displot(data.flatten(), kde=True)\n",
    "    # g = sns.kdeplot(data.flatten(), color='red', marker='o', markevery=10, linestyle=':')\n",
    "    # g.yaxis.set_major_formatter(PercentFormatter(1 / binw))\n",
    "    # sns.distplot(data.flatten(), kde=True, hist=False, norm_hist=True)\n",
    "    # plt.hist(data.flatten(), density=True)\n",
    "    # plt.xlabel(xlabel, fontdict=font_family)\n",
    "    # plt.ylabel(ylable, fontdict=font_family)\n",
    "    ax.set(ylabel=None)\n",
    "    plt.grid(True, alpha=0.5, linestyle='-.')\n",
    "    plt.title(name, fontdict=font_family)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    print(base_path + name + '.pdf')\n",
    "    plt.savefig(base_path + name + '.pdf')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # draw_handle(name.split('.')[0] + '.weight', param.data.numpy())\n",
    "    # draw_handle(name.split('.')[0] + '.weight_grad', param.grad.numpy())\n",
    "\n",
    "    draw_handle(name, param.data.numpy())\n",
    "    draw_handle(name + '_grad', param.grad.numpy())\n",
    "\n",
    "    # logger.add_histogram(name, param.data.numpy(), global_iter_num)\n",
    "    # logger.add_histogram(name+\"_grad\", param.grad.numpy(), global_iter_num)\n",
    "for name, module in model.named_modules():\n",
    "    draw_handle(name + '.act', activations[name])\n",
    "\n",
    "    # logger.add_histogram(name, activations[name], global_iter_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8e35784da7b5309346b9948007e2c8e432cfca57e220859a198433798687959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
