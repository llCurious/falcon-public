{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd.function import InplaceFunction, Function\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Conv2d\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5f240d3850>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training configs\n",
    "batch_size = 64\n",
    "bitw_forward = 32\n",
    "bitf_forward = 12\n",
    "\n",
    "bitw_backward = 64\n",
    "bitf_backward = 20\n",
    "\n",
    "re_scale = False\n",
    "\n",
    "use_LN = False\n",
    "\n",
    "quant = False\n",
    "w_linear = True\n",
    "w_conv = True\n",
    "\n",
    "# SecureML, MiniONN, LeNet, AlexNet, VGG16\n",
    "model_name = \"AlexNet\"\n",
    "\n",
    "shifting_scale = 1.\n",
    "\n",
    "# ds = 'CIFAR-10'\n",
    "ds = 'CIFAR-10' if model_name in ['AlexNet', 'VGG16'] else 'MNIST'\n",
    "\n",
    "verbose = True\n",
    "\n",
    "\"\"\"\n",
    "make sure all the random initiation of parameters are the same in different runs\n",
    "\"\"\"\n",
    "torch.manual_seed(0)\n",
    "# print(torch.rand(2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Quantized Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overflow_validate(val, bitw, location):\n",
    "    if val.abs().max() >= float(2**(bitw-1)):\n",
    "        raise Exception(\"Overflow {4}! Val: {0}, Bit-width: {1}, val: [-{2}, {3}]\".format(val.abs().max(), bitw, 1 << (bitw-1), (1 << (bitw-1)) - 1, location))\n",
    "\n",
    "class FxInput(InplaceFunction):\n",
    "    \"\"\"\n",
    "    Quantize w.r.t. qmax in forward pass; use STE in backward pass\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitw_backward, bitf_forward, bitf_backward, inplace=False, trunc=False):\n",
    "        ctx.inplace = inplace\n",
    "        if ctx.inplace:\n",
    "            ctx.mark_dirty(input)\n",
    "            output = input\n",
    "\n",
    "        else:\n",
    "            output = input.clone()\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow FxInput forward! Val: {0}, Bit-width: {1}, val: {2}\".format(val.max(), bitw, 1 << bitw))\n",
    "\n",
    "        if trunc:   # 2f-bit precision, require truncation by 2^f\n",
    "            output = (output * (1 << bitf_backward)).floor()\n",
    "            overflow_validate(output, bitw_backward, \"FxInput forward\")\n",
    "            output = (output / (1 << (bitf_backward - bitf_forward))).floor()     # FX-representation\n",
    "            overflow_validate(output, bitw_forward, \"FxInput forward\")\n",
    "            output = output / (1 << bitf_forward)                         # translate to FP\n",
    "            # if output.max() >= (1 << (bitw - bitf_after)):                                 # overflow judgement\n",
    "            #     print(\"overflow\")\n",
    "        else:       # 0-bit precision, require multiply by 2^f, currently not used\n",
    "            output = (output * (1 << bitf_forward)).floor()              # FX-representation\n",
    "            overflow_validate(output, bitw_forward, 'FxInput forward')\n",
    "            output = output / (1 << bitf_backward)                         # FP\n",
    "            # if output.max() >= (1 << (bitw - bitf_before)):                                 # overflow judgement\n",
    "            #     print(\"overflow\")\n",
    "        return output\n",
    "    \n",
    "    \"\"\"\n",
    "    Follow the STE method. Such that the gradient of Quant(x) is 1\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_input = grad_output\n",
    "        return grad_input, None, None, None, None, None, None\n",
    "\n",
    "class FxLinear(Function):\n",
    "    \"\"\"\n",
    "    FX multiplication for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    weight: (64, 16)\n",
    "    bias: (64, 16)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            detached_weight = weight.detach()\n",
    "            detached_weight.requires_grad_(True)\n",
    "\n",
    "            detached_bias = bias.detach()\n",
    "            detached_bias.requires_grad_(True)\n",
    "            \"\"\"\n",
    "            7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "            \"\"\"\n",
    "\n",
    "            qweight = FxInput.apply(detached_weight, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            qbias = FxInput.apply(detached_bias, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Print histogram of weights before and after quantization\n",
    "            \"\"\"\n",
    "            if not verbose:\n",
    "                n, bins, patches = plt.hist(detached_weight.data, 50)\n",
    "                plt.show()\n",
    "\n",
    "                n, bins, patches = plt.hist(qweight.data, 50)\n",
    "                plt.show()\n",
    "            \n",
    "\n",
    "            _output = F.linear(detached_input, qweight, qbias)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.saved_param = detached_weight, detached_bias\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'Linear Forward')\n",
    "        # if output.abs().max() >= float(2**(bitw_forward-1)):\n",
    "        #     raise Exception(\"Overflow Linear Forward! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _weight, _bias = ctx.saved_param\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_weight, grad_bias = _weight.grad, _bias.grad   # backward\n",
    "        grad_input = _input.grad\n",
    "        # print(\"ss: \", grad_input)\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow Linear Backward! Val: {0}, Bit-width: {1}\".format(val.max(), bitw))\n",
    "\n",
    "        grad_weight = (grad_weight * (1 << (bitf_forward + bitf_backward))).floor()     #FX\n",
    "        overflow_validate(grad_weight, bitw_backward, 'Linear Backward Weight')\n",
    "        grad_weight = (grad_weight / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_bias = (grad_bias * (1 << (bitf_forward + bitf_backward))).floor()         #FX\n",
    "        overflow_validate(grad_bias, bitw_backward, 'Linear Backward Bias')\n",
    "        grad_bias = (grad_bias / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Linear Backward Activation')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None\n",
    "\n",
    "class FxConv2D(Function):\n",
    "    \"\"\"\n",
    "    FX Convolution for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    weight: (64, 16)\n",
    "    bias: (64, 16)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weight, bias, stride, padding, dilation, groups, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            detached_weight = weight.detach()\n",
    "            detached_weight.requires_grad_(True)\n",
    "\n",
    "            detached_bias = bias.detach()\n",
    "            detached_bias.requires_grad_(True)\n",
    "            \"\"\"\n",
    "            7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "            \"\"\"\n",
    "            qweight = FxInput.apply(detached_weight, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            qbias = FxInput.apply(detached_bias, bitw_forward, bitw_backward, bitf_forward, bitf_backward, False, True) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "            \n",
    "            _output = F.conv2d(detached_input, qweight, qbias, stride, padding, dilation, groups)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.saved_param = detached_weight, detached_bias\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() * (1 << (2*bitf_forward))).floor()           # FX\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'Conv2D Forward')\n",
    "        # if output.abs().max() >= float(2**(bitw_forward-1)):\n",
    "        #     raise Exception(\"Overflow Conv2D Forward! Val: {0}, Bit-width: {1}, val: {2}\".format(output.max(), bitw_forward, 1 << bitw_forward))\n",
    "            \n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX truncate by bitf_forward bits\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _weight, _bias = ctx.saved_param\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_weight, grad_bias = _weight.grad, _bias.grad   # backward\n",
    "        grad_input = _input.grad\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     if val.abs().max() >= float(2**(bitw-1)):\n",
    "        #         raise Exception(\"Overflow Conv2D Backward! Val: {0}, Bit-width: {1}, ring: {2}\".format(val.max(), bitw, 1 << bitw))\n",
    "\n",
    "        grad_weight = (grad_weight * (1 << (bitf_forward + bitf_backward))).floor()     #FX\n",
    "        overflow_validate(grad_weight, bitw_backward, 'Conv2D Backward Weight')\n",
    "        grad_weight = (grad_weight / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_bias = (grad_bias * (1 << (bitf_forward + bitf_backward))).floor()         #FX\n",
    "        overflow_validate(grad_bias, bitw_backward, 'Conv2D Backward Bias')\n",
    "        grad_bias = (grad_bias / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Conv2D Backward Activation')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None, None, None\n",
    "\n",
    "class FxLayerNorm(Function):\n",
    "    \"\"\"\n",
    "    FX LayerNorm for normalization activation outputs.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def fx_mean(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        tar_mean = input.mean(-1, keepdim = True)\n",
    "\n",
    "        # fx mean imp\n",
    "        sum = input.sum(-1, keepdim = True)\n",
    "        divisor = input.shape[-1]\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)      # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        mean = sum * recp\n",
    "\n",
    "        # Fx --> Fp\n",
    "        mean_fx = (mean * (1 << (2 * bitf_forward))).floor()\n",
    "        mean = (mean_fx / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "        # definitely not overflow, so here pass the overflow validate\n",
    "\n",
    "        # print(\"tar mean: {0}, output_mean: {1}, diff: {2}\".format(tar_mean, mean, mean - tar_mean))\n",
    "        return mean\n",
    "\n",
    "    @staticmethod\n",
    "    def fx_var(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        tar_var = input.var(-1, keepdim = True)\n",
    "\n",
    "        # fx var imp\n",
    "        mean = FxLayerNorm.fx_mean(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "        diff = (input - mean)\n",
    "        diff = diff ** 2\n",
    "        diff_sum = diff.sum(-1, keepdim = True)\n",
    "        diff_sum = (diff_sum * (1 << (2*bitf_forward))).floor()\n",
    "        overflow_validate(diff_sum, bitw_forward, \"FX Variance\")\n",
    "\n",
    "        diff_sum = (diff_sum / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "\n",
    "        divisor = input.shape[-1]\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        var = diff_sum * recp\n",
    "\n",
    "        # Fx --> Fp\n",
    "        var_fx = (var * (1 << (2 * bitf_forward))).floor()\n",
    "        var = (var_fx / (1 << bitf_forward)).floor() / (1 << bitf_forward)\n",
    "\n",
    "        # print(\"tar var: {0}, output var: {1}, diff: {2}\".format(tar_var, var, var - tar_var))\n",
    "        return mean, var\n",
    "\n",
    "    @staticmethod\n",
    "    def fx_layer_norm(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward):\n",
    "        mean, var = FxLayerNorm.fx_var(input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "\n",
    "        dividend = input - mean\n",
    "\n",
    "        divisor = torch.sqrt(var)\n",
    "        recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n",
    "        \n",
    "        norm_output = dividend * recp\n",
    "        return norm_output\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "            print(detached_input)\n",
    "            fx_output = FxLayerNorm.fx_layer_norm(detached_input, bitw_forward, bitw_backward, bitf_forward, bitf_backward)\n",
    "            \n",
    "            _output = F.layer_norm(detached_input, (detached_input.shape[-1],))\n",
    "\n",
    "            print(\"tar norm: {0}, output norm: {1}, diff: {2}\".format(_output, fx_output, fx_output - _output))\n",
    "        \n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "        print(output)\n",
    "\n",
    "        overflow_validate(output, bitw_forward, 'LayerNorm Forward')\n",
    "        # if output.abs().max() >= 2 **(bitw_forward-1):\n",
    "        #     raise Exception(\"Overflow! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_input = _input.grad\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'LayerNorm Backward')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "class FxSoftmax(Function):\n",
    "    \"\"\"\n",
    "    FX softmax for linear layers. Difference is the backward gradients shall be truncated by 2^f as well\n",
    "    input: (32, 10)\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, bitw_forward, bitf_forward, bitw_backward, bitf_backward):\n",
    "        with torch.enable_grad():\n",
    "            detached_input = input.detach()\n",
    "            detached_input.requires_grad_(True)\n",
    "\n",
    "            \n",
    "            # TODO: modified to softmax. I think this should be done using polynomial approximation or iterative method\n",
    "            _output = F.softmax(detached_input)\n",
    "\n",
    "        ctx.saved_input = detached_input\n",
    "        ctx.bits = bitw_forward, bitf_forward, bitw_backward, bitf_backward\n",
    "        ctx.save_for_backward(_output)\n",
    "\n",
    "        # trunc after multiplication\n",
    "        output = (_output.detach() *(1 << (2*bitf_forward))).floor()            # FX\n",
    "        overflow_validate(output, bitw_forward, 'Softmax Forward')\n",
    "        # if output.max() >= 2 **bitw_forward:\n",
    "        #     raise Exception(\"Overflow! Val: {0}, Bit-width: {1}\".format(output.max(), bitw_forward))\n",
    "\n",
    "        output = (output / (1 << bitf_forward)).floor()                         # FX Trunc\n",
    "        output = output / (1 << bitf_forward)                                   # FP\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        _output, = ctx.saved_tensors\n",
    "        \"\"\"\n",
    "        grad_output: (64, 16), nothing should be done\n",
    "        grad_param (input, weight, bias): (64, 16) * (32, 10), should be extended. Here, only left-shift by (bitf_backward - bitf_forward) is required.\n",
    "        weights: forward + backward, Here, since the backward-precision weight is kept, we can directly perform right-shift.\n",
    "        \"\"\"\n",
    "        _input = ctx.saved_input\n",
    "        # print(\"grad_output: \", grad_output)\n",
    "        # grad_output.data = torch.tensor(7.6875)\n",
    "\n",
    "        # TODO: this actually should be modified, since 64-bit * 32-bit cannot be directly multiplied. Here, i use FP thus the computation can be carried out, despite the precision\n",
    "        with torch.enable_grad():\n",
    "            _output.backward(grad_output)\n",
    "\n",
    "        \"\"\"\n",
    "        (qweight)122 * 123 (4-bit) --> 15006 --> 937 --> 58.5625\n",
    "        7.625 * 7.6875 --> 58.6171875 --> 7503 --> trunc --> 937\n",
    "        7.6875 --> 123 (4-bit f) --> 61 (3-bit f) --> 7.625\n",
    "        \"\"\"\n",
    "        grad_input = _input.grad\n",
    "        # print(\"ss: \", grad_input)\n",
    "\n",
    "        bitw_forward, bitf_forward, bitw_backward, bitf_backward = ctx.bits\n",
    "\n",
    "        # def overflow_validate(val, bitw):\n",
    "        #     print(val.dtype)\n",
    "        #     if val.abs().max() >= 2 ** (bitw-1):\n",
    "        #         raise Exception(\"Overflow Conv2D! Val: {0}, Bit-width: {1}\".format(val.max(), bitw))\n",
    "\n",
    "        grad_input = (grad_input * (1 << (bitf_forward + bitf_backward))).floor()       #FX\n",
    "        overflow_validate(grad_input, bitw_backward, 'Softmax Backward')\n",
    "        grad_input = (grad_input / (1 << bitf_forward)).floor() / (1 << bitf_backward)\n",
    "\n",
    "        return grad_input, None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fx Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.2500)\n",
      "tensor(15.2500)\n",
      "tensor(7.6250, grad_fn=<FxInputBackward>)\n",
      "====================  FX Linear  ====================\n",
      "tensor(66.4225, grad_fn=<MseLossBackward0>)\n",
      "tensor(58.2500, grad_fn=<FxLinearBackward>)\n",
      "tensor(16.3000)\n",
      "tensor([124.2500])\n",
      "====================  LayerNorm  ====================\n",
      "tensor([[0.0496, 0.0768, 0.0088],\n",
      "        [0.0132, 0.0307, 0.0634]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.0496, 0.0768, 0.0088],\n",
      "        [0.0132, 0.0307, 0.0634]], requires_grad=True)\n",
      "tar norm: tensor([[ 0.1610,  1.1284, -1.2895],\n",
      "        [-1.0731, -0.2396,  1.3127]], grad_fn=<NativeLayerNormBackward0>), output norm: tensor([[ 0.1622,  1.1365, -1.2987],\n",
      "        [-1.0864, -0.2426,  1.3290]], grad_fn=<MulBackward0>), diff: tensor([[ 0.0012,  0.0081, -0.0092],\n",
      "        [-0.0133, -0.0029,  0.0163]], grad_fn=<SubBackward0>)\n",
      "tensor([[ 1.7706e+11,  1.2407e+12, -1.4178e+12],\n",
      "        [-1.1799e+12, -2.6346e+11,  1.4434e+12]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-62-09750be26726>:292: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  recp = torch.tensor(((1. / divisor) * (1 << bitf_forward))).floor() / (1 << bitf_forward)       # TODO: here should use godsmich to approximate the recp, which may incur additional precision loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1610,  1.1284, -1.2895],\n",
       "        [-1.0731, -0.2396,  1.3127]], grad_fn=<FxLayerNormBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(7.6875, requires_grad=True)\n",
    "\n",
    "y = FxInput.apply(x, 16, 32, 3, 4, False, True)\n",
    "# y = FxInput.apply(x, 5, 32, 3, 4, False, False)\n",
    "y.retain_grad()\n",
    "loss = y * y\n",
    "loss.backward()\n",
    "\n",
    "print(y.grad)\n",
    "print(x.grad)\n",
    "print(y)\n",
    "\n",
    "\n",
    "print(\"=\"*20, \" FX Linear \", \"=\"*20)\n",
    "x = torch.full([1], 7.625, requires_grad=True)  # 3-bit\n",
    "w = torch.full([1], 7.6875, requires_grad=True) # 4-bit to 3-bit qweight = 7.625\n",
    "b = torch.tensor(0.2, requires_grad=True)\n",
    "z = FxLinear.apply(x, w, b, 16, 3, 32, 4)       # 58.25\n",
    "z.retain_grad()\n",
    "loss = F.mse_loss(z, torch.tensor(50.1))\n",
    "loss.backward()\n",
    "\n",
    "print(loss)\n",
    "print(z)\n",
    "print(z.grad)\n",
    "print(x.grad)\n",
    "\n",
    "print(\"=\"*20, \" LayerNorm \", \"=\"*20)\n",
    "x = torch.rand([2, 3], requires_grad=True)  # 3-bit\n",
    "func = FxLayerNorm.apply\n",
    "print(x / 10)\n",
    "func(x / 10, 64, 20, 32, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLinear(nn.Linear):\n",
    "    \"\"\"QLinear using FX.\"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "\n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \"\"\"\n",
    "        Print histogram of initialized weights using different f-bits quantization\n",
    "        \"\"\"\n",
    "        if not verbose:\n",
    "            n, bins, patches = plt.hist(self.weight.data, 20)\n",
    "            plt.show()\n",
    "\n",
    "            f_bits = [20, 12, 7, 6, 5]\n",
    "            for i in f_bits:\n",
    "                tmp_weight =  nn.Parameter(((self.weight / shifting_scale * (1 << i))).round() / (1 << i))    # FX64 copy\n",
    "                n, bins, patches = plt.hist(tmp_weight.data, 20)\n",
    "                plt.show()\n",
    "            \n",
    "        self.weight =  nn.Parameter(((self.weight / shifting_scale * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        self.bias =  nn.Parameter(((self.bias / shifting_scale * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Hack the weight initialitization\n",
    "    TODO: modify the kaiming init. Since the backend PyTorch init follows the uniform(-1/sqrt(in_features), 1/sqrt(in_features)) \n",
    "    https://github.com/pytorch/pytorch/issues/57109\n",
    "    \"\"\"\n",
    "    def reset_parameters(self) -> None:\n",
    "        return super().reset_parameters()\n",
    "    # we here assume all the input are already quantized to fixed-point numbers\n",
    "    def forward(self, input):\n",
    "        output = FxLinear.apply(input, self.weight, self.bias, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward) # FX32 weight, should be truncated by (16 - 10) bits\n",
    "\n",
    "        # qoutput = FxSimulation.apply(output, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)      # truncation by 2^f\n",
    "\n",
    "        return output\n",
    "\n",
    "class QConv2d(nn.Conv2d):\n",
    "    \"\"\"QConv2d using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QConv2d, self).__init__(in_channels, out_channels, kernel_size,\n",
    "                                      stride, padding, dilation, groups, bias)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "\n",
    "        self.weight =  nn.Parameter(((self.weight * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        self.bias =  nn.Parameter(((self.bias * (1 << bitf_backward))).round() / (1 << self.bitf_backward))    # FX64 copy\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxConv2D.apply(input, self.weight, self.bias, \n",
    "                            self.stride, self.padding, self.dilation, self.groups, \n",
    "                            self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        # output = F.conv2d(input, qweight, self.bias, self.stride,\n",
    "        #                     self.padding, self.dilation, self.groups)\n",
    "        return output\n",
    "\n",
    "class QLayerNorm(nn.LayerNorm):\n",
    "    \"\"\"QLayerNorm using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QLayerNorm, self).__init__(input)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxSoftmax.apply(input, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        return output\n",
    "\n",
    "class QSoftmax(nn.Softmax):\n",
    "    \"\"\"QSoftmax using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, bitw_forward=bitw_forward, bitw_backward = bitw_backward, bitf_forward=bitf_forward, bitf_backward=bitf_backward):\n",
    "        super(QSoftmax, self).__init__(input)\n",
    "        self.bitw_forward = bitw_forward\n",
    "        self.bitw_backward = bitw_backward\n",
    "        \n",
    "        self.bitf_forward = bitf_forward\n",
    "        self.bitf_backward = bitf_backward\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = FxSoftmax.apply(input, self.bitw_forward, self.bitf_forward, self.bitw_backward, self.bitf_backward)\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "This two functions seems does not need to separately implement FX-version, since the precision does not change during computation\n",
    "\"\"\"\n",
    "class QReLU(nn.ReLU):\n",
    "    \"\"\"QReLU using FX.\"\"\"\n",
    "\n",
    "    def __init__(self, input, num_bits=8, quant=False, quant_scale=False):\n",
    "        super(QReLU, self).__init__(input)\n",
    "        # self.num_bits = num_bits\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = F.relu(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "class QMaxPooling(nn.MaxPool2d):\n",
    "    \"\"\"\n",
    "    FX for MaxPooling.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride, padding = 0, dilation = 1, return_indices = False, ceil_mode = False, num_bits=8, quant=False, quant_scale=False) -> None:\n",
    "        super().__init__(kernel_size, stride=stride, padding=padding, dilation=dilation, return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "\n",
    "        # self.quant = DoReFaQuantizeLayer(num_bits=num_bits, quant=quant, quant_scale=quant_scale)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # assert(input.dtype == torch.int32)\n",
    "        output = F.max_pool2d(input, self.kernel_size, self.stride, self.padding, self.dilation, self.ceil_mode)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SecureML(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(28*28, 128) if quant else Linear(28*28, 128),\n",
    "            # nn.LayerNorm(128, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            QLinear(128, 128) if quant else Linear(128, 128),\n",
    "            # nn.LayerNorm(128, elementwise_affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Sequential(\n",
    "            QLinear(128, 10) if quant else Linear(128, 10),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "class MiniONN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(4*4*16, 100) if quant and w_linear else Linear(4*4*16, 100),\n",
    "            # nn.LayerNorm(100, elementwise_affine=False),\n",
    "            # nn.BatchNorm1d(100, affine=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            QLinear(100, 10) if quant and w_linear else Linear(100, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(20, 50, 5, 1, 'valid'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            QLinear(4*4*50, 500) if quant and w_linear else Linear(4*4*50, 500),\n",
    "            nn.ReLU(),\n",
    "            QLinear(500, 10) if quant and w_linear else Linear(500, 10),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Softmax()\n",
    "        ) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "AlexNet for CIFAR-10\n",
    "https://github.com/soapisnotfat/pytorch-cifar10/blob/master/models/AlexNet.py\n",
    "\"\"\"\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "            \n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=9),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(num_features=96),\n",
    "        )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "        )\n",
    "\n",
    "        self.cnn3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.cnn5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                QLinear(256, 256) if quant and w_linear else Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                QLinear(256, 256) if quant and w_linear else Linear(256, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                QLinear(256, 10) if quant and w_linear else Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 1024),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(1024, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.fc_layers = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=4),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(9216, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, 1000),\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.cnn3(out)\n",
    "        out = self.cnn4(out)\n",
    "        out = self.cnn5(out)\n",
    "        out = self.fc_layers(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "Standard implementation of VGG16 for ImageNet\n",
    "\"\"\"\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),  # Conv1\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),  # Conv2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool1\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # Conv3\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),  # Conv4\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool2\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),  # Conv5\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  # Conv6\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),  # Conv7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool3\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, padding=1),  # Conv8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv9\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv10\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # Pool4\n",
    "\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv11\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv12\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),  # Conv13\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(7*7*512, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    "Standard implementation of VGG16 for CIFAR-10\n",
    "https://www.kaggle.com/willzy/vgg16-with-cifar10\n",
    "\"\"\"\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        if num_classes == 10:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 10),\n",
    "            )\n",
    "        elif num_classes == 200:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 200),\n",
    "            )\n",
    "        elif num_classes == 1000:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.AvgPool2d(2),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(4608, 4096),\n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.Linear(4096, 1000)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    #     self.features = nn.Sequential(\n",
    "    #         nn.Conv2d(3, 32, 3, padding=1),  # Conv1\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(32, 32, 3, padding=1),  # Conv2\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool1\n",
    "\n",
    "    #         nn.Conv2d(32, 64, 3, padding=1),  # Conv3\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(64, 64, 3, padding=1),  # Conv4\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool2\n",
    "\n",
    "    #         nn.Conv2d(64, 128, 3, padding=1),  # Conv5\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(128, 128, 3, padding=1),  # Conv6\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(128, 128, 3, padding=1),  # Conv7\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool3\n",
    "\n",
    "    #         nn.Conv2d(128, 256, 3, padding=1),  # Conv8\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv9\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv10\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.MaxPool2d(2, 2),  # Pool4\n",
    "\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv11\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv12\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Conv2d(256, 256, 3, padding=1),  # Conv13\n",
    "    #         nn.ReLU(),\n",
    "    #         # nn.MaxPool2d(2, 2)  # Pool5 是否还需要此池化层，每个通道的数据已经被降为1*1\n",
    "    #     )\n",
    "\n",
    "    #     self.classifier = nn.Sequential(\n",
    "    #         nn.Flatten(),\n",
    "    #         QLinear(2 * 2 * 256, 512) if quant and w_linear else Linear(2 * 2 * 256, 512) ,\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Dropout(),\n",
    "    #         QLinear(512, 512) if quant and w_linear else Linear(512, 512),\n",
    "    #         nn.ReLU(),\n",
    "    #         nn.Dropout(),\n",
    "    #         QLinear(512, 10) if quant and w_linear else Linear(512, 10),\n",
    "    #     )\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     out = self.features(x)\n",
    "    #     out = self.classifier(out)\n",
    "    #     return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# MNIST Dataset\n",
    "if ds == 'MNIST':\n",
    "    train_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=True,\n",
    "                                transform=transforms.ToTensor(),\n",
    "                                download=True)\n",
    "\n",
    "    test_dataset = datasets.MNIST(root='./data/',\n",
    "                                train=False,\n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "elif ds == 'CIFAR-10':\n",
    "    # CIFAR-10 Dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyTransformer.transformers.torchTransformer import TorchTransformer\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if model_name == 'SecureML':\n",
    "    model = SecureML()\n",
    "elif model_name == \"MiniONN\":\n",
    "    model = MiniONN()\n",
    "elif model_name == \"LeNet\":\n",
    "    model = LeNet()\n",
    "elif model_name == \"AlexNet\":    \n",
    "    model = AlexNet()\n",
    "# model = VGG16()\n",
    "\n",
    "if quant and w_conv:\n",
    "    transformer = TorchTransformer()\n",
    "    transformer.register(torch.nn.Conv2d, QConv2d)\n",
    "    # transformer.register(torch.nn.Linear, QLinear)        # TODO: currently, there is a bug.\n",
    "    model = transformer.trans_layers(model)\n",
    "# for name, modules in model.named_modules():\n",
    "#     print(name)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "logger = SummaryWriter(log_dir = \"log\")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (feature, label) in enumerate(train_loader):\n",
    "        feature, label = Variable(feature), Variable(label)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # re-scale from [0,1] to [-1,1]\n",
    "        if re_scale: \n",
    "            feature = 2 * feature - 1\n",
    "        output = model(feature)\n",
    "        label_raw = label\n",
    "        label = F.one_hot(label, num_classes=10).type(torch.float)\n",
    "        loss = F.cross_entropy(output, label_raw)\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        global_iter_num = epoch * len(train_loader) + batch_idx + 1\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(feature), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), total_loss/100))\n",
    "            total_loss = 0\n",
    "\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(label_raw.data.view_as(pred)).cpu().sum()\n",
    "            train_acc = 100. * correct / len(feature)\n",
    "\n",
    "            acc = test()\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            log into tensorboard\n",
    "            https://stackoverflow.com/questions/37304461/tensorflow-importing-data-from-a-tensorboard-tfevent-file\n",
    "            \"\"\"\n",
    "            logger.add_scalar(\"train loss\", loss.item(), global_iter_num)\n",
    "            logger.add_scalar(\"train accuracy\", train_acc, global_iter_num)\n",
    "            logger.add_scalar(\"test accuracy\", acc, global_iter_num)\n",
    "            for name, param in model.named_parameters():\n",
    "                logger.add_histogram(name, param.data.numpy(), global_iter_num)\n",
    "                logger.add_histogram(name+\"_grad\", param.grad.numpy(), global_iter_num)\n",
    "            for name, module in model.named_modules():\n",
    "                logger.add_histogram(name, activations[name], global_iter_num)\n",
    "                \n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if re_scale:\n",
    "            data = 2 * data -1\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook for logging activation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" hook for activation \"\"\"\n",
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "for name, module in model.named_modules():\n",
    "    module.register_forward_hook(get_activation(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Training Setting ====================\n",
      "Batch size:  32\n",
      "Dataset:  CIFAR-10\n",
      "Data re-scale:  False\n",
      "Model: \n",
      "AlexNet(\n",
      "  (cnn1): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(9, 9))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (cnn2): Sequential(\n",
      "    (0): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (cnn3): Sequential(\n",
      "    (0): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (cnn4): Sequential(\n",
      "    (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (cnn5): Sequential(\n",
      "    (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "==================== Quantization Setting ====================\n",
      "Quant: False, \tw_linear: True,  \tw_conv: True\n",
      "\tForward: \t32\t12\n",
      "\tBackward: \t64\t20\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*20, \"Training Setting\", \"=\"*20)\n",
    "print(\"Batch size: \", batch_size)\n",
    "print(\"Dataset: \", ds)\n",
    "print(\"Data re-scale: \", re_scale)\n",
    "print(\"Model: \")\n",
    "print(model)\n",
    "\n",
    "print(\"=\"*20, \"Quantization Setting\", \"=\"*20)\n",
    "print(\"Quant: {0}, \\tw_linear: {1},  \\tw_conv: {2}\".format(quant, w_linear, w_conv))\n",
    "print(\"\\tForward: \\t{0}\\t{1}\\n\\tBackward: \\t{2}\\t{3}\".format(bitw_forward, bitf_forward,  bitw_backward, bitf_backward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 0.022906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-d5cfd72ff398>:57: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n",
      "/home/whq/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.3035, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 2.304071\n",
      "\n",
      "Test set: Average loss: 2.3034, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.303971\n",
      "\n",
      "Test set: Average loss: 2.3030, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 2.302267\n",
      "\n",
      "Test set: Average loss: 2.3044, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 2.303759\n",
      "\n",
      "Test set: Average loss: 2.3033, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 2.303670\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.302936\n",
      "\n",
      "Test set: Average loss: 2.3025, Accuracy: 1594/10000 (16%)\n",
      "\n",
      "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 2.302432\n",
      "\n",
      "Test set: Average loss: 2.3019, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 2.301281\n",
      "\n",
      "Test set: Average loss: 2.2981, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 2.287814\n",
      "\n",
      "Test set: Average loss: 2.2550, Accuracy: 1822/10000 (18%)\n",
      "\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 2.106306\n",
      "\n",
      "Test set: Average loss: 2.0352, Accuracy: 1933/10000 (19%)\n",
      "\n",
      "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 2.003109\n",
      "\n",
      "Test set: Average loss: 1.9899, Accuracy: 1947/10000 (19%)\n",
      "\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.956472\n",
      "\n",
      "Test set: Average loss: 1.9630, Accuracy: 2078/10000 (21%)\n",
      "\n",
      "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.953375\n",
      "\n",
      "Test set: Average loss: 1.9189, Accuracy: 2129/10000 (21%)\n",
      "\n",
      "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.925906\n",
      "\n",
      "Test set: Average loss: 1.9023, Accuracy: 2110/10000 (21%)\n",
      "\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.896783\n",
      "\n",
      "Test set: Average loss: 1.8447, Accuracy: 2977/10000 (30%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.7624, Accuracy: 2771/10000 (28%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 0.018074\n",
      "\n",
      "Test set: Average loss: 1.7697, Accuracy: 2824/10000 (28%)\n",
      "\n",
      "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.777471\n",
      "\n",
      "Test set: Average loss: 1.7705, Accuracy: 3075/10000 (31%)\n",
      "\n",
      "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.730333\n",
      "\n",
      "Test set: Average loss: 1.7363, Accuracy: 3063/10000 (31%)\n",
      "\n",
      "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.689892\n",
      "\n",
      "Test set: Average loss: 1.6631, Accuracy: 3578/10000 (36%)\n",
      "\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.693549\n",
      "\n",
      "Test set: Average loss: 1.6374, Accuracy: 3903/10000 (39%)\n",
      "\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.665800\n",
      "\n",
      "Test set: Average loss: 1.6209, Accuracy: 3796/10000 (38%)\n",
      "\n",
      "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.608688\n",
      "\n",
      "Test set: Average loss: 1.7204, Accuracy: 3812/10000 (38%)\n",
      "\n",
      "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.587684\n",
      "\n",
      "Test set: Average loss: 1.5662, Accuracy: 4259/10000 (43%)\n",
      "\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.547940\n",
      "\n",
      "Test set: Average loss: 1.5904, Accuracy: 3918/10000 (39%)\n",
      "\n",
      "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.557408\n",
      "\n",
      "Test set: Average loss: 1.4855, Accuracy: 4350/10000 (44%)\n",
      "\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.493793\n",
      "\n",
      "Test set: Average loss: 1.4631, Accuracy: 4583/10000 (46%)\n",
      "\n",
      "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.506148\n",
      "\n",
      "Test set: Average loss: 1.4757, Accuracy: 4462/10000 (45%)\n",
      "\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.478249\n",
      "\n",
      "Test set: Average loss: 1.5004, Accuracy: 4344/10000 (43%)\n",
      "\n",
      "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.465318\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 5):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/AlexNet/quant/cnn1.0.weight.pdf\n",
      "log/AlexNet/quant/cnn1.0.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn1.0.bias.pdf\n",
      "log/AlexNet/quant/cnn1.0.bias_grad.pdf\n",
      "log/AlexNet/quant/cnn1.3.weight.pdf\n",
      "log/AlexNet/quant/cnn1.3.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn1.3.bias.pdf\n",
      "log/AlexNet/quant/cnn1.3.bias_grad.pdf\n",
      "log/AlexNet/quant/cnn2.0.weight.pdf\n",
      "log/AlexNet/quant/cnn2.0.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn2.0.bias.pdf\n",
      "log/AlexNet/quant/cnn2.0.bias_grad.pdf\n",
      "log/AlexNet/quant/cnn2.3.weight.pdf\n",
      "log/AlexNet/quant/cnn2.3.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn2.3.bias.pdf\n",
      "log/AlexNet/quant/cnn2.3.bias_grad.pdf\n",
      "log/AlexNet/quant/cnn3.0.weight.pdf\n",
      "log/AlexNet/quant/cnn3.0.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn3.0.bias.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-350be02b19d1>:23: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log/AlexNet/quant/cnn3.0.bias_grad.pdf\n",
      "log/AlexNet/quant/cnn4.0.weight.pdf\n",
      "log/AlexNet/quant/cnn4.0.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn4.0.bias.pdf\n",
      "log/AlexNet/quant/cnn4.0.bias_grad.pdf\n",
      "log/AlexNet/quant/cnn5.0.weight.pdf\n",
      "log/AlexNet/quant/cnn5.0.weight_grad.pdf\n",
      "log/AlexNet/quant/cnn5.0.bias.pdf\n",
      "log/AlexNet/quant/cnn5.0.bias_grad.pdf\n",
      "log/AlexNet/quant/fc_layers.1.weight.pdf\n",
      "log/AlexNet/quant/fc_layers.1.weight_grad.pdf\n",
      "log/AlexNet/quant/fc_layers.1.bias.pdf\n",
      "log/AlexNet/quant/fc_layers.1.bias_grad.pdf\n",
      "log/AlexNet/quant/fc_layers.3.weight.pdf\n",
      "log/AlexNet/quant/fc_layers.3.weight_grad.pdf\n",
      "log/AlexNet/quant/fc_layers.3.bias.pdf\n",
      "log/AlexNet/quant/fc_layers.3.bias_grad.pdf\n",
      "log/AlexNet/quant/fc_layers.5.weight.pdf\n",
      "log/AlexNet/quant/fc_layers.5.weight_grad.pdf\n",
      "log/AlexNet/quant/fc_layers.5.bias.pdf\n",
      "log/AlexNet/quant/fc_layers.5.bias_grad.pdf\n",
      "log/AlexNet/quant/.act.pdf\n",
      "log/AlexNet/quant/cnn1.act.pdf\n",
      "log/AlexNet/quant/cnn1.0.act.pdf\n",
      "log/AlexNet/quant/cnn1.1.act.pdf\n",
      "log/AlexNet/quant/cnn1.2.act.pdf\n",
      "log/AlexNet/quant/cnn1.3.act.pdf\n",
      "log/AlexNet/quant/cnn2.act.pdf\n",
      "log/AlexNet/quant/cnn2.0.act.pdf\n",
      "log/AlexNet/quant/cnn2.1.act.pdf\n",
      "log/AlexNet/quant/cnn2.2.act.pdf\n",
      "log/AlexNet/quant/cnn2.3.act.pdf\n",
      "log/AlexNet/quant/cnn3.act.pdf\n",
      "log/AlexNet/quant/cnn3.0.act.pdf\n",
      "log/AlexNet/quant/cnn3.1.act.pdf\n",
      "log/AlexNet/quant/cnn4.act.pdf\n",
      "log/AlexNet/quant/cnn4.0.act.pdf\n",
      "log/AlexNet/quant/cnn4.1.act.pdf\n",
      "log/AlexNet/quant/cnn5.act.pdf\n",
      "log/AlexNet/quant/cnn5.0.act.pdf\n",
      "log/AlexNet/quant/cnn5.1.act.pdf\n",
      "log/AlexNet/quant/fc_layers.act.pdf\n",
      "log/AlexNet/quant/fc_layers.0.act.pdf\n",
      "log/AlexNet/quant/fc_layers.1.act.pdf\n",
      "log/AlexNet/quant/fc_layers.2.act.pdf\n",
      "log/AlexNet/quant/fc_layers.3.act.pdf\n",
      "log/AlexNet/quant/fc_layers.4.act.pdf\n",
      "log/AlexNet/quant/fc_layers.5.act.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xlabel = \"Expectation\"\n",
    "ylable = \"Density\"\n",
    "import seaborn as sns\n",
    "import os\n",
    "sns.set()\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "bin_num = 10\n",
    "\n",
    "\n",
    "font_family = {'family': 'Times New Roman', 'size': 14}\n",
    "font_size = {'ax_label': 24, 'ax_tick': 22, 'inner_tick': 14, 'text': 15, 'legend': 12}\n",
    "line_args = {'marker': 'o', 'markevery': bin_num, 'ls': '--', 'mfc': 'blue', 'mec': 'k'}\n",
    "\n",
    "sufix = 'quant' if quant else 'full-precision'\n",
    "base_path = 'log/' + model_name + '/' + sufix + \"/\"\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)\n",
    "\n",
    "def draw_handle(name, data):\n",
    "    plt.clf()\n",
    "    fig, ax = plt.subplots()\n",
    "    # plt.hist(data)\n",
    "    # ax = plt.subplot()\n",
    "    binw = 1\n",
    "    g = sns.histplot(data.flatten(), kde=True, fill=True, stat='probability', line_kws=line_args)\n",
    "    g.lines[0].set_color('crimson')\n",
    "\n",
    "    # STEM PLOT https://github.com/mwaskom/seaborn/issues/2344\n",
    "    points = g.lines[0].get_path().vertices\n",
    "    x, y = np.split(points, 2, 1)\n",
    "    x = x[np.arange(0, len(x), int(len(x)/bin_num))]\n",
    "    y = y[np.arange(0, len(y), int(len(y)/bin_num))]\n",
    "    # plt.stem(x, y, linefmt ='g--', markerfmt='b')\n",
    "    # STEM PLOT\n",
    "\n",
    "    # sns.displot(data.flatten(), kde=True)\n",
    "    # g = sns.kdeplot(data.flatten(), color='red', marker='o', markevery=10, linestyle=':')\n",
    "    # g.yaxis.set_major_formatter(PercentFormatter(1 / binw))\n",
    "    # sns.distplot(data.flatten(), kde=True, hist=False, norm_hist=True)\n",
    "    # plt.hist(data.flatten(), density=True)\n",
    "    # plt.xlabel(xlabel, fontdict=font_family)\n",
    "    # plt.ylabel(ylable, fontdict=font_family)\n",
    "    ax.set(ylabel=None)\n",
    "    plt.grid(True, alpha=0.5, linestyle='-.')\n",
    "    plt.title(name, fontdict=font_family)\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n",
    "    print(base_path + name + '.pdf')\n",
    "    plt.savefig(base_path + name + '.pdf')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # draw_handle(name.split('.')[0] + '.weight', param.data.numpy())\n",
    "    # draw_handle(name.split('.')[0] + '.weight_grad', param.grad.numpy())\n",
    "\n",
    "    draw_handle(name, param.data.numpy())\n",
    "    draw_handle(name + '_grad', param.grad.numpy())\n",
    "\n",
    "    # logger.add_histogram(name, param.data.numpy(), global_iter_num)\n",
    "    # logger.add_histogram(name+\"_grad\", param.grad.numpy(), global_iter_num)\n",
    "for name, module in model.named_modules():\n",
    "    draw_handle(name + '.act', activations[name])\n",
    "\n",
    "    # logger.add_histogram(name, activations[name], global_iter_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8e35784da7b5309346b9948007e2c8e432cfca57e220859a198433798687959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
